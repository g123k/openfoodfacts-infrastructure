
<!DOCTYPE html>

<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<link href="../2023-02-16-off2-shutdown/" rel="prev"/>
<link href="../../assets/images/favicon.png" rel="icon"/>
<meta content="mkdocs-1.4.2, mkdocs-material-9.1.2" name="generator"/>
<title>2023-02-17 Off2 Upgrade - Open Food Facts Infrastructure documentation</title>
<link href="../../assets/stylesheets/main.7bf56d0a.min.css" rel="stylesheet"/>
<link href="../../assets/stylesheets/palette.a0c5b2b5.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
<script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
<link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            
                .gscrollbar-fixer { padding-right: 15px; }
                .gdesc-inner { font-size: 0.75rem; }
                body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
                body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
                body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}
                </style><script src="../../assets/javascripts/glightbox.min.js"></script></head>
<body data-md-color-accent="" data-md-color-primary="" data-md-color-scheme="default" dir="ltr">
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#2023-02-17-off2-upgrade">
          Skip to content
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header md-header--shadow" data-md-component="header">
<nav aria-label="Header" class="md-header__inner md-grid">
<a aria-label="Open Food Facts Infrastructure documentation" class="md-header__button md-logo" data-md-component="logo" href="../.." title="Open Food Facts Infrastructure documentation">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"></path></svg>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            Open Food Facts Infrastructure documentation
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              2023-02-17 Off2 Upgrade
            
          </span>
</div>
</div>
</div>
<div class="md-header__source">
<a class="md-source" data-md-component="source" href="https://github.com/openfoodfacts/openfoodfacts-infrastructure" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.3.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"></path></svg>
</div>
<div class="md-source__repository">
    GitHub
  </div>
</a>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Navigation" class="md-nav md-nav--primary" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="Open Food Facts Infrastructure documentation" class="md-nav__button md-logo" data-md-component="logo" href="../.." title="Open Food Facts Infrastructure documentation">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"></path></svg>
</a>
    Open Food Facts Infrastructure documentation
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-component="source" href="https://github.com/openfoodfacts/openfoodfacts-infrastructure" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.3.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"></path></svg>
</div>
<div class="md-source__repository">
    GitHub
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../..">
        OpenFoodFacts Infrastructure
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../cicd/">
        Continous Integration and Continuous Delivery
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../discourse/">
        Discourse
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../docker/">
        Docker at Open Food Facts
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../docker_architecture/">
        Docker architecture
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../docker_onboarding/">
        Onboarding
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../folksonomy/">
        Folksonomy API
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../free-datacenter/">
        Free Datacenter
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../linux-server/">
        Linux server
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../mail/">
        Mail on Open Food Facts infrastructure
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../matomo/">
        Matomo
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../nginx-reverse-proxy/">
        NGINX Reverse proxy (OVH)
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../observability/">
        Observability
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../odoo/">
        Odoo: our relationship management (connect.openfoodfacts.org)
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../producers_sftp/">
        Producers SFTP
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../promox/">
        Proxmox
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../zammad/">
        Zammad
      </a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_18" type="checkbox"/>
<label class="md-nav__link" for="__nav_18" id="__nav_18_label" tabindex="0">
          Reports
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="true" aria-labelledby="__nav_18_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_18">
<span class="md-nav__icon md-icon"></span>
          Reports
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../2021-02-22-matomo-install/">
        Matomo install
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2021-10-03-network-down/">
        [Postmortem] OpenFoodFacts.net down (#1)
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2021-12-21-disk-extension/">
        Disk extension for preprod and containers prod (ovh2)
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2021-12-22-preprod-crash/">
        Preprod crash 2021 12
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2022-02-18-ovh-reverse-proxy-down/">
        [Postmortem] Reverse proxy down
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2022-02-proxmox-mail-gateway-install/">
        Install of proxmox gateway server
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2022-02-remove-containers-ovh1/">
        2022-02 Removing some containers on ovh1
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2022-03-03-zammad-elasticsearch-not-running/">
        Elasticsearch not running on Zammad
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2022-07-08-journey-to-deploy-off-search/">
        Deploying openfoodfacts-search to staging
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2022-07-11-infra-workshop/">
        Infrastructure future - workshop on 11th July 2022
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2022-07-kibana-down-es-circuit-breaking-exception/">
        Kibana down because of Elasticsearch circuit_breaking_exception
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2022-08-17-openfoodfacts-net-unreachable/">
        2022-08-17 openfoodfacts.net unreachable
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2022-11-09-monitoring-move-to-own-vm/">
        2022-11 moving monitoring to its own machine
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2023-02-13-zammad-down/">
        2023-02-13 Zammad down
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2023-02-16-off2-shutdown/">
        2023 02 16 off2 shutdown
      </a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
          2023-02-17 Off2 Upgrade
          <span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="./">
        2023-02-17 Off2 Upgrade
      </a>
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#hardware">
    Hardware
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bios-config">
    Bios config
  </a>
<nav aria-label="Bios config" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#slot-bifurcation">
    Slot bifurcation
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#idrac-ipmi">
    IDRAC / IPMI
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#perc-adapter">
    PERC adapter
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#proxmox-install">
    Proxmox install
  </a>
<nav aria-label="Proxmox install" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#first-launch">
    first launch
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#zfs-commands">
    zfs commands
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#putting-things-back-in-place">
    Putting things back in place
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#post-zfs-install">
    Post ZFS install
  </a>
<nav aria-label="Post ZFS install" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#preparing-the-optane">
    preparing the optane
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#resizing-rpool">
    Resizing rpool
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#upgrading-rpool2">
    Upgrading rpool2
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#nvme-pool">
    NVME pool
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#zfs-hdd-pool">
    zfs-hdd pool
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#final-configuration">
    final configuration
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#other-post-install">
    Other Post install
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#still-todo">
    Still TODO ?
  </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#hardware">
    Hardware
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bios-config">
    Bios config
  </a>
<nav aria-label="Bios config" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#slot-bifurcation">
    Slot bifurcation
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#idrac-ipmi">
    IDRAC / IPMI
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#perc-adapter">
    PERC adapter
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#proxmox-install">
    Proxmox install
  </a>
<nav aria-label="Proxmox install" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#first-launch">
    first launch
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#zfs-commands">
    zfs commands
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#putting-things-back-in-place">
    Putting things back in place
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#post-zfs-install">
    Post ZFS install
  </a>
<nav aria-label="Post ZFS install" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#preparing-the-optane">
    preparing the optane
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#resizing-rpool">
    Resizing rpool
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#upgrading-rpool2">
    Upgrading rpool2
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#nvme-pool">
    NVME pool
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#zfs-hdd-pool">
    zfs-hdd pool
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#final-configuration">
    final configuration
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#other-post-install">
    Other Post install
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#still-todo">
    Still TODO ?
  </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<h1 id="2023-02-17-off2-upgrade">2023-02-17 Off2 Upgrade<a class="headerlink" href="#2023-02-17-off2-upgrade" title="Permanent link">#</a></h1>
<h2 id="hardware">Hardware<a class="headerlink" href="#hardware" title="Permanent link">#</a></h2>
<p>We plug a monitor and a keyboard on off2 (on the rear).
We shut down the server. And remove both plugs.</p>
<p>Then we removed the two 4Tb disks (they were on the right). We unscrew them from the rail, and put the new 14Tb disks. We now have 4x14Tb.
The new 14Tb disks have exactly the same reference numbers as the old one.</p>
<p>Then we put the new RAMs in the server. They are symmetric to the already existing one, on the other side of the CPU. The cover indicates in which orientation memories should be connected.</p>
<p>We removed the card with the two SSDs on it to put a new card that support four SSD. We removed the box containing the card from the back of the server (next to ethernet ports) and plug the card out. The new card is a bit bigger than the previous so we have to remove a small piece of plastic to pull it into place. But first we put the three SSDs + the Optane disk (two on each sides). We miss one small screw for on of the SSD, but as it is blocked by the box bottom, it's not a problem.
We put the card back into the server.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-title="SDD adpater card" data-width="100%" href="../../img/2023-02-free-dc-ssd-pci-card.jpg"><img alt="SDD adpater card" src="../../img/2023-02-free-dc-ssd-pci-card.jpg" title="The SDD adapter card" width="50%"/></a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-title="Inserting adpater card" data-width="100%" href="../../img/2023-02-free-dc-ssd-pci-card-insertion.jpg"><img alt="Inserting adpater card" src="../../img/2023-02-free-dc-ssd-pci-card-insertion.jpg" title="Inserting SDD adapter card into the extension slot" width="50%"/></a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-title="Insert back slot in server" data-width="100%" href="../../img/2023-02-free-dc-ssd-pci-setup.jpg"><img alt="Insert back slot in server" src="../../img/2023-02-free-dc-ssd-pci-setup.jpg" title="Inserting the extension slot with the SDD adapter card back in the server" width="50%"/></a></p>
<p>Now we are ready for reboot.</p>
<p>We plug the server again.</p>
<p>We power on (small button on the right on the front of the server).</p>
<h2 id="bios-config">Bios config<a class="headerlink" href="#bios-config" title="Permanent link">#</a></h2>
<p>At some point, after memory check, the server display a dark screen and indicates some key to trigger BIOS. We hit F2 to enter BIOS (System Setup).</p>
<h3 id="slot-bifurcation">Slot bifurcation<a class="headerlink" href="#slot-bifurcation" title="Permanent link">#</a></h3>
<p>We go in:
* System BIOS
* Integrated devices
* Slot bifurcation
* we choose: Auto discovery of bifurcation</p>
<p>This is to specify how the PCI card supporting SSD will work (16 port divided in 4x4).</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-title="BIOS screen - slot bifurcation menu" data-width="100%" href="../../img/2023-02-free-dc-bios-slot-bifurcation-menu.jpg"><img alt="BIOS screen - slot bifurcation menu" src="../../img/2023-02-free-dc-bios-slot-bifurcation-menu.jpg" title="Getting to Slot bifurcation menu in BIOS" width="50%"/></a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-title="BIOS screen - slot bifurcation" data-width="100%" href="../../img/2023-02-free-dc-bios-slot-bifurcation-auto.jpg"><img alt="BIOS screen - slot bifurcation" src="../../img/2023-02-free-dc-bios-slot-bifurcation-auto.jpg" title="Slot bifurcation set to Auto Discovery in BIOS" width="50%"/></a></p>
<h3 id="idrac-ipmi">IDRAC / IPMI<a class="headerlink" href="#idrac-ipmi" title="Permanent link">#</a></h3>
<p>We go in Network settings to configure IDRAC / IPMI (which has its own ethernet card):
* disable DHCP and auto-discovery
* Static IP address: 213.36.253.209
* Gateway: 213.36.253.222
* Subnet Mask: 255.255.255.224
* Static Prefered DNS: 213.36.253.10
* Static Alter: 213.36.252.131</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-title="BIOS screen - IDRAC" data-width="100%" href="../../img/2023-02-free-dc-bios-idrac-settings.jpg"><img alt="BIOS screen - IDRAC" src="../../img/2023-02-free-dc-bios-idrac-settings.jpg" title="IDRAC settings in BIOS" width="50%"/></a></p>
<p>These settings are given by our host provider (free)</p>
<h3 id="perc-adapter">PERC adapter<a class="headerlink" href="#perc-adapter" title="Permanent link">#</a></h3>
<p>We go reboot and go again in the BIOS to configure PERC Adapter Bios (Power Edge RAID Controller), and change configuration to be in HBA mode for disks.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-title="BIOS screen - PERC - 1" data-width="100%" href="../../img/2023-02-free-dc-bios-disks-perc-1.jpg"><img alt="BIOS screen - PERC - 1" src="../../img/2023-02-free-dc-bios-disks-perc-1.jpg" title="PERC settings in BIOS - first screen" width="50%"/></a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-title="BIOS screen - PERC - 2" data-width="100%" href="../../img/2023-02-free-dc-bios-disks-perc-2.jpg"><img alt="BIOS screen - PERC - 2" src="../../img/2023-02-free-dc-bios-disks-perc-2.jpg" title="PERC settings in BIOS - second screen" width="50%"/></a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-title="BIOS screen - PERC - hba" data-width="100%" href="../../img/2023-02-free-dc-bios-disks-perc-hba.jpg"><img alt="BIOS screen - PERC - hba" src="../../img/2023-02-free-dc-bios-disks-perc-hba.jpg" title="PERC settings in BIOS - HBA mode screen" width="50%"/></a></p>
<h2 id="proxmox-install">Proxmox install<a class="headerlink" href="#proxmox-install" title="Permanent link">#</a></h2>
<p>We plug the bootable USB stick with <a href="https://www.proxmox.com/en/proxmox-ve">Proxmox VE</a> iso for installation.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-title="USB stick plugged for install" data-width="100%" href="../../img/2023-02-free-dc-usb-stick.jpg"><img alt="USB stick plugged for install" src="../../img/2023-02-free-dc-usb-stick.jpg" title="The USB stick with iso of Proxmox VE ready to boot" width="50%"/></a></p>
<p>As we re-start the server we go in boot setup (F11) and in boot option menu, we choose the USB key media to boot.</p>
<p>We arrive on Proxmox install screen, and start installation.</p>
<p>It discovers the network, and ask to validate EULA (End User License Agreement).</p>
<p>We have to choose the target start disk. We can see all disks are there.
We choose target ZFS. We choose RAID5, and we setup to only use the four 14TB disks. advanced config but keep the defaults.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-title="Proxmox VE first screen" data-width="100%" href="../../img/2023-02-free-dc-pve-install-start.jpg"><img alt="Proxmox VE first screen" src="../../img/2023-02-free-dc-pve-install-start.jpg" title="The first screen of install of Proxmox VE" width="50%"/></a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-title="Proxmox VE choosing ZFS" data-width="100%" href="../../img/2023-02-free-dc-pve-install-choice-zfs.jpg"><img alt="Proxmox VE choosing ZFS" src="../../img/2023-02-free-dc-pve-install-choice-zfs.jpg" title="We choose to install on ZFS" width="50%"/></a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-title="ZFS pool choice screen" data-width="100%" href="../../img/2023-02-free-dc-pve-install-zfs-pool.jpg"><img alt="ZFS pool choice screen" src="../../img/2023-02-free-dc-pve-install-zfs-pool.jpg" title="We choose the disks to add in the ZFS pool" width="50%"/></a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-title="ZFS advanced" data-width="100%" href="../../img/2023-02-free-dc-pve-install-zfs-pool-advanced.jpg"><img alt="ZFS advanced" src="../../img/2023-02-free-dc-pve-install-zfs-pool-advanced.jpg" title="We kept default for ZFS advanced options" width="50%"/></a></p>
<p>But maybe we did two errors:</p>
<ul>
<li>we should have changed the HDSize to have a bigger partition (Proxomx VE choose 13Gb because this was the size of the smallest disk)</li>
<li>we should have looked for an option to rename the ZFS to something else than rpool, as a ZFS pool named rpool was already present (see below)</li>
</ul>
<p>We then add a root password, email: root@openfoodfacts.org
hostname: off2.openfoodfacts.org</p>
<p>Eno2 network setup:
* IP static: 213.36.253.208/27
* Gateway: 213.36.253.222
* DNS Server: 213.36.253.10</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-title="eno2 config" data-width="100%" href="../../img/2023-02-free-dc-pve-install-network.jpg"><img alt="eno2 config" src="../../img/2023-02-free-dc-pve-install-network.jpg" title="Configuration for eno2 interface" width="50%"/></a></p>
<p>We are asked for confirmation:</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-title="Install summary screen" data-width="100%" href="../../img/2023-02-free-dc-pve-install-summary.jpg"><img alt="Install summary screen" src="../../img/2023-02-free-dc-pve-install-summary.jpg" title="Summary of Proxmox install" width="50%"/></a></p>
<p>And we launch install. It takes time. At the end it reboots.
We remove USB stick.</p>
<h3 id="first-launch">first launch<a class="headerlink" href="#first-launch" title="Permanent link">#</a></h3>
<p>First launch does not work and we find ourselves on a initramfs prompt.</p>
<p>This is due to the fact that we have two ZFS pools with the same name: <code>rpool</code>: one that was existing (on our old disks) and one we created at install (we should have tried to avoid default name at install).</p>
<p><code>zpool import -f rpool</code> does not work because we have more than one numeric pool for the same name (because the zpool).</p>
<p>So we use <code>zpool list</code> and <code>zpool status</code> to get the numeric id.
Then <code>zpool import -f &lt;numeric-id&gt;</code>.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-title="import of rpool with numeric id" data-width="100%" href="../../img/2023-02-free-dc-pve-setup-zpool-import-rpool.jpg"><img alt="import of rpool with numeric id" src="../../img/2023-02-free-dc-pve-setup-zpool-import-rpool.jpg" title="We had to use numeric id to import rpool because of name clash" width="50%"/></a></p>
<p><code>zpool list</code> show us the pool.</p>
<p>We see the size of the pool is not what we wanted (it only took part of it), because we did not change HD Size parameter on install.</p>
<p>We exit with crtl+D</p>
<p>Now we have access to login.</p>
<p>We use <code>zpool import</code> to get other id for the old rpool and off-zfs</p>
<p><code>zpool import -f &lt;numeric id&gt; rpool2</code> to import old rpool with a new name (rpool2).</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-title="import of rpool2 with numeric id" data-width="100%" href="../../img/2023-02-free-dc-pve-setup-zpool-import-rpool2.jpg"><img alt="import of rpool2 with numeric id" src="../../img/2023-02-free-dc-pve-setup-zpool-import-rpool2.jpg" title="We had to use numeric id to import rpool2 because of name clash" width="50%"/></a></p>
<p>And finally import the last zfs pool
<code>zpool import -f off-zfs</code>. W still need the <code>-f</code> because ZFS knows it was part of another system, and thus ask confirmation to import it in this new system.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-title="final zfs pools" data-width="100%" href="../../img/2023-02-free-dc-pve-setup-zpool-import-off-zfs.jpg"><img alt="final zfs pools" src="../../img/2023-02-free-dc-pve-setup-zpool-import-off-zfs.jpg" title="Final ZFS pools" width="50%"/></a></p>
<h3 id="zfs-commands">zfs commands<a class="headerlink" href="#zfs-commands" title="Permanent link">#</a></h3>
<p>History of commands made by install for rpool:
<div class="highlight"><pre><span></span><code>2023-02-17.11:18:33 zpool create -f -o cachefile=none -o ashift=12 rpool mirror /dev/disk/by-id/ata-TOSHIBA_MG07ACA14TEY_X120A00LF9RG-part3 /dev/disk/by-id/ata-TOSHIBA_MG07ACA14TEY_X120A00PF9RG-part3 /dev/disk/by-id/ata-TOSHIBA_MG07ACA14TEY_X8F0A060F9RG-part3 /dev/disk/by-id/ata-TOSHIBA_MG07ACA14TEY_X8F0A0H8F9RG-part3
2023-02-17.11:18:33 zfs create rpool/ROOT
2023-02-17.11:18:33 zfs create rpool/data
2023-02-17.11:18:33 zfs create rpool/ROOT/pve-1
2023-02-17.11:18:34 zfs set atime=on relatime=on rpool
2023-02-17.11:18:34 zfs set compression=on rpool
2023-02-17.11:18:34 zfs set sync=disabled rpool
2023-02-17.11:22:57 zfs set sync=standard rpool
2023-02-17.11:22:57 zfs set mountpoint=/ rpool/ROOT/pve-1
2023-02-17.11:22:57 zpool set bootfs=rpool/ROOT/pve-1 rpool
2023-02-17.11:22:57 zpool export rpool
</code></pre></div></p>
<p>Then Christian on first (failing) boot:
<div class="highlight"><pre><span></span><code>2023-02-17.11:26:05 zpool import 9614896434456967606
2023-02-17.11:31:09 zpool import -N rpool
</code></pre></div></p>
<p>rpool2 was created long ago:
<div class="highlight"><pre><span></span><code>2021-02-16.11:06:54 zpool create -o ashift=12 rpool nvme0n1p2
</code></pre></div></p>
<p>And Christian on first (failing) boot:
<div class="highlight"><pre><span></span><code>2023-02-17.11:27:48 zpool import 3186033342002808046 rpool2 -f
2023-02-17.11:31:15 zpool import -c /etc/zfs/zpool.cache -aN
</code></pre></div></p>
<h2 id="putting-things-back-in-place">Putting things back in place<a class="headerlink" href="#putting-things-back-in-place" title="Permanent link">#</a></h2>
<p>We put the cover back in place, it make the led turn from blinking orange to stable blue again !</p>
<h2 id="post-zfs-install">Post ZFS install<a class="headerlink" href="#post-zfs-install" title="Permanent link">#</a></h2>
<h3 id="preparing-the-optane">preparing the optane<a class="headerlink" href="#preparing-the-optane" title="Permanent link">#</a></h3>
<p>We want to use the optane in 3 data pool, so we use parted to divide it into partitions.</p>
<p>Final partition is:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># parted /dev/nvme2n1 print</span>
Model:<span class="w"> </span>INTEL<span class="w"> </span>MEMPEK1J016GA<span class="w"> </span><span class="o">(</span>nvme<span class="o">)</span>
Disk<span class="w"> </span>/dev/nvme2n1:<span class="w"> </span><span class="m">14</span>,4GB
Sector<span class="w"> </span>size<span class="w"> </span><span class="o">(</span>logical/physical<span class="o">)</span>:<span class="w"> </span>512B/512B
Partition<span class="w"> </span>Table:<span class="w"> </span>gpt
Disk<span class="w"> </span>Flags:<span class="w"> </span>

Number<span class="w">  </span>Start<span class="w">   </span>End<span class="w">     </span>Size<span class="w">    </span>File<span class="w"> </span>system<span class="w">  </span>Name<span class="w">           </span>Flags
<span class="w"> </span><span class="m">1</span><span class="w">      </span>1049kB<span class="w">  </span>7202MB<span class="w">  </span>7201MB<span class="w">  </span>zfs<span class="w">          </span>zfs-log-hdd
<span class="w"> </span><span class="m">2</span><span class="w">      </span>7202MB<span class="w">  </span><span class="m">10</span>,8GB<span class="w">  </span>3601MB<span class="w">  </span>zfs<span class="w">          </span>zfs-log-ssd
<span class="w"> </span><span class="m">3</span><span class="w">      </span><span class="m">10</span>,8GB<span class="w">  </span><span class="m">14</span>,4GB<span class="w">  </span>3600MB<span class="w">  </span>zfs<span class="w">          </span>zfs-log-rpool
</code></pre></div>
<h3 id="resizing-rpool">Resizing rpool<a class="headerlink" href="#resizing-rpool" title="Permanent link">#</a></h3>
<p>Afterwards, Christian changed the size of the ZFS pool rpool to have 64Gb size for system.</p>
<p>We have the third partitions in the rpool, so resizing them will resize the <code>rpool</code> (if autoexpand is on)</p>
<p>For each sda/sdb/sdc/sdd
<div class="highlight"><pre><span></span><code>parted<span class="w"> </span>/dev/sdX<span class="w"> </span>resizepart<span class="w"> </span><span class="m">3</span><span class="w"> </span>70g
</code></pre></div></p>
<p>And later on, add a log disk and set <code>autoexpand=on</code> to insure we take into account the resizing of partitions:
<div class="highlight"><pre><span></span><code>2023-02-17.16:01:03 zpool add rpool log nvme-INTEL_MEMPEK1J016GA_PHBT817502WX016N-part3
2023-02-17.16:21:17 zpool set autoexpand=on rpool
</code></pre></div></p>
<h3 id="upgrading-rpool2">Upgrading rpool2<a class="headerlink" href="#upgrading-rpool2" title="Permanent link">#</a></h3>
<p>As said above, the former rpool pool was renamed to <code>rpool2</code>.</p>
<p>We must upgrade it (because we changed zfs version)</p>
<p>We take a snapshot, and then send the snapshot to the new zfs-nvme pool.
<div class="highlight"><pre><span></span><code><span class="m">2023</span>-02-17.16:03:42<span class="w"> </span>zfs<span class="w"> </span>snap<span class="w"> </span>-r<span class="w"> </span>rpool2@move
<span class="m">2023</span>-02-17.16:19:05<span class="w"> </span>zpool<span class="w"> </span>upgrade<span class="w"> </span>rpool2
<span class="m">2023</span>-02-17.18:23:27<span class="w"> </span>zfs<span class="w"> </span>send<span class="w"> </span>-vwR<span class="w"> </span>rpool2@move
</code></pre></div></p>
<h3 id="nvme-pool">NVME pool<a class="headerlink" href="#nvme-pool" title="Permanent link">#</a></h3>
<p>This was created some time before the migration, using a SSD and a log</p>
<div class="highlight"><pre><span></span><code>History<span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="s1">'zfs-nvme'</span>:
<span class="m">2023</span>-02-08.19:05:06<span class="w"> </span>zpool<span class="w"> </span>create<span class="w"> </span>off-zfs<span class="w"> </span>-o<span class="w"> </span><span class="nv">ashift</span><span class="o">=</span><span class="m">12</span><span class="w"> </span>mirror<span class="w"> </span>nvme-Corsair_MP600_GS_22508026000132450051<span class="w"> </span>nvme-WD_BLACK_SN770_2TB_22517U454111
<span class="m">2023</span>-02-08.19:06:05<span class="w"> </span>zpool<span class="w"> </span>add<span class="w"> </span>off-zfs<span class="w"> </span>log<span class="w"> </span>nvme-INTEL_MEMPEK1J016GA_PHBT817502WX016N
</code></pre></div>
<p>we mount it on first (failing) boot:
<div class="highlight"><pre><span></span><code>2023-02-17.11:28:20 zpool import off-zfs -f
</code></pre></div></p>
<p>we removed the log and put it back but using only part of the octane:
<div class="highlight"><pre><span></span><code>2023-02-17.15:57:38 zpool remove off-zfs nvme-INTEL_MEMPEK1J016GA_PHBT817502WX016N
2023-02-17.15:59:25 zpool add off-zfs log nvme-INTEL_MEMPEK1J016GA_PHBT817502WX016N-part1
</code></pre></div></p>
<p>We set some properties and rename it from off-zfs to zfs-nvme and create the zfs-nvme/pve dataset:</p>
<div class="highlight"><pre><span></span><code>2023-02-17.16:26:03 zfs set xattr=sa off-zfs
2023-02-17.16:42:43 zpool trim off-zfs
...
2023-02-17.19:39:34 zpool export off-zfs
2023-02-17.19:39:57 zpool import off-zfs zfs-nvme
2023-02-17.19:42:19 zfs create zfs-nvme/pve
</code></pre></div>
<p>we also receive the data from rpool2 back here:</p>
<div class="highlight"><pre><span></span><code><span class="w"> </span><span class="nb">time</span><span class="w"> </span>zfs<span class="w"> </span>send<span class="w"> </span>-vwR<span class="w"> </span>rpool2@move<span class="w"> </span><span class="p">|</span><span class="w"> </span>zfs<span class="w"> </span>recv<span class="w"> </span>off-zfs<span class="w"> </span>-F
</code></pre></div>
<h3 id="zfs-hdd-pool">zfs-hdd pool<a class="headerlink" href="#zfs-hdd-pool" title="Permanent link">#</a></h3>
<p>First we create partitions for those rpool.
For each sda/sdb/sdc/sdd:
<div class="highlight"><pre><span></span><code>parted<span class="w"> </span>/dev/sdX<span class="w"> </span>mkpart<span class="w"> </span>zfs-hdd<span class="w"> </span>zfs<span class="w"> </span>70g<span class="w"> </span><span class="m">100</span>%
</code></pre></div></p>
<p>We creates a zfs-hdd pool with partitions mounted as zraid1 and (sda4, sdb4, sdc4 and sdd4) and a partition on the octane disk as log and some properties.</p>
<div class="highlight"><pre><span></span><code><span class="m">2023</span>-02-17.19:25:55<span class="w"> </span>zpool<span class="w"> </span>create<span class="w"> </span>zfs-hdd<span class="w"> </span>-o<span class="w"> </span><span class="nv">ashift</span><span class="o">=</span><span class="m">12</span><span class="w"> </span>raidz1<span class="w"> </span>sda4<span class="w"> </span>sdb4<span class="w"> </span>sdc4<span class="w"> </span>sdd4
<span class="m">2023</span>-02-17.19:26:44<span class="w"> </span>zfs<span class="w"> </span><span class="nb">set</span><span class="w"> </span><span class="nv">compress</span><span class="o">=</span>on<span class="w"> </span><span class="nv">xattr</span><span class="o">=</span>sa<span class="w"> </span>zfs-hdd
<span class="m">2023</span>-02-17.19:27:27<span class="w"> </span>zpool<span class="w"> </span>add<span class="w"> </span>zfs-hdd<span class="w"> </span>log<span class="w"> </span>nvme-INTEL_MEMPEK1J016GA_PHBT817502WX016N-part2
</code></pre></div>
<p>We also create the off, backups, images and pve volumes.
<div class="highlight"><pre><span></span><code>2023-02-17.19:32:17 zfs create zfs-hdd/off
2023-02-17.19:42:25 zfs create zfs-hdd/pve
2023-02-17.19:44:19 zfs create zfs-hdd/backups
2023-02-19.01:12:03 zfs recv zfs-hdd/off/images -s
</code></pre></div></p>
<p>And we receive images from ovh3 (with -s to be able to resume).</p>
<div class="highlight"><pre><span></span><code>zfs<span class="w"> </span>recv<span class="w"> </span>zfs-hdd/off/images<span class="w"> </span>-s
</code></pre></div>
<h3 id="final-configuration">final configuration<a class="headerlink" href="#final-configuration" title="Permanent link">#</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># zpool list</span>
NAME<span class="w">       </span>SIZE<span class="w">  </span>ALLOC<span class="w">   </span>FREE<span class="w">  </span>CKPOINT<span class="w">  </span>EXPANDSZ<span class="w">   </span>FRAG<span class="w">    </span>CAP<span class="w">  </span>DEDUP<span class="w">    </span>HEALTH<span class="w">  </span>ALTROOT
rpool<span class="w">     </span><span class="m">64</span>.5G<span class="w">  </span><span class="m">1</span>.65G<span class="w">  </span><span class="m">62</span>.9G<span class="w">        </span>-<span class="w">         </span>-<span class="w">     </span><span class="m">2</span>%<span class="w">     </span><span class="m">2</span>%<span class="w">  </span><span class="m">1</span>.00x<span class="w">    </span>ONLINE<span class="w">  </span>-
rpool2<span class="w">     </span>684G<span class="w">   </span>518G<span class="w">   </span>166G<span class="w">        </span>-<span class="w">         </span>-<span class="w">    </span><span class="m">87</span>%<span class="w">    </span><span class="m">75</span>%<span class="w">  </span><span class="m">1</span>.00x<span class="w">    </span>ONLINE<span class="w">  </span>-
zfs-hdd<span class="w">   </span><span class="m">50</span>.7T<span class="w">  </span><span class="m">13</span>.6T<span class="w">  </span><span class="m">37</span>.1T<span class="w">        </span>-<span class="w">         </span>-<span class="w">     </span><span class="m">0</span>%<span class="w">    </span><span class="m">26</span>%<span class="w">  </span><span class="m">1</span>.00x<span class="w">    </span>ONLINE<span class="w">  </span>-
zfs-nvme<span class="w">  </span><span class="m">1</span>.81T<span class="w">   </span>584G<span class="w">  </span><span class="m">1</span>.24T<span class="w">        </span>-<span class="w">         </span>-<span class="w">     </span><span class="m">7</span>%<span class="w">    </span><span class="m">31</span>%<span class="w">  </span><span class="m">1</span>.00x<span class="w">    </span>ONLINE<span class="w">  </span>-
</code></pre></div>
<h2 id="other-post-install">Other Post install<a class="headerlink" href="#other-post-install" title="Permanent link">#</a></h2>
<p>Christian installed fail2ban</p>
<p>Christian also copied off1 root certificate to off2 authorized_keys.</p>
<p>Alex installed sudo, tree, vim.</p>
<p>Copied my user .bashrc to /root/ (as the root .bashrc was almost empty)</p>
<h2 id="still-todo">Still TODO ?<a class="headerlink" href="#still-todo" title="Permanent link">#</a></h2>
<ol>
<li>install  iptables-persistent</li>
</ol>
</article>
</div>
</div>
</main>
<footer class="md-footer">
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
      Material for MkDocs
    </a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
<script src="../../assets/javascripts/bundle.fc8c2696.min.js"></script>
<script>document$.subscribe(() => {const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});})</script></body>
</html>