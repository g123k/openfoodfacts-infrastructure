{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OpenFoodFacts Infrastructure","text":"<p>Sysadmin repository for the various parts of the Open Food Facts infrastructure. We have a specific repository regarding monitoring</p>"},{"location":"#documentation","title":"DocumentationWeekly meetings","text":"<p>Link to Github Page</p> <p>The infrastructure documentation is as follows:</p> <ul> <li>Mail - servers mail setup</li> <li>Linux Server - servers general setup</li> <li>Proxmox - about proxmox management</li> <li>CICD - continuous integration and deployment</li> <li>Observability - doc on monitoring / logs / etc.</li> <li>Docker Onboarding</li> <li>Docker Infrastructure</li> <li>Virtual Machines</li> </ul> <p>Some services:</p> <ul> <li>Discourse for forum</li> <li>NGINX reverse proxy the reverse proxy for OVH services</li> <li>Folksonomy user editable labels and values</li> <li>Matomo for web analytics</li> <li>Producers sftp to push product updates on producer platform</li> <li>Zammad for support</li> </ul> <p>Also look at all install and post-mortem reports in docs/reports</p>   * We e-meet monthly at 16:00 Paris Time (15:00 London Time, 20:30 IST, 07:00 AM PT) * ![Google Meet](https://img.shields.io/badge/Google%20Meet-00897B?logo=google-meet&amp;logoColor=white) Video call link: https://meet.google.com/nnw-qswu-hza * Join by phone: https://tel.meet/nnw-qswu-hza?pin=2111028061202 * Add the Event to your Calendar by [adding the Open Food Facts community calendar to your calendar](https://wiki.openfoodfacts.org/Events) * [Weekly Agenda](https://drive.google.com/open?id=1LL8-aiSF482xaJ1o0AKmhXB5QWfVE0_jzvYakq3VXys): please add the Agenda items as early as you can.  * Make sure to check the Agenda items in advance of the meeting, so that we have the most informed discussions possible.  * The meeting will handle Agenda items first, and if time permits, collaborative bug triage. * We strive to timebox the core of the meeting (decision making) to 30 minutes, with an optional free discussion/live debugging afterwards. * We take comprehensive notes in the Weekly Agenda of agenda item discussions and of decisions taken."},{"location":"#requests","title":"Requests","text":""},{"location":"#virtual-machines","title":"Virtual Machines","text":"Title State OS CPU # RAM SSD (Local) HDD (Remote) Services     Monitoring - VM (QEMU host for docker) [#159] open Debian * 4 CPUs * 12G for we have influxdb and elastic-search that needs memory * 30 Go disk (it is currently around 14G, but this will grow because we want to harvest more logs and more metrics) * 50Go for ES backups Docker, docker-compose   CT for new blog engine [#80] open Debian stable. 3 CPU. 2 GB. 10 GB -- LAMP + wordpress.   CT for Folksonomy Engine API dev [#76] open Default to Debian last Stable. 2 1 GB 12 GB. - PostgreSQL, Python3.    Wild School Eco-Score project [#37] open Debian 10 4 16 Gb 30 Gb 0 MongoDB    slack-org [#36] open Debian 10 1 1 Gb 10 Gb None Node.js    adminer-org [#29] open Debian 10 2 512 Mb. 4 Gb or even less. 0 Nginx, PHP, Adminer.   Containers (x2) to build a replica set for OFF database [#28] open Debian 10 4 32 GB 50 GB (DB = 20 GB). 0 Mongodb.    feedme-org [#27] open Debian 10 3 3 Gb. 15 Gb. 0 PostgreSQL, Node.js, Nginx.    off-wiki-org [#21] open Debian 10 2 3 Gb 14 Gb. 14 Gb Apache, PHP, MySQL, Mediawiki.   VM for the Community Portal [#124] closed Debian last Stable. [Explain if &gt; 4.] [Explain if &gt; 4 Gb.] [Explain if &gt; 32 Gb.] [Explain if &gt; 1 Tb.] Python/Django, probably PostgreSQL, probably Apache and all Dockerized   VM for the Taxonomy Editor [#123] closed Debian last Stable. [Explain if &gt; 4.] [Explain if &gt; 4 Gb.] [Explain if &gt; 32 Gb.] [Explain if &gt; 1 Tb.] Python, probably PostgreSQL, probably Apache for lightweight API serving from Docker   New VM QEMU for prod docker containers [#71] closed Debian 11 (stable) 8 24 GB 256 GB. - Services deployed in production:    monitoring [#59] closed Debian 11 4 32GB 64GB 500GB (ovh3 mount) Docker: ElasticSearch (Kibana?, Logstash?), Grafana, InfluxDB, Prometheus, Alertmanager    impactestimator-net [#55] closed Debian 11 1 1GB 1Gb 0 https://github.com/openfoodfacts/impactestimator    robotoff-ml [#53] closed Debian 11 8 96GB (Tensorflow, ANN) 192GB [ML models] 100GB Tensorflow + ElasticSearch    robotoff-net [#51] closed Debian 11 4 16GB (DB 4GB, Services 8GB) 92GB 0GB Robotoff API + Schedulers + Workers, PostgreSQL DB    mongo-dev [#45] closed Debian 10 2 16GB 40GB  MongoDB running in Docker    off-net [#41] closed Debian 10 4 16GB (PO needs &gt; 6GB) 192GB 0GB ProductOpener frontend + backend, MongoDB, PostgreSQL, Memcached    robotoff-dev [#40] closed Debian 10 4 8 Gb 32 Gb 100 Gb robotoff, elastic search, tensorflow, postgresql    Matomo [#24] closed Debian 10 No idea. No idea. No idea. No idea. LAMP    robotoff-org [#20] closed Debian 10 4 8 Gb 32 Gb 100 Gb robotoff, elastic search, tensorflow, postgresql     <p>\u00a0\u00a0Request a VM</p>"},{"location":"cicd/","title":"Continous Integration and Continuous Delivery","text":"<p>This document presents the Continous Integration and Continous Delivery (CICD) process at Open Food Facts. </p> <p>The information below is valid for most OFF repositories containing apps deployed on OFF servers. A summary table is given at the end to get the status of the deployment / test automation across different OFF repositories.</p>"},{"location":"cicd/#technology-stack","title":"Technology Stack","text":"<p>This section gives an overview on the technologies used to automate the CI and CD process at Open Food Facts. Feel free to skip it if you already know these technologies !</p>"},{"location":"cicd/#docker","title":"Docker","text":"<p>See docker</p>"},{"location":"cicd/#makefile-uniformity","title":"Makefile (uniformity)","text":"<p>A <code>Makefile</code> proves very useful for wrapping up and centralizing all the commands we run (locally or on remote environments) and have a lighter development and deployment process using simpler aliases.</p> <p>Open Food Facts contributors should know that the Makefile is the simplest entrypoint for collaborating to Open Food Facts repos, although they are not mandatory if the user have a good knowledge of the application at hand.</p> <p><code>Makefile</code>s should stay away from complexities when possible and be streamlined enough that we can easily understand what the commands stand for.</p> <p>It is important to be able to switch between the different Open Food Facts repositories but keep the same interface to set up our local developer workflow. </p> <p>Most of the existing OFF repos try to have the commands below in their <code>Makefile</code>:</p> <ul> <li><code>make dev</code> is the only command needed to set up a complete developer environment after cloning the repo. It should hopefully never fail, but if it does anyway please open an issue to track it.</li> </ul> <ul> <li><code>make up</code>, <code>make down</code>, <code>make hdown</code>, <code>make restart</code>, <code>make status</code> map exactly to <code>docker-compose</code> commands, respectively <code>docker-compose up</code>, <code>docker-compose down</code>, <code>docker-compose down -v</code>, <code>docker-compose restart</code> and <code>docker-compose ps</code>.</li> </ul> <p>Using a different <code>.env</code> file (e.g: <code>.env.test</code>) is supported by setting the env variable <code>ENV_FILE=.env.test</code> so that the Make commands still work.</p>"},{"location":"cicd/#github-actions","title":"GitHub Actions","text":"<p>We use GitHub Actions to automatically run tests pull requests (unit / integration / lint / performance), but also to build and deploy Docker containers to pre-production and production environments.</p> <p>GitHub actions workflows are stored in <code>.github/workflows</code> in each repository.</p> <p>In order to ease the deployments of new repositories and have uniform deployments across OFF apps, 2 GitHub Actions workflow templates were created, which can be setup by going to the <code>Actions</code> tab on the GitHub repo and selecting the \"Docker image build\" and \"Docker Compose Deployment\" actions:</p>"},{"location":"cicd/#github-bot","title":"Github Bot","text":"<p>Some actions that needs their actions to trigger new actions, like release-please, needs a PAT (Personnal Access Token) for that.</p> <p>We created a bot account for those cases. It's linked to <code>tech</code> mailing list. Having a bot account is better than using personal account because it clearly identified that this is automatic generated stuff, but also because if you use your account to generate PRs, you won't be able to validate them !</p>"},{"location":"cicd/#continuous-integration","title":"Continuous Integration","text":"<p>Continous integration (CI) is the practice of automating the integration of code  changes from multiple contributors into a single software project.</p> <p>It is thus essential in the DevOps space, as it allows developers to frequently merge code changes into a central repository where builds and tests are run.</p> <p>A good CI process consists of the following:</p> <ul> <li>On pull requests, run <code>style checks</code> as well as <code>unit</code>, <code>integration</code>, and <code>performance</code> tests.</li> <li>On merge to <code>main</code> branch, deploy to a live environment and run integration tests on it.</li> </ul>"},{"location":"cicd/#continuous-delivery","title":"Continuous Delivery","text":"<p>Continuous Delivery (CD) is the process of automatically deploying build  artifacts (Docker containers, tars, static assets, data, etc\u2026) to the target  environment servers.</p>"},{"location":"cicd/#deployment-model","title":"Deployment model","text":"<p>The diagram below represents a standard development git tree and how the deployment process wraps up around it. It shows the developers workflow to get a change into  <code>net</code> and <code>org</code> environments, with the following principles in mind:</p> <ul> <li>A pull request needs to be tested automatically before an administrator can merge it. Additionally, an administrator can deploy it to pre-production by pushing the PR branch to a new branch called <code>deploy-&lt;something&gt;</code>.</li> </ul> <ul> <li>Any change needs to be successfully deployed to pre-production before it is deployed to production.</li> </ul> <ul> <li>No humans should have to worry about making releases. The process should be fully automated.</li> </ul>  <p>The following diagram represents the same process, but seen from a persona perspective (Developer, Maintainer, Release Administrator):</p>  <p>Summary:</p> <ul> <li>On pull requests: run the <code>pull_request.yml</code> Github Action workflow that builds and runs unit / integration / load tests locally.</li> <li>On commit to <code>master</code> / <code>main</code>: <ul> <li>run the <code>release-please.yml</code> workflow that will create a release branch or add the commit to an existing release branch; </li> <li>run the <code>container-build.yml</code> workflow that will build the container image and tag it with the merge commit SHA</li> <li>run the <code>container-deploy.yml</code> workflow that will deploy the container image to the pre-production <code>.net</code> environment.</li> </ul> </li> <li>On merge of branches matching <code>release-v*.*.*</code>, run the <code>release-please.yml</code> workflow that will create the <code>v*.*.*</code> tag.</li> <li>On push to tags matching <code>v*.*.*</code>, run the <code>container-deploy.yml</code> workflow that will reload the container image to the off-org environment. The version tag is automatically created when merging a release branch.</li> <li>On push to branches matching <code>deploy-*</code>, run the <code>container-deploy.yml</code> workflow that will push the image to the pre-production (<code>.net</code>) environment. This is useful to quickly test a pull request in the pre-prod environment to see if it breaks anything.</li> </ul> <p>Notes:</p> <ul> <li>deployment process to pre-prod <code>.net</code> and production <code>.org</code> environments is identical (<code>container-deploy.yml</code>), as pre-production should be as close as possible to the production environment to avoid any pitfalls when pushing a release to production.</li> <li>the special branches (<code>deploy-*</code> and <code>release-v*.*.*</code>) MUST be protected branches.</li> <li>deploying to production <code>.org</code> can also be done manually by pushing a tag to the repository that follows semantic versioning: <code>git tag v1.1.0tc1 &amp;&amp; git push --tags</code> although this is not recommended as it contradicts with the automated deployment workflow.</li> <li>release please has to use a user PAT (Access Token) to be able to run release please. See #84, and we have a specific account for that: https://github.com/openfoodfacts-bot</li> </ul> <ul> <li>use github SECRETS only for real secrets !   To set environment variables that depends on the deploy target, use environment modification with a <code>if</code> directive.</li> </ul>"},{"location":"cicd/#rollbacks","title":"Rollbacks","text":"<p>In the advent where pre-production or production environments are broken by a 'bad' change, it is important to be able to rollback to the previous version.</p> <p>Automated rollbacks are tricky with <code>docker-compose</code> (a discussion to migrate to <code>docker swarm</code> should be envisioned), but manual rollbacks are easily done.</p> <p>The steps for executing a manual rollback are as follow:</p> <ul> <li>SSH to the QEMU VM (either pre-production or production) and go to the deployment folder (usually named after the GitHub environment we are deploying to)</li> <li>Replace the <code>TAG</code> variable by <code>sha-&lt;COMMIT_SHA&gt;</code> (where <code>COMMIT_SHA</code> is the last 'good' commit) in the <code>.env</code> file and restore it in the checked out repository.</li> <li>Copy the <code>.env</code> file outside of the checked out repository so that it can be restored later.</li> <li>Run <code>git checkout -qf &lt;COMMIT_SHA&gt;</code> of the last 'good' commit.</li> <li>move the <code>.env</code> file from previous step into the directory</li> </ul> <p>Note that since deployments are automated, the following alternative also exists and is safer, although it can be a bit longer considering the git process:</p> <ul> <li>Revert the 'bad' commit (<code>git revert &lt;COMMIT_SHA&gt;</code>) and make a pull request</li> <li>Push to a branch called <code>deploy-&lt;something&gt;</code> to deploy to pre-prod</li> <li>Merge the pull request: the release workflow runs and creates a new release branch.</li> <li>Merge the release branch: the revert will be deployed in production.</li> </ul>"},{"location":"cicd/#cicd-status","title":"CICD status","text":"<p>The current status of the automation of the deployment and testing processes across Open Food Facts repositories is as follows:</p>    Repository Continuous Testing Continuous Deployment Pre-production deployment Production deployment Release automation     openfoodfacts-server :heavy_exclamation_mark: Weak (lint, unit) :heavy_check_mark: Good :heavy_check_mark: Automated :heavy_exclamation_mark: Manual :heavy_check_mark: Automated   robotoff :heavy_exclamation_mark: Weak (lint, unit) :heavy_check_mark: Good :heavy_check_mark: Automated :heavy_check_mark: Automated :heavy_check_mark: Automated   robotoff-ann :heavy_exclamation_mark: Weak (lint, unit) :heavy_check_mark: Good :heavy_check_mark: Automated :heavy_check_mark: Automated :heavy_check_mark: Automated   impactestimator :heavy_exclamation_mark: Weak :heavy_check_mark: Good :heavy_check_mark: Automated :heavy_check_mark: Automated disabled :heavy_check_mark: Automated disabled   openfoodfacts-monitoring None :heavy_check_mark: Good :heavy_check_mark: Automated :heavy_check_mark: Automated :heavy_check_mark: Automated   smooth-app :heavy_exclamation_mark: Weak (lint, flutter) :heavy_check_mark: Good None :heavy_check_mark: Automated (deployment to Android + IOS stores) :heavy_check_mark: Automated    <p>FIXME: add taxonomy-editor, openfoodfacts-events, facets-knowledge-panels, robotoff-ml</p>"},{"location":"cicd/#qa","title":"Q&amp;A","text":""},{"location":"cicd/#container-deployment-is-failing-how-do-i-fix-it","title":"Container deployment is failing, how do I fix it ?","text":"<p>Have a look at the <code>Actions</code> tab in the GitHub repository and finds out why it  is failing. If the process has trouble checking out the appropriate commit sha,  you might have to ssh to the machine, bring down the deployment (<code>make down</code>)  and delete the repository folder. It will be automatically re-created for the  next deployment.</p>"},{"location":"cicd/#i-forgot-to-set-an-env-variable-on-github-can-i-re-trigger-the-deployment","title":"I forgot to set an env variable on GitHub, can I re-trigger the deployment ?","text":"<p>Yes, simply make an empty commit to your deployment branch: <code>git commit --allow-empty -m \"trigger\" &amp;&amp; git push</code>.  You can also re-trigger a deployment in the repo's <code>Actions</code> tab, assuming you are a repo maintainer.</p>"},{"location":"cicd/#how-do-i-set-up-a-deployment-on-a-new-repository","title":"How do I set up a deployment on a new repository ?","text":"<p>Go to the <code>Actions</code> tab and click on <code>New workflow</code>: scroll down to <code>Workflows created by Open Food Facts</code> and click on <code>Set up this workflow</code> for both <code>Docker image build</code> and <code>Docker Compose Deployment</code> workflows. It will generate pre-configured workflow files in <code>.github/workflows</code> that you can then tweak to your needs and commit to the repository.</p> <p>You will also need to create two GitHub environments (in Settings &gt; Environments) and set up a few secrets needed by the deployment, mainly <code>HOST</code>, <code>SSH_PRIVATE_KEY</code>, <code>PROXY_HOST</code> and <code>USERNAME</code>.</p>"},{"location":"cicd/#what-do-i-do-if-the-deployment-fails-after-merging-my-pr-to-the-main-or-master-branch","title":"What do I do if the deployment fails after merging my PR to the <code>main</code> or <code>master</code> branch ?","text":"<p>Contact an OFF administrator to analyze why it is failing: the admin might have  to revert your PR to restore the previous working version in pre-production; you  can then continue to work on your branch to fix the problem, and make another PR.</p> <p>Ask the OFF administrator to deploy your PR before merging it, so that it is  known ahead of time if the PR will break the pre-production environment.</p>"},{"location":"cicd/#i-dont-have-much-confidence-in-the-next-release-can-i-make-a-release-candidate-before-publishing-the-official-release","title":"I don't have much confidence in the next release, can I make a release candidate before publishing the official release ?","text":"<p>Yes, assuming your next version is <code>v1.1.0</code>, just create a git tag following semantic versioning using <code>git tag v1.1.0rc1 &amp;&amp; git push --tags</code> and the automated process will deploy this release candidate to production.</p>"},{"location":"discourse/","title":"Discourse","text":"<p>Discourse is a forum application.</p> <p>It is installed on our proxmox infrastructure in a QEMU virtual machine (202)</p>"},{"location":"discourse/#software-installation","title":"Software installation","text":"<p>Software is installed in /var/discourse.</p> <p>We use the https://github.com/discourse/discourse_docker.git.</p> <p>The docker configuration is contained in <code>config/app.yml</code></p> <p>It creates a unique docker container containing whole application, postgresql and redis database, and so on !</p> <p>It defines volumes where all the data is found. It's in <code>/var/discourse/shared/standalone</code> and mounted in container as <code>/shared</code></p> <p>You might get a shell into the docker container using <code>./launcher enter app bash</code>.</p>"},{"location":"discourse/#mail","title":"Mail","text":"<p>Mail is very important as a lot of notifications are sent by the forum.</p> <p>Mail can be tested at https://forum.openfoodfacts.org/admin/email</p> <p>We use promox mail gateway.</p> <p>\u26a0 Warning: the sender email have to be on main domain, NOT forum.openfoodfacts.org.</p>"},{"location":"docker/","title":"Docker at Open Food Facts","text":""},{"location":"docker/#technology-stack","title":"Technology stack","text":"<p>See also:  Continous Integration and Continuous Delivery</p>"},{"location":"docker/#docker-idempotency","title":"Docker (idempotency)","text":"<p>The process of dockerizing applications is an important step towards achieving a modern-day great Continuous Integration and Continous Delivery process.</p> <p>Dockerization avoids common pitfalls in deployment processes, such as having to write idempotent deployment scripts to deploy an application. A Docker container build can be run many times producing each time the same resulting image.</p> <p>Most of Open Food Facts git repositories have a <code>Dockerfile</code> that is used both to test changes locally, but also to ease automated testing and automated deployments through idempotency (a.k.a repeatabilty, or the ability to re-run a deployment X times without problems).</p>"},{"location":"docker/#docker-compose-orchestration","title":"Docker-Compose (orchestration)","text":"<p>We use <code>docker-compose</code> to deploy our applications to our servers: it is a simple orchestator that can deploy to a single machine at once.</p> <p>An alternative like <code>docker swarm</code> or <code>kubernetes</code> could be considered in the future to deploy to multiple machines at scale, but it currently does not make much sense considering the small amount of servers used to run Open Food Facts.</p>"},{"location":"docker/#env-file-secret-management","title":"Env file (secret management)","text":"<p>Every OFF repo has a <code>.env</code> file that contains the secrets needed by the application to run properly. The <code>.env</code> file is loaded by the <code>docker-compose</code> commands.</p> <p>The default <code>.env</code> file in the repo is ready for local development and should rarely be modified.</p> <p>In pre-production and production, the <code>.env</code> file is populated by the GitHub action (using GitHub environment secrets) before deploying to the target environment.</p> <p>Warnings: * The default <code>.env</code> file should rarely change. If you need a different environment locally, create a new env file (e.g <code>.env.test</code>) and set <code>ENV_FILE=.env.test</code> before running the <code>Makefile</code> commands.  * Do not commit your env files to the repos ! * you may use <code>direnv</code> to override some variables on a folder basis. See how-to for openfoodfacts-server</p>"},{"location":"docker/#best-practice-for-docker-containers","title":"Best Practice for Docker containers","text":"<p>Here are some important rules. The document also explain why we follow those rules. From time to time you might have good reason to bend or break the rules,  but only do it if needed. Rules also enables having a consistent experiences between projects.</p>"},{"location":"docker/#images","title":"Images","text":"<ul> <li>If possible use an official image. If you use another image take a look on how it's built.   It's important to be future proof and to be able to rely on a good base.</li> </ul> <ul> <li>We try to favor images based on debian, if really needed you can use arch or other architecture.   This is to keep consistent and manageable to admin and developers to debug images.</li> </ul>"},{"location":"docker/#enable-configuration-through-environment","title":"Enable configuration through environment","text":"<p>We really want to be able to run the same project multiple time on same machine / server. For that we need to ensure that we can configure the docker-compose project.</p> <p>You have two mechanism to configure the docker-compose: - docker-compose file composition, use it for structural changes - .env is the prefered way to change configuration (but can't solve it all)</p> <ul> <li>Avoid too generic name for services. Like <code>postgresql</code> it's better to use <code>myproject_db</code>.</li> <li>Every public network should have a configurable name.   To be able to run the project more than once, to also be able to connect docker-compose between them.</li> <li>Every port exposure should be changeable through env.   We want to be able to change port (run multiple time same project),   and to keep exposure to localhost on dev (avoid exposure on public wifi).</li> <li>Never use container_name (let docker-compose build the name)</li> <li>Never user static names for volumes, let docker-compose add a prefix</li> <li>try to stitch to the default network   and setup a network with a configurable name for exchanges with other projects services   (that is located in other docker-compose).</li> <li>restart directive should always be configurable.   While we want auto-start in production, we don't want it on dev machines.</li> <li>always prefer prod defaults for variable, or safe default.   For example it's better to only expose to localhost.   And if a variable is missing in prod it should never create a disaster.</li> </ul>"},{"location":"docker/#dev-config","title":"Dev config","text":"<p>The docker-compose.yml should be as close as possible to production.</p> <p>Put specific configurations in a docker/dev.yml</p> <ul> <li>The build part should only be in dev docker-compose.   (see why we use images only in prod)</li> <li>use a USER_UID / USER_GID parameter to align docker user with host user uid.   This avoid having problems with file permissions.</li> <li>bind mount code so that it's easy to develop.</li> <li>make it possible to connect the project between them on dev, as if it was on production.   This enables manual integration testing of all the project all together.</li> </ul>"},{"location":"docker/#prod-config","title":"Prod config","text":"<p>Here I talk about production, but staging is as much possible identical to prod.</p> <ul> <li>There should be no build in production, containers should be defined by their images.   We want to be able to redeploy easily only depending on the container registry, not external packages repositories and so on.</li> </ul> <ul> <li>every volume containing production data should be external (to avoid a <code>docker-compose down</code> fatality if <code>-v</code> is added). The Makefile should contain a creation target (<code>create_external_volumes</code>)</li> <li>shared network name should have a prefix which reflect the environment: like stagging / prod</li> <li>COMPOSE_PROJECT_NAME should use _: like po_stagging, po_prod, ...  <p>see also https://github.com/openfoodfacts/openfoodfacts-infrastructure/issues/146</p>"},{"location":"docker/#security","title":"Security","text":"<ul> <li>Try hard not to use root in docker images. (it's ok if root is only used to launch a service that immediately drops privileges)<ul> <li>for containers that contains code, or elements that are edited by developers and bind mounted at dev time</li> </ul> </li> <li>expose to localhost only whenever possible. Only expose to all interfaces when needed</li> <li>be aware that docker use an alternative table for ip tables.   A blocking INPUT or OUTPUT rule won't apply to docker exposed port.   You can instead add rules to DOCKER-USER chain.</li> </ul>"},{"location":"docker_architecture/","title":"Docker architecture","text":"<p>Below is a diagram of how the various OFF apps interact within the Docker environments:</p>"},{"location":"docker_architecture/#docker-server-for-staging","title":"Docker server for staging","text":"<p>The 200 VM on ovh2 is the serveur hosting the docker for stagging.</p>"},{"location":"docker_architecture/#docker-server-for-prod","title":"Docker server for prod","text":"<p>The 201 VM on ovh2 is the serveur hosting the docker for production.</p>"},{"location":"docker_architecture/#useful-commands","title":"Useful commands","text":"<p>List all mapped ports on a VM:</p> <pre><code>docker ps --format 'table {{.Names}}\\t{{.Ports}}'|grep '\\-&gt;'\n</code></pre>"},{"location":"docker_onboarding/","title":"Onboarding","text":"<p>Focusing on the developer onboarding process into a project is a very important step to make a repository popular: no one wants to struggle for hours to setup an application and start developing on it.</p> <p>A streamlined and easy setup process is thus critical to having meaningful contributions on an open-source repository.</p>"},{"location":"docker_onboarding/#rules-of-the-off-on-boarding-process","title":"Rules of the OFF on-boarding process","text":"<p>The setup steps should be as simple as possible and stick by the following rules:</p> <ul> <li>Respect the user's time:<ul> <li>reduce the amount of time needed to setup a project to the least amount possible.</li> <li>but also make it as fast as possible to take into account code modifications (ideally live reload, if needed a container restart)</li> </ul> </li> <li>Make it easy for non-developers to contribute: the dev setup should not require a high comprehension of the application at hand.</li> <li>Repeatable and tested developer workflow: the dev setup should be automated and tested with every pull request to ensure that it does not break accidentally when making changes.</li> <li>Make it possible to reach a wide audience:   try to make the dev deployment platform agnostic,   at least for commands used to develop in a normal process.   On windows, git comes with git bash which should be priviledge as a console. You can use symlink on windows.</li> </ul> <p>The <code>make dev</code> command should work across all repos to streamline the applications setup process.</p>"},{"location":"folksonomy/","title":"Folksonomy API","text":"<p>Folksonomy is a service to allow contributors to freely add labels and values to products.</p> <p>The code is at https://github.com/openfoodfacts/folksonomy_api/</p>"},{"location":"folksonomy/#deployment","title":"Deployment","text":"<p>Folksonomy is deployed on a LXC container. (108 at the time of writing)</p> <p>Code is in <code>/home/folksonomy/folksonomy_api</code></p> <p>It is started thanks to a systemd unit: <code>folksonomy.service</code> (config at <code>/etc/systemd/system/folksonomy.service</code>)</p> <p>Server is running uvicorn on port 8000 with user folksonomy.</p> <p>It is served behind the NGINX reverse proxy</p>"},{"location":"folksonomy/#useful-commands","title":"Useful commands","text":"<p>Status (reload/restart/etc.): <pre><code>systemctl status folksonomy\n</code></pre></p> <p>See service logs: <pre><code>sudo journalctl -u folksonomy\n</code></pre></p>"},{"location":"linux-server/","title":"Linux server","text":"<p>Here are some guidelines for linux servers.</p> <p>Note that we have some servers (which are bare metal installs. While others are proxmox hosts. On proxmox some VM are lxc containers, while other are QEMU VM.</p> <p>Every server is referenced in  CT and VM list of OFF infrastructure</p>"},{"location":"linux-server/#etckeeper","title":"Etckeeper","text":"<p>We use <code>etckeeper</code> with <code>git</code> backend on as much server as possible.</p> <p>See https://etckeeper.branchable.com/README/</p> <p>So whenever you make a change to <code>/etc</code>. When possible before making your change,  as root, do a <code>git status</code> and then <code>etckeeper commit \"save before changes\"</code>. And after, do a <code>etckeeper commit \"&lt;a descriptive message&gt;\"</code> afterwards.</p>"},{"location":"linux-server/#email","title":"Email","text":"<p>We use either postfix or exim as a satellite of a smart_host.</p> <p>Every outgoing mail must pass through the proxmox mail gateway, which is registered in spf record and adds DKIM signature.</p> <p>For configuration, see mail - Servers</p>"},{"location":"linux-server/#iptables","title":"Iptables","text":"<p>We use iptables on a lot of servers (generally host servers).</p> <p>We use iptables-persistent to save rules, and restore them at startup.</p> <p>On ovh servers, rules are in <code>/etc/iptables/rule.v{4,6}</code> On free servers, rules are in <code>/etc/iptables.up.rules</code></p> <p>Remember, that docker as it's own chains that are not affected by <code>INPUT</code> and <code>OUTPUT</code> rules. So it won't block a port exposed by docker. Use <code>DOCKER-USER</code> chain for that. see https://docs.docker.com/network/iptables/</p>"},{"location":"mail/","title":"Mail on Open Food Facts infrastructure","text":"<p>Because mail is difficult to setup, we use Proxmox Mail Gateway as a relay to all servers. It ensure correct SPF, but also it adds DKIM signature.</p> <p>No server receive mail, but they should be able to send them.</p>  <p>\ud83d\udcddNote: We ONLY support emails address on primary domain (<code>openfoodfacts.org</code>) and we DO NOT support emails on sub domains (aka <code>xxx.openfoodfacts.org</code>).</p>"},{"location":"mail/#proxmox-mail-gateway","title":"Proxmox Mail Gateway","text":"<p>This is the <code>pmg</code>  lxc VM (aka <code>102</code>) currently on <code>ovh1.openfoodfacts.org</code>. The install follows procedure starting from a debian distribution.</p>"},{"location":"mail/#administration","title":"Administration","text":"<p>You can access the administration of pmg by using a tunnel on ovh1:</p> <p><pre><code>ssh  -L 8006:10.1.0.102:8006 ovh1.openfoodfacts.org -N\n</code></pre> then connect to https://localhost:8006 (beware the s of https !)</p> <p>You must have an account to connect to the service.</p> <p>The most important tools you will find is Administration where you can see queues of deferred mails or logs (on the administration entry).</p> <p>See https://pmg.proxmox.com/pmg-docs/pmg-admin-guide.html#_administration</p>"},{"location":"mail/#notable-configurations-options","title":"Notable configurations options","text":"<p>First bare in mind that even if proxmox mail gateway is built to handle outgoing and incoming emails, we want to use it only for outgoing emails.</p> <p>We registered an letsencrypt account with tech@openfoodfacts.org</p> <p>Administrator email is tech@openfoodfacts.org</p> <p>Relay is set to smtp-relay.gmail.com, although we should not need it (this is for incoming mails).</p> <p>SMTP private port is set to <code>25</code> and public port to <code>26</code></p> <p>In networks tab, there are all servers ips, though because of redirects, PMG sees each incoming request as issued by <code>10.1.0.1</code>.</p> <p>TLS (TLS tab) is enabled</p> <p>DKIM is configured with label <code>pmg-openfoodfacts</code>, with a key size of 2048, the public key was reported in DNS configuration</p> <p>see also installation report</p>"},{"location":"mail/#redirects","title":"redirects","text":"<p>On DNS: * <code>pmg.openfoodfacts.org</code> is a CNAME to <code>ovh1.openfoodfacts.org</code> * <code>ovh1</code> ip address is reported in the spf entry of the DNS * there is an entry for the dkim key</p> <p>An iptable rule on host (<code>ovh1</code>) redirects our server incoming ips to the VM smtp port (25). Private network address (corresponding to VM and docker ranges) are also redirected.</p> <p>To add a new machine <pre><code>sudo iptables -t nat -A PREROUTING -s 213.36.253.206,213.36.253.208,146.59.148.140,51.210.154.203,1.210.32.79 -d pmg.openfoodfacts.org -p tcp  --dport 25 -j DNAT --to 10.1.0.102:25\n</code></pre> don't forget to save iptables</p> <p>(a generic masquerading rule for VM also exists)</p> <p>Note that the port 25 is in fact the private (trusted) port (not the public one, as we are not receiving emails). This is a tweak in default config.</p> <p>Also the nginx reverse proxy (VM <code>101</code> on <code>ovh1</code>) proxies requests to <code>pmg.openfoodfacts.org</code> on port 80, to the proxmox mail gateway VM (<code>102</code>),  this is needed for certificate generation through letsencrypt by the gateway.</p>"},{"location":"mail/#testing-that-the-gateway-is-well-configured","title":"Testing that the gateway is well configured","text":"<p>To do a simple test, you may use:</p> <p><pre><code>echo \"Subject: mail gateway test\" | sudo sendmail -f alex@openfoodfacts.org -v alex@openfoodfacts.org\n</code></pre> directly on the <code>pmg</code> server.</p> <p>To test quality of the configuration we can send a mail to a service such as https://www.mail-tester.com/</p> <p>To test using this service: - I temporarily added an iptables rule to enable my personal ip address    to be forwarded to the mail gateway (as for servers) - I added pmg.openfoodfacts.org as an smtp server in thunderbird   and configured my personal address to use it - I sent a mail as requested by the service - I could then check the result - I undo iptables and thunderbird config</p>"},{"location":"mail/#servers","title":"Servers","text":"<p>On client servers (be it a VM, a host or a standalone server), we use either exim4 or postfix.</p>"},{"location":"mail/#email-aliases","title":"Email aliases","text":"<p>We normally keeps a standard <code>/etc/aliases</code>. We have specific groups to receive emails: <code>root@openfoodfacts.org</code> and <code>off@openfoodfacts.org</code></p> <p>You may add some redirections for non standard users to one of those groups. Do not forget to run <code>newaliases</code>, and <code>etckeeper</code>.</p>"},{"location":"mail/#postfix-configuration","title":"Postfix configuration","text":"<p>Run: <code>dpkg-reconfigure postfix</code>:</p> <ul> <li>configuration type : satellite system</li> <li>mail name: openfoodfacts.org</li> <li> <p>relayhost: pmg.openfoodfacts.org</p> <p>(with an exception for ovh1: 10.1.0.2) * mail for root: tech@openfoodfacts.org * other dest: blank * sync: no * local network: leave default * use procmail: no * default for the rest</p> </li> </ul>"},{"location":"mail/#exim4-configuration","title":"Exim4 configuration","text":"<p>Run: <code>dpkg-reconfigure exim4-config</code>:</p> <ul> <li>mail sent by smarthost; no local mail</li> <li>mail name: openfoodfacts.org</li> <li>listen: 127.0.0.1</li> <li>other dest: off1.free.org</li> <li>visible domain name : openfoodfacts.org</li> <li>IP address smarthost:  pmg.openfoodfacts.org</li> <li>keep DNS queries minimal (dial-up): no</li> <li>local: maildir format in home</li> <li>split config : no</li> </ul>"},{"location":"mail/#testing","title":"Testing","text":"<p>To test that mail is well configured, you can use:</p> <pre><code>echo \"Subject: sendmail test xxx\" | sudo sendmail -f alex@openfoodfacts.org -v root\n</code></pre> <p>or, with the <code>mailutils</code> package installed:</p> <pre><code>echo \"test message from xxx\" |mail.mailutils -s \"test root xxx\" -r alex@openfoodfacts.org root\n</code></pre> <p>If you do not receive the email on expected group, here are some checks: * look at logs on your server:   * <code>/var/log/exim/</code> if you use <code>exim4</code>   * <code>/var/log/mail*</code> if you use postfix * look at logs on pmg VM:   * <code>/var/log/mail*</code> * <code>ping pmg.openfoodfacts.org</code> to verify network * <code>nc -vz pmg.openfoodfacts.org 25</code> suceed, else you might have firewall problems:   verify iptables on your machine and on ovh1.   (nc command belongs to <code>netcat-traditional</code> or <code>netcat-openbsd</code> packages) * you can even try to send a mail manualy using smtp protocol using   <code>nc pmg.openfoodfacts.org</code> and following a tutorial about sending email with netcat   If it suceed this may mean you have a problem in username to email address translation. * <code>/etc/mailname</code> contains <code>openfoodfacts.org</code> (see also debian wiki) * check <code>/etc/aliases</code> and <code>/etc/email-adresses</code> if you use exim4</p>"},{"location":"mail/#references","title":"References","text":"<ul> <li>Debian reference</li> <li>Debian wiki Postfix</li> <li>Debian wiki Exim</li> </ul>"},{"location":"matomo/","title":"Matomo","text":"<p>Matomo is the web analytics platform.</p> <p>Available at: https://analytics.openfoodfacts.org/</p> <p>You must have a user account to access it (hopefully !). Ask for an admin to create you an account if you need it (Beware, there are personal information in the sense of GDPR like ip addresses). Ask for it to contact email.</p> <p>See also Install log</p>"},{"location":"matomo/#site-setup","title":"Site setup","text":"<ul> <li>goto manage / websites and add a website</li> </ul>"},{"location":"matomo/#gdpr","title":"GDPR","text":"<p>To be GDPR compliant (and user friendly) 1:</p> <ul> <li>in your Matomo Tag, you can check the option \u00ab Disable cookies \u00bb which will disable all first party tracking cookies for Matomo. 2</li> <li>To ensure that you do not store the visitor IP, which is Personally Identifiable Information (PII), please go to Administration &gt; Privacy &gt; Anonimyze data, to enable IP anonymization, and check you have 2 bytes or 3 bytes masked from the IP address. 3</li> </ul>"},{"location":"matomo/#in-productopener","title":"In productopener","text":"<p>We use the <code>$google_analytics</code> variable in config to add the javascript snippet for Matomo.</p>   <ol> <li> <p>https://fr.matomo.org/blog/2018/04/how-to-make-matomo-gdpr-compliant-in-12-steps/\u00a0\u21a9</p> </li> <li> <p>https://fr.matomo.org/faq/general/faq_157/\u00a0\u21a9</p> </li> <li> <p>https://matomo.org/faq/general/configure-privacy-settings-in-matomo/#step-1-automatically-anonymize-visitor-ips\u00a0\u21a9</p> </li> </ol>"},{"location":"nginx-reverse-proxy/","title":"NGINX Reverse proxy (OVH)","text":"<p>At OVH we have a lxc container dedicated to reverse proxy http/https applications.</p>"},{"location":"nginx-reverse-proxy/#network-specific-interface","title":"Network specific interface","text":"<p>It as a specific network configurations with two ethernet address: * one internal, to communicate with other VMs * one which is bridged on host network card, with ip fail over mechanism.</p> <p>Important: only the public ip should have a gateway 1</p>"},{"location":"nginx-reverse-proxy/#configuring-a-new-service","title":"Configuring a new service","text":"<p>To make a new service, hosted on proxmox, available you needs to:</p> <ul> <li>have this service available on proxmox internal network</li> <li>in the DNS, CNAME you service name to <code>proxy1.openfoodfacts.org</code></li> <li>write a configuration on nginx for this service</li> <li>eventually add https</li> </ul>"},{"location":"nginx-reverse-proxy/#steps-to-create-nginx-configuration","title":"Steps to create Nginx configuration","text":"<p>we will imagine we configure my-service.openfoodfacts.net</p> <p>You will have to be root to do that.</p> <p>Login on container (101) and start a root bash.</p> <p>Create the basic configuration file for your service in <code>/etc/nginx/conf.d</code> named <code>my-service.openfoodfacts.net.conf</code> on machine <code>222</code>, port <code>8888</code>.</p> <p>Important: your file has to ends with <code>.conf</code> to be taken into account.</p> <p>It's a good idea to first test it exists, using nc and curl:</p> <pre><code>nc -vz 10.1.0.222 8888\ncurl  http://10.1.0.222:8888\n</code></pre> <p>Create a config, say:</p> <pre><code>server {\n\n    listen 80;\n    listen [::]:80;\n    server_name  my-service.openfoodfacts.net;\n\n    access_log  /var/log/nginx/my-service.off.net.log  main;\n    error_log   /var/log/nginx/my-service.off.net.err;\n\n    location / {\n        proxy_pass http://10.1.0.222:8888$request_uri;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto https;\n        proxy_read_timeout 90;\n        client_max_body_size 512M;\n    }\n\n}\n</code></pre> <p>If you need more nginx docs are here</p> <p>We test nginx configuration is ok (Mandatory)2:</p> <pre><code>$ nginx -t\nnginx: the configuration file /etc/nginx/nginx.conf syntax is ok\nnginx: configuration file /etc/nginx/nginx.conf test is successful\n</code></pre> <p>if it's ok, we reload. <pre><code>$ systemctl reload nginx\n</code></pre></p> <p>Next step is probably to setup https (putting something in http should be an exception, with good reason for that !) Otherwise jump to EtcKeeper</p>"},{"location":"nginx-reverse-proxy/#adding-https","title":"Adding https","text":"<p>Most of the time https certificates are managed on the nginx reverse proxy VM. Here is how to configure them to enable https.</p> <p>We use certbot to manage certificates.</p> <p>First prepare the service definition to answer on port 443 and 80. That is:</p> <ul> <li>change your current configuration to listen on 443:   <pre><code>{\n  server {\n\n  listen 443;\n  listen [::]:443;\n  server_name  my-service.openfoodfacts.net;\n  \u2026\n</code></pre></li> <li>but after it, add a new bare section for port 80:   <pre><code>server {\n  listen 80;\n  listen [::]:80;\n  server_name  my-service.openfoodfacts.net;\n}\n</code></pre></li> </ul> <p>We will first validate our config using <code>--test-cert</code> option (WARNING only skip this part if you really are used to certbot and nginx configuration, as after five tentatives, we won't be able to renew any certificates on the domain for one full week, and if you do too much error, the IP itself might be out of limit, see letsencrypt Rate limits)</p> <pre><code>$ certbot --test-cert -d my-service.openfoodfacts.net\n\u2026\nEnter email address (used for urgent renewal and security notices) (Enter 'c' to\ncancel): root@openfoodfacts.org\n\u2026\nterms of service\u2026\n(A)gree/(C)ancel: A\n\u2026\nshare email\u2026\n(Y)es/(N)o: n\n\nObtaining a new certificate\nPerforming the following challenges:\nhttp-01 challenge for my-service.openfoodfacts.net\nWaiting for verification...\nCleaning up challenges\nDeploying Certificate to VirtualHost /etc/nginx/conf.d/my-service.openfoodfacts.net.conf\n</code></pre> <p>Note: verify it's the right file which has been impacted ! If it's not you may have the option to restore the file with <code>git checkout wrong-touched-file</code> (but look before with <code>git status</code>)</p> <p>Test it's working (apart from the security alert in your browser, because certificate is from an unknown issuer).</p> <p>Then install the real certificate (you get the first section if you first did a test certificate):</p> <pre><code>$ certbot -d my-service.openfoodfacts.net\n1: Attempt to reinstall this existing certificate\n2: Renew &amp; replace the cert (limit ~5 per 7 days)\nSelect \u2026 [enter] (press 'c' to cancel): 2\n\u2026\nEnter email address (used for urgent renewal and security notices) (Enter 'c' to\ncancel): root@openfoodfacts.org\n\u2026\nterms of service\u2026\n(A)gree/(C)ancel: A\n\u2026\nshare email\u2026\n(Y)es/(N)o: n\n\nObtaining a new certificate\nPerforming the following challenges:\nhttp-01 challenge for my-service.openfoodfacts.net\nWaiting for verification...\nCleaning up challenges\nDeploying Certificate to VirtualHost /etc/nginx/conf.d/my-service.openfoodfacts.net.conf\n</code></pre> <p>Test again if it's working</p>"},{"location":"nginx-reverse-proxy/#multiple-domains","title":"Multiple domains","text":"<p>If you're in the case where same configuration must serve multiple domains, like ui.my-site.openfoodfacts.net and api.my-site.openfoodfacts.net, simply add all those domain to the certbot command with the <code>-d</code> parameters.</p> <p>For example:</p> <pre><code>certbot -d ui.my-site.openfoodfacts.net -d api.my-site.openfoodfacts.net -d my-site.openfoodfacts.net\n</code></pre> <p>If a certificate already exists for a domain, certbot will propose to extend it with the other domains.</p>"},{"location":"nginx-reverse-proxy/#etc-keeper","title":"Etc Keeper","text":"<p>We use etckeeper Do not forget to commit your changes:</p> <pre><code>etckeeper commit -m \"Configured my-service.openfoodfacts.net\"\n</code></pre> <p>Now we are done \ud83c\udf89</p>   <ol> <li> <p>The default proxmox interface does not offer options to indicate which gateway should be the default gateway, and the public ip needs to have its gateway as the default one, and there is no trivial way to achieve this reliably and elegantly, thus the best solution is to have only one gateway. See also ovh reverse proxy incident of 2022-02-18 \u21a9</p> </li> <li> <p>the nginx script will normally do the check before trying to restart nginx, but this way you are able to also see warnings.\u00a0\u21a9</p> </li> </ol>"},{"location":"observability/","title":"Observability","text":"<p>This document describes the observability stack used at Open Food Facts to monitor applications. </p> <p>Having a good observability stack is critical to spend less time when debugging failures, to have a comprehension of how applications behave over time, and to have the ability to compare a software version with the previously deployed one.</p> <p>The observability stack used in the OFF stack is comprised of the following applications:</p> <ul> <li>Filebeat as a logs collection agent deployed on each QEMU VM with Docker containers.</li> </ul> <ul> <li>ElasticSearch for centralized storage and indexing of logs collected from Docker.</li> </ul> <ul> <li>Kibana UI to visualize and use logs collected by ElasticSearch. (official doc)</li> </ul> <ul> <li>Prometheus for scraping metrics from Prometheus exporters' <code>/metrics</code> endpoint, running as sidecar containers of the applications.</li> </ul> <ul> <li>AlertManager to send alerts based on Prometheus metrics, integrated with dedicated Slack channels.</li> </ul> <ul> <li>InfluxDB is the storage backend for data harvested by prometheus</li> </ul> <ul> <li>Grafana for visualizing Prometheus metrics, InfluxDB and other metrics; and create dashboards. (official doc)</li> </ul> <ul> <li>Prometheus exporters such as the Apache Prometheus Exporter, which collect metrics from applications and expose them on a port in the Prometheus metric format. Some applications natively export Prometheus metrics and do not need additional exporters.</li> </ul> <p>The observability stack diagram is as follows:</p>"},{"location":"odoo/","title":"Odoo: our relationship management (connect.openfoodfacts.org)","text":"<p>Odoo is a very rich tool: we need to think a little bit before doing things. Some actions can't be canceled; for example: some modules, when installed, can't be removed.</p> <p>Quick guidelines to follow: 1. Technical and functional admins rights should be separated. 2. Usages should be clearly expressed and modules should be discussed before implemented. An issue have to be open before each module or group of module installation. 3. Tests need to be made in a staging environement.</p>"},{"location":"odoo/#install","title":"Install","text":"<p>The current test instance (Odoo 15) have been installed with the following commands.</p> <ul> <li>Install Debian 11</li> <li><code>apt install postgresql -y</code></li> <li><code>apt install wkhtmltopdf</code></li> <li><code>apt install gnupg gnupg1 gnupg2</code></li> <li><code>wget -O - https://nightly.odoo.com/odoo.key | apt-key add -</code></li> <li><code>echo \"deb http://nightly.odoo.com/15.0/nightly/deb/ ./\" &gt;&gt; /etc/apt/sources.list.d/odoo.list</code></li> <li><code>apt-get update &amp;&amp; apt-get install odoo</code></li> <li>Setup Nginx proxy</li> </ul>"},{"location":"odoo/#startstop","title":"Start/stop","text":"<pre><code>sudo systemctl start odoo # (starts service)\nsudo systemctl stop odoo # (stops service)\nsudo systemctl restart odoo # (restarts Service)\nsudo systemctl status odoo # (status of service)\nsudo systemctl enable odoo # (starts service at boot)\nsudo systemctl disable odoo # (disables service at boot)\n</code></pre>"},{"location":"odoo/#install-a-new-module","title":"Install a new module","text":"<p>Examples: <pre><code>cd /usr/lib/python3/dist-packages/odoo/addons\nwget https://apps.odoo.com/loempia/download/formio/15.0/formio.zip\nunzip formio.zip\n</code></pre></p> <pre><code>cd /usr/lib/python3/dist-packages/odoo/addons\nwget https://apps.odoo.com/loempia/download/mass_editing/15.0/mass_editing.zip\nunzip formio.zip\n</code></pre>"},{"location":"odoo/#contributed-modules-from-oca-store","title":"Contributed modules from OCA store","text":"<p>OCA hosts hundreds of modules. Those ones are disseminated into dozens of git repositories. For example, the Mass Editing module can be find inside the https://github.com/OCA/server-ux repository.</p> <p>Note: In github, always search for the 15.0 branch (if 15.0 is your version) because the front page might be that of 14.0 and mislead you on supported versions.</p> <p>In this case we will add all addons of the repository:</p> <pre><code>cd /usr/lib/python3/dist-packages/odoo/addons\n# for odoo version 15.0: adapt according to desired version\ngit clone https://github.com/OCA/server-ux --branch 15.0\n</code></pre> <p>Update the addon path in your <code>/etc/odoo/odoo.conf</code> file to add our new directory <pre><code>addons_path = /usr/lib/python3/dist-packages/odoo/addons,/usr/lib/python3/dist-packages/odoo/addons/server-ux\n</code></pre></p> <p>Look at the <code>__manifest__.py</code>, for the product you want to install, and see if there are python modules to be installed (python dependencies). Also do that for any product, it depends on.</p> <p>Restart Odoo:</p> <pre><code>systemctl restart odoo\n</code></pre> <p>Then, as an admin, in Odoo: * pass in developer mode (ctrl+K debug:) * Apps menu * <code>Update Apps List</code> sub-menu * then find the app in the <code>search</code> field (eventually remove the \"app\" filter if you installed a utility)</p>"},{"location":"odoo/#create-a-test-environment-from-production-instance","title":"Create a test environment from production instance","text":"<pre><code># previously verify that CT 112 is currently connect-staging, and delete it before renewing it\npct snapshot 110 temp # create a \"temp\" named snapshot of CT with ID 120 (production)\npct clone 110 112 --hostname connect-staging --snapname temp # take the snapshot and create a new CT (112) named connect-staging\npct delsnapshot 110 temp # del production snapshot\n# New CT configuration\npct set 112 --cores 2 --memory 4096 --net0 name=eth0,bridge=vmbr0,gw=10.0.0.1,ip=10.1.0.112/24\npct start 112\n# pct exec 112 .................. # a way to execute things on a CT from host\n\n# TO BE CONTINUED\n</code></pre>"},{"location":"producers_sftp/","title":"Producers SFTP","text":"<p>We have a producer SFTP which is part of the producer platform.</p> <p>This sftp is used by producers who send files for regular automated updates of their products.</p> <p>The sftp is located on off1.openfoodfacts.org</p> <p>The <code>/home/sftp</code> folder links to <code>/srv/sftp/</code> and contains home for sftp users.</p>"},{"location":"producers_sftp/#adding-a-new-sftp-user","title":"Adding a new sftp user","text":"<p>Use the script <code>add_sftp_user.pl</code> (present in <code>/home/script</code>) with user root.</p>"},{"location":"promox/","title":"Proxmox","text":"<p>On ovh1 and ovh2 we use proxmox to manage VMs.</p> <p>TODO this page is really incomplete !</p>"},{"location":"promox/#http-reverse-proxy","title":"HTTP Reverse Proxy","text":"<p>The VM 101 is a http / https proxy to all services.</p> <p>It has it's own bridge interface with a public facing ip.</p> <p>See Nginx reverse proxy</p>"},{"location":"promox/#unlocking-a-vm","title":"Unlocking a VM","text":"<p>Sometimes you may get alerts in email telling backup failed on a VM because it is locked. (<code>CT is locked</code>)</p> <p>This might be a temporary issue, so you should first verify in proxmox console if it's already resolved.</p> <p>If not you can unlock it using this command:</p> <pre><code>pct unlock &lt;vm-id&gt;\n</code></pre>"},{"location":"promox/#storage","title":"Storage","text":"<p>We use two type of storage: the NVME and zfs storage. There are also mounts of zfs storage from ovh3.</p> <p>TODO tell much more</p>"},{"location":"promox/#adding-space-on-a-qemu-disk","title":"Adding space on a QEMU disk","text":"<p>following https://pve.proxmox.com/wiki/Resize_disks</p> <p>Example: adding 8GB on VM for monitoring (203) and disk scsi0</p> <p>On host: <code>sudo qm resize 203 scsi0 +8G</code></p> <p>On VM: <pre><code>sudo parted /dev/sda\n(parted) print\nModel: QEMU QEMU HARDDISK (scsi)\nDisk /dev/sda: 42,9GB\nSector size (logical/physical): 512B/512B\nPartition Table: msdos\nDisk Flags: \n\nNumber  Start   End     Size    Type      File system     Flags\n 1      1049kB  33,3GB  33,3GB  primary   ext4            boot\n 2      33,3GB  34,4GB  1022MB  extended\n 5      33,3GB  34,4GB  1022MB  logical   linux-swap(v1)\n (parted) quit\n</code></pre></p> <p>We have a problem because of swap. We must deactivate swap, remove swap partition and extended partition, augment our main partition, recreate extended and swap partition !</p> <pre><code># deactivate swap\n$ sudo swapoff -a\n# remove partition in parted, augment main partition, recreate them\n$ sudo parted /dev/sda\n(parted) rm 5\n(parted) rm 2\n(parted) print\n...\nNumber  Start   End     Size    Type     File system  Flags\n 1      1049kB  33,3GB  33,3GB  primary  ext4         boot\n...\n(parted) help resizepart\n(parted) resizepart 1 41,3GB\n(parted) mkpart extended\nStart? 41,3GB\nEnd? 100%\n(parted) mkpart logical linux-swap 41,3GB 100%\n(parted) quit\n# resize partition\n$ sudo resize2fs /dev/sda1\nresize2fs 1.46.2 (28-Feb-2021)\nFilesystem at /dev/sda1 is mounted on /; on-line resizing required\nold_desc_blocks = 4, new_desc_blocks = 5\nThe filesystem on /dev/sda1 is now 10082751 (4k) blocks long.\n</code></pre> <p>Now we have to re-enable swap, but UUID for partition have changed, so we have to edit <code>/etc/fstab</code> first.</p> <pre><code># prepare swap\n$ sudo mkswap /dev/sda5\nSetting up swapspace version 1, size = 1,5 GiB (1648357376 bytes)\nno label, UUID=431b8e8e-0691-471c-be8b-2c1039321142\n# edit /etc/fstab to point to new UUID\n$ sudo vim /etc/fstab\n...\n# swap was on /dev/sda5 during installation\nUUID=431b8e8e-0691-471c-be8b-2c1039321142 none            swap    sw              0       0\n/dev/sr0        /media/cdrom0   udf,iso9660 user,noauto     0       0\n...\n# tell systemd\n$ sudo systemctl daemon-reload\n# remount swap\nsudo swapon -a\n</code></pre>"},{"location":"promox/#creating-a-new-vm","title":"Creating a new VM","text":"<p>TODO (see wiki page)</p>"},{"location":"promox/#loggin-to-a-container-or-vm","title":"Loggin to a container or VM","text":"<p>Most of the time we use ssh to connect to containers and VM.</p> <p>The mkuser script helps you create users using github keys.</p> <p>For the happy few sudoers on the host, they can attach to containers using <code>lxc-attach -n &lt;num&gt;</code> where <code>&lt;num&gt;</code> is the VM number. This gives a root console in the container.</p>"},{"location":"zammad/","title":"Zammad","text":"<p>Zammad is a tool for support.</p> <p>It is installed in a VM at CQuest home (we should migrate it), in an LXC container.</p> <p>It is exposed at https://support.openfoodfacts.org/</p> <p>It was setup using zammad package (see install docs).</p> <p>It has different services, all begining with the name <code>zammad-</code>. It uses postgresql.</p> <p>It also uses Elasticsearch. Because we are in a container, heap memory size has to be configured manually through a file in <code>/etc/elasticsearch/jvm.options.d/memory.options</code>. <code>600m</code> seems like a good size.</p>"},{"location":"reports/2021-02-22-matomo-install/","title":"Matomo install","text":"<p>see: https://github.com/openfoodfacts/openfoodfacts-infrastructure/issues/24</p>"},{"location":"reports/2021-02-22-matomo-install/#install-log","title":"Install log","text":"<p>Nginx and SSL configuration for https://analytics.openfoodfacts.org/:</p> <pre><code>$ ssh -i ~/.ssh/id_rsa -J CharlesNepote@ovh1.openfoodfacts.org CharlesNepote@10.1.0.101\n$ sudo cp /etc/nginx/conf.d/default.conf /etc/nginx/conf.d/analytics.conf\n$ sudo nano /etc/nginx/conf.d/analytics.conf\naccess_log  /var/log/nginx/analytics.access.log  main;\nserver_name  analytics.openfoodfacts.org;\n\n$ sudo systemctl restart nginx\n$ sudo certbot\nhttps://analytics.openfoodfacts.org/\n</code></pre> <p>make a snapshot before https://www.atechtown.com/install-nginx-and-php-on-debian-10/</p> <pre><code>$ ssh -i ~/.ssh/id_rsa -J CharlesNepote@ovh1.openfoodfacts.org CharlesNepote@10.1.0.107\n$ sudo apt install nginx php7.3-{fpm,cli,curl,gd,imap,json,mbstring,mysql,xml,zip} mariadb-server\n$ sudo chmod 755 -R /var/www/html/\n$ sudo chown www-data:www-data -R /var/www/html/\n\n\n$ sudo nano /etc/nginx/sites-available/default\nlocation ~ \\.php$ {\ninclude snippets/fastcgi-php.conf;\n</code></pre> <p>With php-fpm (or other unix sockets): <pre><code>fastcgi_pass unix:/var/run/php/php7.3-fpm.sock;\n# With php-cgi (or other tcp sockets):\n# fastcgi_pass 127.0.0.1:9000;\n}\n</code></pre> <pre><code>$ sudo systemctl restart nginx\n$ echo \"&lt;?php phpinfo(); ?&gt;\" | sudo -u www-data tee /var/www/html/test.php\nhttps://analytics.openfoodfacts.org/test.php\nsudo rm /var/www/html/test.php\n\n$ sudo systemctl enable mariadb\n$ sudo mysql_secure_installation\n# https://matomo.org/faq/how-to-install/faq_23484/\n$ sudo mysql -e \"CREATE DATABASE matomo_db;\"\n$ read matomopass # enter the matomo user password\n$ sudo mysql -e \"CREATE USER 'matomo'@'localhost' IDENTIFIED BY '$matomopass';\"\n$ sudo mysql -e \"GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, INDEX, DROP, ALTER, CREATE TEMPORARY TABLES, LOCK TABLES ON matomo_db.* TO 'matomo'@'localhost';\"\n$ sudo mysql -e \"GRANT FILE ON *.* TO 'matomo'@'localhost';\"\n$ unset matomopass\n</code></pre></p> <pre><code>$ cd /var/www/html\n$ sudo -u www-data wget https://builds.matomo.org/matomo.zip\n$ sudo apt install unzip\n$ sudo -u www-data unzip matomo.zip\n$ sudo sed -i \"s|root /var/www/html|/var/www/html/matomo|\" /etc/nginx/sites-available/default\n\nhttps://github.com/matomo-org/matomo-nginx/blob/master/sites-available/matomo.conf\n$ sudo sed -i \"s|index index.html index.htm index.nginx-debian.html;|index index.html index.htm index.nginx-debian.html index.php;|\" /etc/nginx/sites-available/default # non!\n$ sudo systemctl restart nginx\nhttps://analytics.openfoodfacts.org\n</code></pre>"},{"location":"reports/2021-02-22-matomo-install/#activate-reverse-proxy-mode-2022-05-09","title":"Activate reverse proxy mode (2022-05-09)","text":"<p>We have to configure matomo to take into account it is behind a reverse proxy.</p> <p>We follow https://matomo.org/faq/how-to-install/faq_98/</p> <p>And on matomo machine (container 107):</p> <ol> <li> <p>edit <code>/var/www/html/matomo/config/config.ini</code> to add :    <pre><code>[General]\n...\nforce_ssl = 1\nassume_secure_protocol = 1\n...\nproxy_client_headers[] = HTTP_X_FORWARDED_FOR\nproxy_host_headers[] = HTTP_X_FORWARDED_HOST\n</code></pre></p> </li> <li> <p>restart pfm to take this into account:    <pre><code>systemctl restart php7.3-fpm.service\n</code></pre></p> </li> </ol> <p>On proxy machine (container 101), we insure the header is passed by nginx:</p> <ol> <li> <p>edit <code>/etc/nginx/conf.d/analytics.conf</code> <pre><code>server {\n server_name  analytics.openfoodfacts.org;\n ...\n location / {\n     ...\n     proxy_set_header  X-Forwarded-For $remote_addr;\n</code></pre></p> </li> <li> <p>reload nginx config:    <pre><code>systemctl reload nginx\n</code></pre></p> </li> </ol> <p>Verify it's working by looking at real time traffic on matomo.</p>"},{"location":"reports/2021-02-22-matomo-install/#trigger-archival-offline-2022-05-10","title":"Trigger archival offline - 2022-05-10","text":"<p>By default matomo tries to run archival on analytics visit. But our website as a lot of analytics so it's important to run it on a regular basis and out of a request (which will timeout too soon).</p> <p>We follow https://matomo.org/faq/troubleshooting/faq_19489/ and thus https://matomo.org/faq/on-premise/how-to-set-up-auto-archiving-of-your-reports/</p> <p>On analytics (container 107):</p> <ul> <li>ensure mail is setup correctly on the server</li> </ul> <ul> <li>create directory for logs:   <pre><code>mkdir /var/log/matomo/\nchown www-data:www-data /var/log/matomo/\n</code></pre></li> </ul> <ul> <li> <p>edit  <code>/etc/cron.d/matomo-archive</code></p> <pre><code>MAILTO=\"root@openfoodfacts.org\"\n5 * * * * www-data /usr/bin/php /var/www/html/matomo/console core:archive --url=http://analytics.openfoodfacts.org/ &gt;&gt; /var/log/matomo/matomo-archive.log 2&gt;&gt;/var/log/matomo/matomo-archive-err.log\n</code></pre> </li> </ul> <ul> <li>add files to logrotate, by adding <code>/etc/logrotate.d/matomo</code> with   <pre><code>/var/log/matomo/*.log {\n        daily\n        missingok\n        rotate 14\n        compress\n        delaycompress\n        notifempty\n        create 0640 www-data adm\n        sharedscripts\n}\n</code></pre></li> </ul> <ul> <li>tweak <code>/etc/php/7.3/cli/php.ini</code> (this correspond to settings for php command), to have:   <pre><code>max_execution_time = 3000\n...\nmemory_limit = -1\n</code></pre></li> </ul> <p>In matomo (https://analytics.openfoodfacts.org), click on <code>Administration</code> \u2192 <code>System</code> \u2192 <code>General Settings</code>, and select:</p> <ul> <li>Archive reports when viewed from the browser: No</li> <li>Archive reports at most every X seconds : 3600 seconds</li> </ul>"},{"location":"reports/2021-02-22-matomo-install/#performance-settings","title":"Performance settings","text":"<p>I tweak <code>/etc/php/7.3/fpm/php.ini</code> to have:</p> <p><pre><code>max_execution_time = 120\n...\nmemory_limit = 250M\n</code></pre> then reload to take the change into account: <pre><code>systemctl reload php7.3-fpm.service\n</code></pre></p>"},{"location":"reports/2021-10-03-network-down/","title":"[Postmortem] OpenFoodFacts.net down (#1)","text":"<p>Date: 03/10/2021</p> <p>Authors: ocervello,  <p>Status: Complete, action items in progress</p> <p>Summary: openfoodfacts.net down after Docker storage driver configuration change. Proxmox containers <code>off-net</code>, <code>robotoff-net</code>, <code>robotoff-dev</code>, <code>mongo-dev</code>, and <code>monitoring</code> are unreachable.</p> <p>Impact: Integration tests failing on openfoodfacts-dart (example), pre-prod environment (openfoodfacts.net) down.</p> <p>Root Causes: Cascading failure probably due to Docker storage driver configuration change from <code>vfs</code> to <code>fuse-overlayfs</code>, and probably an incompatibility between LXC, Docker and <code>fuse-overlayfs</code>, causing containers to crash, and unable to SSH. Exact root cause is still unknown, as some containers using <code>fuse-overlayfs</code> have not crashed.</p> <p>Trigger: Unknown. First outage happened 3 days after the Docker storage driver change.</p>"},{"location":"reports/2021-10-03-network-down/#resolution","title":"Resolution","text":"<ul> <li>Short term: revert Docker storage driver configuration from <code>fuse-overlayfs</code> to <code>vfs</code>.</li> <li>Long term: run Docker containers in a QEMU host instead.</li> </ul> <p>Detection: message on Slack #infrastructure channel + openfoodfacts-dart integration tests failing with timeouts.</p>"},{"location":"reports/2021-10-03-network-down/#action-items","title":"Action Items","text":"Action Item Type Owner Status     Revert Docker storage driver to <code>vfs</code> mitigate olivier DONE   Snapshot off-net CT and start a new CT from the snapshot mitigate charles FAILED   Create vanilla CT with storage driver <code>vfs</code> and re-deploy openfoodfacts.net on it + ZFS Mounts + NGINX config change mitigate charles,olivier,stephane,christian IN PROGRESS   Open ticket to Proxmox forums to investigate the crash process charles TODO   Run all crashed Docker containers on QEMU VM for stability + ZFS mounts + NGINX configuration prevent olivier,charles,stephane,christian https://github.com/openfoodfacts/openfoodfacts-infrastructure/issues/62"},{"location":"reports/2021-10-03-network-down/#lessons-learned","title":"Lessons Learned","text":""},{"location":"reports/2021-10-03-network-down/#what-went-well","title":"What went well","text":"<ul> <li>Community + staff got quickly alerted of openfoodfacts.net being down</li> <li>Worked together to solve the issues</li> </ul>"},{"location":"reports/2021-10-03-network-down/#what-went-wrong","title":"What went wrong","text":"<ul> <li>No explicit alert message was sent to productopener-alerts Slack channel \u2192 need integration tests on openfoodfacts-server repository</li> <li>Too many CTs brought down simultaneously - should have done the storage engine change on only 1 host and wait</li> <li>Proxmox container cloning failed, increasing the ETTR (Estimated Time To Repair)</li> <li>Proxmox container failed to reboot, increasing the ETTR</li> <li>Too much noise on <code>productopener-alerts</code>, failed deployments were missed.</li> <li>Sysadmins were not aware about all the impacts of off-net downtime.</li> <li>No single point to track the investigation and resolution (e.g. GitHub issue)</li> </ul>"},{"location":"reports/2021-10-03-network-down/#where-we-got-lucky","title":"Where we got lucky","text":"<ul> <li>Did not bring down production as it is still running on the Free machines.</li> <li>Automated deployments allowed us to re-deploy openfoodfacts.net pretty fast</li> <li>The right people were available.</li> </ul>"},{"location":"reports/2021-10-03-network-down/#what-we-learned","title":"What we learned","text":"<ul> <li>Assuming the root cause is correct: Proxmox LXC + Docker + ZFS + fuse-overlayfs storage driver can trigger severe issues where even Proxmox administration tools do not work (clones, snapshots, etc\u2026)</li> </ul>"},{"location":"reports/2021-10-03-network-down/#timeline","title":"Timeline","text":""},{"location":"reports/2021-10-03-network-down/#29-09-2021-all-times-cest","title":"29-09-2021 (All times CEST)","text":"<ul> <li>15:46 ROOT CAUSE \u2014 Docker storage driver switch from <code>vfs</code> to <code>fuse-overlayfs</code> made on all CTs w/ Docker deployments.</li> </ul>"},{"location":"reports/2021-10-03-network-down/#03-10-2021-all-times-cest","title":"03-10-2021 (All times CEST)","text":"<ul> <li>02:17 OUTAGE BEGINS \u2014 Automated message on #infrastructure-alerts Slack channel about timeouts when trying to access world.openfoodfacts.net</li> <li>19:16 OUTAGE BEGINS \u2014 Manual message by contributor on #infrastructure Slack channel about timeouts when trying to access world.openfoodfacts.net</li> </ul>"},{"location":"reports/2021-10-03-network-down/#04-10-2021-all-times-cest","title":"04-10-2021 (All times CEST)","text":"<ul> <li>9:23 Message on #infrastructure Slack channel that multiple containers are unresponsive.</li> </ul>"},{"location":"reports/2021-10-03-network-down/#06-10-2021-all-times-cest","title":"06-10-2021 (All times CEST)","text":"<ul> <li>14:36 OUTAGE MITIGATED, deployed openfoodfacts.net and a new machine. Mounts are still missing on disk.</li> <li>14:45 Decision taken to switch Docker containers to QEMU VM.</li> <li>15:36 Creation of QEMU VM 128GB RAM, 8 cores, 196GB drive.</li> </ul>"},{"location":"reports/2021-10-03-network-down/#07-10-2021-all-times-cest","title":"07-10-2021 (All times CEST)","text":"<ul> <li>09:00 Starting to manually deploy openfoodfacts-server, robotoff, robotoff-ann and monitoring containers on QEMU VM</li> <li>09:30 Openfoodfacts server is deployed on QEMU VM</li> <li>10:20 Robotoff deployment is blocked by a CPU flag issue (avx flag needed for Tensorflow library)</li> </ul>"},{"location":"reports/2021-10-03-network-down/#supporting-information","title":"Supporting information:","text":"<ul> <li>Document  a clear realistic \u201cacceptable downtime\u201d for each CT/VM/machines we manage (using the existing spreadsheet).</li> <li>Document the main owner and his/her co-owner (?) of each machine, ie people able to restore a service within the \u201cacceptable downtime\u201d and owning this responsibility.</li> <li>Decide how we document the infrastructure (not well decided yet).</li> <li>Is it possible to publish only real alerts in #infrastructure-alerts? Eg, only publish alerts if the machine is down for more than 15 minutes. Most of the alerts seems to be false positives.</li> <li>Define a process to resolve future incidents (e.g. should we systematically file a github issue for each incident?)</li> </ul>"},{"location":"reports/2021-12-21-disk-extension/","title":"Disk extension for preprod and containers prod (ovh2)","text":"<p>We got alerts on low disk space on preprod VM (dockers (200) on ovh2)</p> <p>We added disk space in proxmox to this machine and the container machine but we still add to make it available on the system side.</p> <p>Operation was done on ovh2, VM dockers (200) and VM dockers-prod (201)</p>"},{"location":"reports/2021-12-21-disk-extension/#snapshoting","title":"Snapshoting","text":"<p>Before this delicate operation, we snapshoted the VM in proxmox.</p>"},{"location":"reports/2021-12-21-disk-extension/#extending-partition-size","title":"Extending partition size","text":"<p>This is the commands we run to make it happens.</p> <p>Install tools:</p> <pre><code>apt install parted etckeeper\n</code></pre> <p>The swap partition is after the main partition, so we will have to remove it, resize main partition and recreate it.</p> <p>Turn swap off: <pre><code>swapoff -a\n</code></pre></p> <p>Edit partition.</p> <pre><code>parted /dev/sda\n# change unit to sectors\n(parted) u\nUnit?  [compact]? s\n# print partition table\n(parted) p\nNumber  Start   End    Size    Type      File system     Flags\n 1      1049kB  274GB  274GB   primary   ext4            boot\n 2      274GB   275GB  1022MB  extended\n 5      274GB   275GB  1022MB  logical   linux-swap(v1)\n# note tha swap partition is to 534872062 536868863 (1996802 sectores in size)\n# remove swap partitions\n(parted) rm 5\n(parted) rm 2\n# print to check\n(parted) p\nNumber  Start  End         Size        Type     File system  Flags\n 1      2048s  534870015s  534867968s  primary  ext4         boot\n# resize partition 1, leaving space for swap\n(parted) resizepart 1 -1996801\nWarning: Partition /dev/sda1 is being used. Are you sure you want to continue?\nYes/No? y\n# print\number  Start  End         Size        Type     File system  Flags\n 1      2048s  627148799s  627146752s  primary  ext4         boot\n# recreate swap (Start is end of part 1 + 1)\n(parted) mkpart\nPartition type?  primary/extended? primary\nFile system type?  [ext2]? linux-swap\nStart? 836864000\nEnd? -1s\n(parted) quit\n</code></pre>  <p>:pencil: Note: 1. we switch units to sectors because    it helps having better aligned partitions    (by multiples of 2048 in this case) 2. The swap partition was on an extended partition,    but we put it back as a simple partition</p>  <p>We recreated swap, now we have to format it: <pre><code>mkswap /dev/sda2\n</code></pre></p> <p>show new uids: <pre><code>blkid\n\n/dev/sda1: UUID=\"082b4523-f4d6-4d39-b5dd-48c5bdba2541\" BLOCK_SIZE=\"4096\" TYPE=\"ext4\" PARTUUID=\"1a39b366-01\"\n/dev/sr0: BLOCK_SIZE=\"2048\" UUID=\"2021-08-14-10-10-00-00\" LABEL=\"Debian 11.0.0 amd64 n\" TYPE=\"iso9660\" PTUUID=\"3c15dbf8\" PTTYPE=\"dos\"\n/dev/sda2: UUID=\"f8e99f04-88eb-4550-9308-10a470175e45\" TYPE=\"swap\" PARTUUID=\"1a39b366-02\"\n</code></pre></p> <p>Change /etc/fstab accordingly (<code>sda1</code> UUID did not change, only swap changed):</p> <pre><code># / was on /dev/sda1 during installation\nUUID=082b4523-f4d6-4d39-b5dd-48c5bdba2541 /               ext4    errors=remount-ro 0       1\n# swap was on /dev/sda2 during installation\nUUID=f8e99f04-88eb-4550-9308-10a470175e45 none            swap    sw              0       0\n</code></pre> <p>We should have booked our operation in etckeeper  (not done on dockers-prod, etckeeper was not yet installed) <pre><code>etckeeper commit \"changed partition size\"\n</code></pre></p> <p>Now we resized the partition, but we have to resize the filesystem. Let's resize sda1: <pre><code>resize2fs /dev/sda1\n</code></pre></p> <p>Reactivate swap: <pre><code>swapon -a\n</code></pre></p> <p>Let's see our changes:</p> <pre><code>df -h /\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/sda1       294G  187G   93G  67% /\n</code></pre>"},{"location":"reports/2021-12-22-preprod-crash/","title":"Preprod crash 2021 12","text":""},{"location":"reports/2021-12-22-preprod-crash/#what-happens","title":"What happens","text":"<ul> <li>zfs storage on OVH2 was full</li> <li>this blocks the restart of two VM : dockers (200) and dockers-prod (201)</li> </ul>"},{"location":"reports/2021-12-22-preprod-crash/#why-this-happens","title":"Why this happens","text":"<ul> <li>probably because of resize of partition the day before on both machine,   this makes snapshot diverge and take much more space.   (see 2021-12-21-disk-extension)</li> <li>out of space on zfs, blocks machine because proxmox needs to snapshot memory</li> </ul>"},{"location":"reports/2021-12-22-preprod-crash/#what-was-done","title":"What was done","text":"<p>The same day:</p> <ul> <li>rollback VM dockers (200) to its snapshoted version</li> <li>removed dockers-prod (201) snapshot</li> <li>extension of disk space for this VM dockers (200) was reduced from 400G to 300G (this is still a 50G improvment over the previous size for dockers VM)</li> </ul> <p>The day after:</p> <ul> <li>removed the efficientnet.tar.gz in /home/off/robotoff-ann-net as it was already untared in the ann_data folder</li> <li>hard reboot of VM dockers (200)</li> <li>resized partition to 300G following previous operating mode</li> </ul>"},{"location":"reports/2022-02-18-ovh-reverse-proxy-down/","title":"[Postmortem] Reverse proxy down","text":"<p>Date: 2022-02-18</p> <p>Authors: Alex alex@openfoodfacts.org</p> <p>Summary: wiki.openfoodfacts.net down after ovh1 reboot, as well as all services using a CNAME on proxy1.openfoodfacts.org or proxy.openfoodfacts.org.</p> <p>Impact: Tools / services unreachable : feedme, wiki, slack page, etc.</p> <p>Root Causes: Network configuration on proxy (VM 101) did not return to a correct state after reboot, because of two default gateway resulting in a race condition, and in an incorrect default gateway for public ip (fail over ip) of proxy.</p> <p>Trigger: Reboot of ovh1, to be able to cleanup some old VM, blocked by stalled processes (uninterruptible sleep)</p>"},{"location":"reports/2022-02-18-ovh-reverse-proxy-down/#diagnosis","title":"Diagnosis","text":"<p>We got a problem after rebooting ovh1 :\u00a0the proxy was not responding on it's public ip. On <code>proxy</code> VM aka <code>101</code>. We had two IPs, one internal (NAT mode) <code>eth0</code> (<code>10.1.0.101</code>) and one public (bridged) sharing a virtual bridge with host: <code>eth1</code> <code>193.70.55.124</code>. The problem was that nated requests (passing through ovh1 ip and forwarded by iptables) did work but not requests to public ip. * ping to ip did work * <code>nc -vz 193.70.55.124 80</code>  did succeed * <code>lsof -i tcp</code> in proxy did show nginx listening on all interfaces * but doing HTTP requests manually  using <code>nc</code>, did not succeed (it was working only from inside the VM)</p> <p>proxy: <pre><code>root@proxy:/etc/nginx# nc -v 193.70.55.124 80\nproxy1.openfoodfacts.org [193.70.55.124] 80 (http) open\nGET /\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;301 Moved Permanently&lt;/title&gt;&lt;/head&gt;\n</code></pre> my computer: <pre><code>alex@tignasse:~$ nc -v 193.70.55.124 80\nConnection to 193.70.55.124 80 port [tcp/http] succeeded!\nGET / HTTP/1.1\n</code></pre></p> <p>Long story short: the problem was that default route was passing through eth0, and thus packets did not return. <pre><code># ip route list\ndefault via 10.0.0.1 dev eth0\n10.0.0.1 dev eth0 scope link\n10.1.0.0/24 dev eth0 proto kernel scope link src 10.1.0.101\n193.70.55.1 dev eth1 scope link\n</code></pre> Using: <pre><code>ip route del default via 10.0.0.1 dev eth0\nip route add default via 193.70.55.1 dev eth1\n</code></pre> We get to: <pre><code># ip route list\ndefault via 193.70.55.1 dev eth1\n10.0.0.1 dev eth0 scope link\n10.1.0.0/24 dev eth0 proto kernel scope link src 10.1.0.101\n193.70.55.1 dev eth1 scope link\n</code></pre></p> <p>And everything works.</p>"},{"location":"reports/2022-02-18-ovh-reverse-proxy-down/#resolution-tentative-1","title":"Resolution tentative 1","text":"<p>The problem: proxmox do control the interfaces settings and in <code>/etc/network/interfaces</code>, and route are set there using <code>post-up</code> and <code>pre-down</code> rules. But as this part is managed by Proxmox, hence this is a bit hard to modify. And at boot time, eth cards are in competition to have their route the global default and eth0 wins over eth1, leading to the non working situation.</p> <p>I did try to add this a script in <code>/etc/network/if-up.d/90-insure-eth1-default-route</code> <pre><code>#!/usr/bin/env bash\n# this script tries to ensure defaults route is on eth1\n# this is important, otherwise apps served on  eth1 won't be able to respond to incoming packets\n# if eth1 is up\n# it should be in /etc/network/if-up.d/\n\n# ..note: this is a bit fragile for we hardcode the gateway here\n#    but greping for right gateway in interfaces seems a bit overkill..\n\nif ( ip address show eth1 | grep \"state UP\" )\nthen\n  # and default route not via eth1\n  if ! ( ip route show default | grep eth1 )\n  then\n    &gt;&amp;2 echo \"Setting up default route via eth1\"\n    # eventually remove eth0 route\n    ip route del default via 10.0.0.1 dev eth0\n    # add eth1 route\n    ip route add default via 193.70.55.1 dev eth1 \\\n      || &gt;&amp;2 echo \"Error while adding default route via eth1\"\n  fi\nfi\n</code></pre></p> <p>this is a bit complicated though and hard code ip, while we would have to keep them in sync with proxmox settings. Note: to use a script in <code>/etc/network/if-up.d/</code>, remember to <code>chmod +x</code> but also do not use any extension (a <code>.sh</code> extension makes run-parts ignore it !).</p>"},{"location":"reports/2022-02-18-ovh-reverse-proxy-down/#final-resolution","title":"Final Resolution","text":"<p>Christian (@cquest) proposes a more radical solution: remove gateway for eth0, and only keep it for eth1.</p> <p>This has the advantage of being driven from proxmox.</p> <p>Before: </p> <p>After: </p> <p>This works but then we cannot use forwarding requests through ovh1 ip + iptables. But that's fine, as this was not a portable configuration however (if we have to move proxy, there would have been no ip failover).</p> <p>Still we add the gateway for eth0 with ip route, until we moved all CNAME to the public ip proxy1.openfoodfacts.org.</p> <p>So a second action was to move CNAME for all services behind proxy from <code>ovh1.openfoodfacts.org</code> to <code>proxy1.openfoodfacts.org</code></p>"},{"location":"reports/2022-02-proxmox-mail-gateway-install/","title":"Install of proxmox gateway server","text":"<p>Note: Remember this documents keeps track of installation, it might not be complete or up-to-date</p> <p>See also: the mail documentation</p>"},{"location":"reports/2022-02-proxmox-mail-gateway-install/#proxmox-mail-gateway-install","title":"Proxmox Mail Gateway install","text":"<p>Following https://www.proxmox.com/en/proxmox-mail-gateway/get-started</p> <p>VM is already there as Christian as started</p> <p>I take at Install Proxmox Mail Gateway on Debian, see https://pmg.proxmox.com/pmg-docs/pmg-admin-guide.html#pmg_install_on_debian</p> <p>I started with apt update &amp;&amp; apt upgrade * I had to re-install package manager version of config file for clamav * I also installed vim ;-) * I edited <code>/etc/locale.gen</code> to add <code>fr_FR.UTF-8</code> then I run <code>locale-gen</code></p> <p>Then apt install <code>proxmox-mailgateway</code></p> <p>reading <code>/etc/network/interfaces</code> show me we have a static address, this is ok</p> <p><code>/etc/apt/sources.list.d/pmg-enterprise.list</code> is ok  section 3.5. Also gpg key is ok I skipped the non-free section, we do not care about <code>rar</code> files.</p> <p>To get graphical user interface: <code>ssh -v -L 8006:10.1.0.102:8006 ovh1.openfoodfacts.org</code> then https://localhost:8006/, I had to accept risk (auto-signed certificate) to proceed.</p> <p>I edit <code>/etc/pmg/user.conf</code> following section 16.3.</p> <p>first create a password hash: <pre><code>openssl passwd  -5  -stdin -salt xxxxxx\n******\n</code></pre> where <code>xxxx</code> was randomly generated (using <code>pwgen 8</code>) <code>****</code> is the password.</p> <p>Then add this line to <code>/etc/pmg/user.conf</code>: <pre><code>alex:1:0:$5$xxxxxx$xxxxxxxxxx:admin:alex@openfoodfacts.org:Alex:Garel::\n</code></pre></p> <p>\ud83d\udcdd\ufe0f Note: you need the '::' at the end contrarily of what's written in documentation (there is a column after keys) see bug https://bugzilla.proxmox.com/show_bug.cgi?id=3879</p> <p>\u2757\ufe0fWarning: '+' is not an acceptable sign in password for it will be converted to space by http !!!</p> <p>Following section 4.5.2</p> <p>Following section 4.6.3</p> <p>Prepared: - In ovh web DNS,    I registered <code>pmg.openfoodfacts.org</code> as a <code>CNAME</code> to <code>ovh1.openfoodfacts.org</code> - I changed the configuration on nginx proxy (lxc 101) to redirect to pmg (so that certbot could work) :</p> <pre><code>```\nroot@proxy:/etc/nginx/conf.d# cat pmg.openfoodfacts.org.conf\n# PMG stands for Promox Mail Gateway\n# We need to redirect port 80, for letsencrypt's certificate management\nserver {\n\n    listen 80;\n    listen [::]:80;\n    server_name  pmg.openfoodfacts.org;\n\n    access_log  /var/log/nginx/pmg.off.log  main;\n    error_log   /var/log/nginx/pmg.off_errors.log;\n\n    location / {\n        proxy_pass http://10.1.0.102:80$request_uri;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto https;\n        proxy_read_timeout 90;\n        client_max_body_size 512M;\n    }\n\n}\n\nroot@proxy:/etc/nginx/conf.d# nginx -t\nnginx: the configuration file /etc/nginx/nginx.conf syntax is ok\nnginx: configuration file /etc/nginx/nginx.conf test is successful\nroot@proxy:/etc/nginx/conf.d# systemctl reload nginx\n```\n</code></pre> <p>On the pmg machine: <pre><code>root@mail-gateway:/home/alex# pmgconfig acme account register default tech@openfoodfacts.org\nDirectory endpoints:\n0) Let's Encrypt V2 (https://acme-v02.api.letsencrypt.org/directory)\n1) Let's Encrypt V2 Staging (https://acme-staging-v02.api.letsencrypt.org/directory)\n2) Custom\nEnter selection: 0\n\nAttempting to fetch Terms of Service from 'https://acme-v02.api.letsencrypt.org/directory'..\nTerms of Service: https://letsencrypt.org/documents/LE-SA-v1.2-November-15-2017.pdf\nDo you agree to the above terms? [y|N]: y\n\nAttempting to register account with 'https://acme-v02.api.letsencrypt.org/directory'..\nRegistering new ACME account..\nRegistration successful, account URL: 'https://acme-v02.api.letsencrypt.org/acme/acct/403308140'\nTask OK\n</code></pre></p> <p>On the pmg web interface, in Certificates section, in ACME accounts / challenges I can see my account, http challenge plugin is already configured.</p>"},{"location":"reports/2022-02-proxmox-mail-gateway-install/#proxmox-mail-gateway-configuration","title":"Proxmox Mail Gateway configuration","text":"<p>In section Configuration, options tab, set Administrator Email to <code>tech@openfoodfacts.org</code>.</p> <p>Following 4.7.1 Relaying put default relay to <code>smtp-relay.gmail.com</code>. In our case, normally we should not relay (it's for incoming mails)</p> <p>Removed any relay domains</p> <p>Ports: I move public port to 24 and private port to 25, this is because we will only join PMV from trusted servers</p> <p>4.7.8 Networks: - In networks tab I put the address of off1 and off2 without any mask. - Same for ovh1/2/3. - I also added usual private networks: (see https://en.wikipedia.org/wiki/Reserved_IP_addresses): <code>10.0.0.0/8</code>, <code>172.16.0.0/12</code>, <code>192.168.0.0/16</code></p> <p>4.7.9 TLS - In TLS tab: I enables TLS</p> <p>4.7.10 DKIM - I added selector with <code>pmg-openfoodfacts</code> value, key size 2048 - Then I activated DKIM and tell to sign all outgoing mail - I clicked \"View DNS record\" - In ovh I then have to use the DKIM record type and use the form to create a similar looking entry:   checked version / Algorithm checked hash 256 / checked key type / public key (I add to join the string) / service type none / checked key valid for subdomains</p> <ul> <li>I added domain openfoodfacts.org (still in DKIM tab)</li> </ul> <p>At this point I did a backup (Configuration : backup / restore)</p> <p>I tested it directly on the PMG machine: <pre><code>echo \"Subject: sendmail test\" | sudo sendmail -f alex@openfoodfacts.org -v alex@openfoodfacts.org\n</code></pre></p>"},{"location":"reports/2022-02-proxmox-mail-gateway-install/#iptables-on-ovh1","title":"iptables on ovh1","text":"<p>FIXME: I stopped postfix on ovh1 sudo systemctl stop postfix</p> <p><code>sudo iptables -t nat -L</code> shows me that masquerading is already configured: <pre><code>    Chain POSTROUTING (policy ACCEPT)\n    target     prot opt source               destination\n    MASQUERADE  all  --  10.1.0.0/16          anywhere\n    MASQUERADE  all  --  10.1.0.0/16          anywhere\n    MASQUERADE  all  --  10.1.0.0/16          anywhere\n    MASQUERADE  all  --  anywhere             anywhere\n    MASQUERADE  all  --  10.1.0.0/16          anywhere\n</code></pre></p> <p>I configure NAT to postfix</p> <p>~~<code>sudo iptables -t nat -A PREROUTING -p tcp  --dport 25 -j DNAT --to 10.1.0.102:25</code>~~</p> <p>Note that, because we use nating, this means that pmg sees all incoming mails as coming from <code>10.1.0.1</code> thus avoiding PMG to filter out addresses</p> <p>So finally we have to limit to certain machines directly in iptables: <pre><code>sudo iptables -t nat -A PREROUTING -s 10.0.0.0/8,172.16.0.0/12,192.168.0.0/16 -d pmg.openfoodfacts.org -p tcp  --dport 25 -j DNAT --to 10.1.0.102:25\nsudo iptables -t nat -A PREROUTING -s 213.36.253.206,213.36.253.208,146.59.148.140,51.210.154.203,1.210.32.79 -d pmg.openfoodfacts.org -p tcp  --dport 25 -j DNAT --to 10.1.0.102:25\n</code></pre></p> <p>\u2757\ufe0fNote: important to set destination, otherwise pmg won't be able to reach external servers (because nat rule will apply to it's outgoing requests !)</p> <p>To save rules (ovh1 use netfilter-persistent): <pre><code>    cd /etc\n    git status\n    iptables-save -f iptables/rules.v4\n    etckeeper commit \"iptables rules to redirect port 25 to PMG proxmox mail gateway\"\n</code></pre></p>"},{"location":"reports/2022-02-proxmox-mail-gateway-install/#configuring-servers","title":"configuring servers","text":""},{"location":"reports/2022-02-proxmox-mail-gateway-install/#on-dockers-aka-preprod","title":"on dockers (aka preprod)","text":"<p>Configuration VM200 to send emails (this is a good candidates for tests):</p> <p><code>sudo apt intall postfix</code> * configuration type : satellite system * mail name: openfoodfacts.org * relayhost: pmg.openfoodfacts.org * mail for root: tech@openfoodfacts.org * other dest: blank * sync: no * local network: leave default * use procmail: no * default for the rest</p> <p>test on preprod: <pre><code>echo \"Subject: sendmail test\" | sendmail -f alex@openfoodfacts.org -v alex@openfoodfacts.org\n</code></pre></p>"},{"location":"reports/2022-02-proxmox-mail-gateway-install/#on-off1","title":"on off1","text":"<p><code>dpkg-reconfigure exim4-config</code> * mail sent by smarthost; no local mail * mail name: openfoodfacts.org * listen: 127.0.0.1 * other dest: off1.free.org * visible domain name : openfoodfacts.org * IP address smarthost:  pmg.openfoodfacts.org * keep DNS queries minimal (dial-up): no * local: maildir format in home * split config : no</p>"},{"location":"reports/2022-02-proxmox-mail-gateway-install/#on-off2","title":"on off2","text":"<p>Same config of exim as on off1.</p> <p>But Initially I cannot send !</p> <p>It's an iptables rule directly on the machine that block OUTGOING smtp requests: <pre><code>REJECT     tcp  --  anywhere             anywhere             tcp dpt:smtp reject-with icmp-port-unreachable`\n</code></pre> I removed this rule and put instead a rule to limit destination:</p> <pre><code>iptables -A OUTPUT -p tcp  '!' -d '146.59.148.140' --dport smtp -j REJECT --reject-with icmp-port-unreachable\n</code></pre> <p>I don't see any difference in iptables on ovh1. I've tried to log on ovh1: <pre><code>sudo iptables -I INPUT -s 213.36.253.208,213.36.253.206 -j LOG --log-prefix '** ALEX DEBUG **'\nsudo iptables -t nat -I PREROUTING -s 213.36.253.208,213.36.253.206 -j LOG --log-prefix '** ALEX DEBUG PRE **'\n</code></pre> but I don't see any incoming request !</p> <p>Trying to get root mails:</p> <p>I look into masquerading and see Step 4 \u2014 Forwarding System Mail https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-postfix-as-a-send-only-smtp-server-on-debian-10 https://debian-facile.org/doc:reseau:exim4:redirection-mails-locaux</p> <p>but on debian <code>/etc/mailname</code> also does the job see https://wiki.debian.org/Postfix and https://wiki.debian.org/EtcMailName but I'm not able to have alias work with <code>root: root@openfoodfacts.org</code></p>"},{"location":"reports/2022-02-proxmox-mail-gateway-install/#on-ovh3","title":"on ovh3","text":"<p>strange configuration in aliases, sending emails to admins@openfoodfacts.org !</p> <p>When I unblocked the mails, I got an incredible of mails sent to root@openfoodfacts.org ! (because of a munin error, running in cron every 5 minute and sending a mail every time). I add to suppress whole queue in PMG admin\u2026 (because it does not allow to bulk delete in queue). I received emails for more than 2 hours (fortunately gmail did some throttling)</p> <p>Munin error was: <code>There is nothing to do here, since there are no nodes with any plugins</code>. I did a <code>munin-node-configure --suggest --shell</code> without success. Finally changing <code>munin.conf</code> to use <code>127.0.0.1</code> for <code>ovh3</code> node (instead of 10.1.0.3) fixed it !</p>"},{"location":"reports/2022-02-proxmox-mail-gateway-install/#testing-mail-quality","title":"Testing mail quality","text":"<ul> <li>in thunderbird I setup a smtp using pmg.openfoodfacts.org</li> <li>I added an ip rule to forward if source is my ip address (from home)</li> <li>i sent a mail, result: https://www.mail-tester.com/test-kuklm574u then https://www.mail-tester.com/test-g6jk6pdto (after fixing name)</li> </ul>"},{"location":"reports/2022-02-remove-containers-ovh1/","title":"2022-02 Removing some containers on ovh1","text":""},{"location":"reports/2022-02-remove-containers-ovh1/#backups","title":"Backups","text":"<p>On datacenter we have a backup menu --&gt; click edit you can see the policies. We only have one. We use snapshot mode as we are on ZFS, and we use ZSTD compression. There also we configure which mail will be notified on errors. Some VM maybe excluded because inactive</p> <p>Backups goes in the backups disk resource (NFS mount of ovh3 zpool, NFS is integrated in ZFS).</p> <p>If we click on it we can see the backups.</p>"},{"location":"reports/2022-02-remove-containers-ovh1/#backup-of-a-machine","title":"Backup of a machine","text":"<p>115 backup is a bit old. We do a manal backup. We go on 115, backup tab, type <code>snapshot</code>, compression <code>ZSTD</code>.</p> <p>Problem: It can't acquire global lock\u2026 We see that there are no backup since quite a long date of ovh1 machine ! We also see that backup task on ovh1 have been failing for a while ! We didn't have mail, but ovh1 mail was not sending reliabely and more over the address where to send alerts was wrong (it was that of the CA). It seems that there is an old blocked vzdump running since 2021 !. We had to kill a lot of processes to get all down, and then remove /var/run/vzdump.{pid,lock}</p> <p>We then had to remove a backup because we where above the limit of 4 backups</p> <p>We then backup, it was a bit long (25 minutes).</p>"},{"location":"reports/2022-02-remove-containers-ovh1/#removal-of-a-vm","title":"Removal of a VM","text":"<p>Important: * Ensure you have backups for the machine before removal * stop the machine</p> <p>In options we edit protection to remove \"Yes\" if present. In upper bar on the machine : More -&gt; destroy You then check \"remove from tasks\"</p> <p>We had a failure because it cun't umount rpool/subvol-115-disk-0.</p> <p>This maybe due to mounts inside it.</p> <pre><code>cat mtab |grep subvol\n...\n10.0.0.3:/rpool/off/images-clone /rpool/subvol-115-disk-0/mnt/images nfs4 rw,relatime,vers=4.2,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.0.0.1,local_lock=none,addr=10.0.0.3 0 0\n10.0.0.3:/rpool/off/products-clone /rpool/subvol-115-disk-0/mnt/products nfs4 rw,relatime,vers=4.2,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.0.0.1,local_lock=none,addr=10.0.0.3 0 0\n...\n</code></pre> <p>umount myself:</p> <pre><code>umount  /rpool/subvol-115-disk-0/mnt/images\numount  /rpool/subvol-115-disk-0/mnt/products\n</code></pre> <p>we check zfs / CT volumes, there is no more 115 volume, and we see the free space on the graph.</p> <p>We also removed 110 machine. It was hard because of a process in status D (uninteruptible sleep). We had to reboot OVH1.</p> <p>Side Note: we see that munin is down, it's because Christian monitor it from it's own munin.</p> <p>Important remember to report changes to  https://docs.google.com/spreadsheets/d/19RePmE_-1V_He73fpMJYukEMjPWNmav1610yjpGKfD0/edit#gid=0</p>"},{"location":"reports/2022-02-remove-containers-ovh1/#creating-a-vm","title":"Creating a VM","text":"<p>Right click on ovh1 \"create VM) or use the button up right.</p> <p>We prefer not to reuse an ip that: * from 100 to 200 for lxc containers * from 200 and up fot QEMU</p>"},{"location":"reports/2022-02-remove-containers-ovh1/#creating-a-qemu-vm","title":"Creating a QEMU VM","text":"<ul> <li>use the iso in backups</li> </ul>"},{"location":"reports/2022-03-03-zammad-elasticsearch-not-running/","title":"Elasticsearch not running on Zammad","text":"<p>Date: 2022-03-03</p>"},{"location":"reports/2022-03-03-zammad-elasticsearch-not-running/#problem","title":"Problem","text":"<p>I wanted to Set up ES for Zammad</p> <p>After having access to the machine (which up to now is gracefully hosted by CQuest), I verify, as told by CQuest, that elasticsearch was installed.</p> <p>The problem was the elasticsearch service was not running, according to <code>systemctl status elasticsearch</code>.</p> <p><code>journalctl -r -u elasticsearch</code> shows this error: <code>Failed to start elasticsearch due to a fatal signal received by control process (code=killed, signal=9/KILL)</code></p>"},{"location":"reports/2022-03-03-zammad-elasticsearch-not-running/#solution","title":"Solution","text":"<p>After searching about in various directions, it reminds me of a OOMKill situation. Although we didn't have a OOMKill message in <code>/var/log/syslog</code> because this is an LXC container, so OOMKill happens in the guest.</p> <p>As I look into <code>/etc/elasticsearch/jvm.options</code> I've seen that by default it handles memory alone by looking at available memory.</p> <p>Being in an LXC container, it sees more memory than the container is allowed to take. Hence the kill situation.</p> <p>Thus I decided to fix the heap size manually. I created a file <code>/etc/elasticsearch/jvm.options.d/memory.options</code> 1 with:</p> <pre><code>-Xms300m\n-Xmx300m\n</code></pre> <p>After a <code>systemctl start elasticsearch</code>, elasticsearch was up.</p>"},{"location":"reports/2022-03-03-zammad-elasticsearch-not-running/#resolution-2-zammad-messed-up","title":"Resolution 2 - zammad messed up","text":"<p>I first had a hardtime figuring out how zammad was running.</p> <p>The real services in <code>/etc/systemd/system/</code> are the one with a <code>-1</code> in their name (others are just sleep commands, they act as placeholders to handle dependencies, I guess). But the executable was not there !</p> <p>This was because I unadvertantly de-installed it, while trying to solve elasticsearch issue (where I did a uninstall / reinstall of elasticsearch). I did not see it was also de-installing zammad :-/</p> <p>Hopefully I did not purge, and an <code>apt install zammad</code> solved the issue !</p>"},{"location":"reports/2022-03-03-zammad-elasticsearch-not-running/#finish-es-integration","title":"Finish ES integration","text":"<p>Following https://docs.zammad.org/en/latest/install/elasticsearch.html, I wanted to verify settings:</p> <pre><code>$ zammad run rails r 'print Setting.get(\"es_index\") + \"\\n\"'\nzammad\n</code></pre> <p>I reindex everything using <code>zammad run rake searchindex:rebuild</code> as indicated on zammad issue 1630</p> <p>It failed with: <pre><code>rake aborted!\nUnable to send Ticket.find(33).search_index_update_backend backend: #&lt;RuntimeError: Unable to process post request to elasticsearch URL 'http://localhost:9200/zammad_production_ticket/_doc/33?pipeline=zammad116611039594'. Elasticsearch is not reachable, probably because it's not running or even installed.\n\nResponse:\n#&lt;UserAgent::Result:0x0000563cf7686228 @success=false, @body=nil, @data=nil, @code=0, @content_type=nil, @error=\"#&lt;Errno::ECONNREFUSED: Failed to open TCP connection to localhost:9200 (Connection refused - connect(2) for \\\"localhost\\\" port 9200)&gt;\", @header=nil&gt;\n</code></pre></p> <p>Indeed <code>systemctl status elasticsearch</code> shows me it failed. Due to <code>Terminating due to java.lang.OutOfMemoryError: Java heap space</code></p> <p>I edit <code>/etc/elasticsearch/jvm.options.d/memory.options</code> to give more memory:</p> <pre><code>-Xms600m\n-Xmx600m\n</code></pre> <p>Then relaunched:</p> <p><code>zammad run rake searchindex:rebuild</code></p> <p>Finally it works !</p>"},{"location":"reports/2022-03-03-zammad-elasticsearch-not-running/#lessons-learned","title":"Lessons learned","text":"<p>This Elasticsearch memory problem is a recurring one ! In this case it was hard to spot at first sight. But this is always a lead to investigate on a hard kill of an Elasticsearch instance.</p> <p>Be prudent when doing <code>apt remove</code> to verify you aren't removing something else ! \ud83d\ude13</p>   <ol> <li> <p>in fact I was induced in error because in <code>jvm.options</code> the mandatory <code>.options</code> extension was not cited, so I first created a file with no extension and it did not work (but no log could tell me that). Reading online doc I sort this out. The problem is  already fixed in elasticsearch repos \u21a9</p> </li> </ol>"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/","title":"Deploying openfoodfacts-search to staging","text":"<p>I'm taking note to further upgrade documentation or simply remember the steps through an example.</p>"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#docker-preparation","title":"Docker preparation","text":"<p>I added a prod.yml to make volume external (it's important to avoid the pitfall of having a docker down -v remove all data !, it also gives better control).</p> <p>I made it a bit different from other project where name are absolute, because it is also dangerous, if at some point we have two different deployments on same machine\u2026 volume would be shared. I prefer to avoid a nightmare.</p>"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#avoiding-root","title":"Avoiding root","text":"<p>We want to avoid dockers running root in production.</p> <p>I checked other container, one is elasticvue is in fact an nginx, elasticsearch change user after launch, redis is ok.</p> <p>Modifying the Dockerfile to create a user and pass user uid as parameter. Also modifying the makefile to add uid</p>"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#securing-elasticvue-access","title":"Securing Elasticvue access","text":"<p>Elasticvue gives full access to the ES instance, but we want to access it in prod because it is handy to have a quick access to ES. We need to secure it. As it is a vue app served by a nginx service, the best way is to have basic auth inside it. I first tried to use the configuration by template option provided by nginx official docker image, creating a elasticvue.conf.template and elasticvue_htppasswd.template. but finally elasticvue image use an old nginx docker version, which do not have this template mechanism. As it does not even have a specific entrypoint I redefined the entrypoint and use a script that creates the htpasswd file and use sed to edit the config, this is even more flexible.</p>"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#image-creation","title":"Image creation","text":"<p>I first added the image creation github action. Did copy from off-server (but we got template also in openfoodfacts) and adapting it. I had to take care that image name should be the same as the one in docker-compose. I also had to add the TAG variable in docker-compose to set image version.</p> <p>I did it on deploy-init-stagging branch (starting with deploy-* to trigger action).</p>"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#deployment","title":"Deployment","text":"<p>I first added the image deployment github action. Did copy from off-server (but we got template also in openfoodfacts) and adapting it.</p> <p>I commented prod deployment in the triggers (v.*) because I wanted to be sure stagging would work before.</p> <p>The environment name is important because it is the folder where the project will live. Better have it ending with -net or -org to mark the type of deployment (help not messing up in servers). If it's unique among servers it's better so that we can mix deployment on same machine if we want (eg. in an emergency scenario).</p> <p>Did wrote the create_external_volumes in Makefile. I tested locally by tweaking  the env variable, and removing volumes afterwards</p> <pre><code>declare -x DOCKER_LOCAL_DATA=$(pwd)/tmp/\ndeclare -x COMPOSE_PROJECT_NAME=po_search_prod \ndeclare -x COMPOSE_PATH_SEPARATOR=\";\"\ndeclare -x COMPOSE_FILE=\"docker-compose.yml;docker/prod.yml\"\nmake create_external_volumes\n# test it works just starting setup\ndocker-compose up setup\nsudo ls tmp/*\n# clean\ndocker-compose rm -sf setup\ndocker volume rm po_search_prod_{certs,esdata01,esdata02,rediscache}\nunset DOCKER_LOCAL_DATA COMPOSE_PROJECT_NAME COMPOSE_PATH_SEPARATOR COMPOSE_FILE\n</code></pre>"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#secrets","title":"Secrets","text":"<p>There are a lot secrets to set on the github repo, I had to look at all used variables.</p> <p>I edited branch protection rule, because workflow are sensible: - restrict only admins and maintainers to push to deploy-* branches - same for main branch, with of course pull request review etc.</p> <p>To create a GRAPHANA token I had to go to graphana configuration -&gt; API keys - made an editor key and put it at repository level</p>"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#first-run","title":"First run","text":"<p>I did had a very hard way making connection to server successful:</p> <p>Lessons learned: - you have to connect through ovh1 as proxy, not ovh2 - secret key is to be as in the id_rsa key, that is with the \"BEGIN PRIVATE\u2026\" and \"END \u2026\" lines</p> <p>I had problem with the volume creation because /srv/off/docker_data was owned by root. <pre><code>sudo chown off:off /srv/off/docker_data\n</code></pre></p>"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#reverse-proxy-setup","title":"Reverse proxy setup","text":"<p>On OVH1, attaching to lxc 101.</p> <p>Added config (copying from robotoff, removing all certbot specific stuff)</p> <p>Then test and reload:</p> <pre><code>nginx -t\nsystemctl reload nginx\n</code></pre> <p>Also I generated the certificates: <pre><code>certbot -d search.openfoodfacts.net\n</code></pre></p>"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#enabling-live-update-2022-11-10","title":"Enabling live update (2022-11-10)","text":"<p>see https://github.com/openfoodfacts/openfoodfacts-search/issues/28</p>"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#fix-common-net-name","title":"Fix common net name","text":"<p>First I tried to see if I could ping redis from backend in off-net deployment.</p> <p><pre><code>sudo -u off bash\n$ cd /home/off\n$ docker-compose exec -u root backend bash\n$ apt update &amp;&amp; apt install iputils-ping\n$ ping searchredis\nPING searchredis.openfoodfacts.org (213.36.253.206) 56(84) bytes of data.\n64 bytes from off1.free.org (213.36.253.206): icmp_seq=1 ttl=49 time=6.72 ms\n</code></pre> This is not working. After a small investigation, I found that the problem is the \"webnet\" name, which is po_webnet in off-net and webnet in off-search-net.</p> <ul> <li>in openfoodfacts-search github repo,<ul> <li>I changed te deploy workflow so that COMMON_NET_NAME is po_webnet</li> <li>I git pushed as a \"deploy-\" branch and created a PR.</li> </ul> </li> </ul> <p>then I was able to really ping searchredis container from backend <pre><code>ping searchredis\nPING searchredis (172.30.0.12) 56(84) bytes of data.\n64 bytes from po_search_searchredis_1.po_webnet (172.30.0.12): icmp_seq=1 ttl=64 time=0.407 ms\n</code></pre></p> <p>see https://github.com/openfoodfacts/openfoodfacts-search/issues/27</p>"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#setting-redis_url-in-off-net","title":"Setting REDIS_URL in off-net","text":"<ul> <li>simply set it in deploy (although I first forgot the port, which is part of the URL)</li> </ul> <p>see: https://github.com/openfoodfacts/openfoodfacts-server/pull/7682</p>"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#setting-openfoodfacts_api_url","title":"Setting OPENFOODFACTS_API_URL","text":"<p>The search update was working but I did get errors telling some product did not exist, and having my products updates not taken into account.</p> <p>I finally realized, updates where fetch from openfoodfacts.org because we forgot to set OPENFOODFACTS_API_URL in deployment.</p> <p>I then had errors because I miss the basic auth that is necessary to reach openfoodfacts.net service. I added it in url and it worked.</p> <p>see https://github.com/openfoodfacts/openfoodfacts-search/pull/29</p>"},{"location":"reports/2022-07-11-infra-workshop/","title":"Infrastructure future - workshop on 11th July 2022","text":"<p>Participants: Alex, Charles, Christian, Stephane, Pierre</p>"},{"location":"reports/2022-07-11-infra-workshop/#goals","title":"Goals","text":"<p>what goals do we want to achieve?</p> <ol> <li> <p>Be able to scale horizontally</p> <ul> <li>disk space / scalability is one the main issue</li> <li>network management also</li> </ul> </li> <li> <p>Improve both the software and hardware side of the infrastructure</p> </li> <li> <p>Take ecology (carbon impact etc.) into account</p> </li> <li> <p>Human redundancy</p> <ul> <li>reduce stress</li> </ul> </li> <li> <p>Monitor thresholds, trends and critical alerts</p> </li> <li> <p>Redundancy, geographical PCA/PRA</p> </li> <li> <p>delegate, scale the potential for HR</p> </li> <li> <p>Reduce stress level</p> </li> <li> <p>Decrease the barrier to contribute for people who want to train their models</p> <ul> <li>32 GB RAM, GPU for training</li> </ul> </li> </ol>"},{"location":"reports/2022-07-11-infra-workshop/#solutions","title":"Solutions","text":""},{"location":"reports/2022-07-11-infra-workshop/#priorities","title":"Priorities","text":"<ol> <li>Forecast threshold and critical alerts and trends<ul> <li>Identify and document in one place thresholds and critical things to monitor<ul> <li>=&gt; Infra repo on github;    rationale:<ul> <li>external tool</li> <li>decentralized tool: .md files than can be cloned by everyone</li> </ul> </li> <li>Alex</li> </ul> </li> <li>Identify and document mega-trends (eg. images weight on disk)<ul> <li>Infra repo on github</li> <li>Which trends; what to look for to identify the trends</li> <li>What\u2019s critical and what\u2019s not</li> <li>Who?</li> </ul> </li> <li>Software evolutions of the monitoring infrastructure<ul> <li>which tools<ul> <li>Eg. Elastic search</li> </ul> </li> </ul> </li> <li>Human process<ul> <li>Documented in infrastructure repo.</li> <li>Who for what kind of task?</li> <li>Pipeline to acquire, fidelize and level up competent people/contributors</li> <li>Public infra meetings (40 minutes each month)<ul> <li>Prioritization of issues</li> <li>Nearly achieved threshold</li> <li>\u2026</li> <li>Alex, St\u00e9phane, Christian; community: Olivier? Hangy? Syl20? Alligator? SRE\u2019s Meetup in Paris? Admin sys without borders?</li> </ul> </li> <li>=&gt; Github issues</li> <li>=&gt; Automatic issues?</li> <li>=&gt; Sentry?</li> </ul> </li> </ul> </li> </ol>"},{"location":"reports/2022-07-11-infra-workshop/#human-redundancy-everywhere","title":"Human redundancy everywhere","text":"<ul> <li>Documentation:<ul> <li>Document global infrastructure</li> <li>=&gt; Github openfoodfacts-infrastructure repo</li> <li>=&gt; .md files</li> <li>List all systems/areas and who masters what<ul> <li>Identify gaps where we have only 1 or 2 persons<ul> <li>e.g. zfs</li> </ul> </li> <li>Identify owners: technical services, product owner</li> </ul> </li> <li>Infrastructure spreadsheet</li> <li>Distinguish OFF\u2019s core infrastructure and peripheral services<ul> <li>3 levels: critical for us, critical for others, peripheral</li> </ul> </li> </ul> </li> <li>Technologies upskilling<ul> <li>Fill gaps</li> <li>Skills spreadsheet: Munin, network, MongoDB, Docker, Proxmox, Nginx<ul> <li>At least 2 people skilled of each technology</li> </ul> </li> <li>Skills table: CT and VM list of OFF infrastructure<ul> <li>TODO: fill the spreadsheet (All)</li> </ul> </li> <li>Colleague's courses: ZFS, Docker, Proxmox, \u2026</li> <li>Commercial courses: security (YesWeHack ?),</li> </ul> </li> <li>diminish complexity where possible<ul> <li>some complexity: due to hosting on OVH</li> </ul> </li> </ul>"},{"location":"reports/2022-07-11-infra-workshop/#horizontal-scaling","title":"Horizontal scaling","text":"<p>1rst step: OFF1-OFF2 redundancy.</p> <ul> <li>MongoDB on OFF1</li> <li>OFF2 hardware + OS  upgrade<ul> <li>Proxmox on OFF1/OFF2?</li> </ul> </li> <li>Ask free to add a 3rd machine to make the migration. This machine could be the future test server for IA.<ul> <li>Email to Jean-Claude (Christian).</li> <li>Christian &amp; Stephane are listing hardware needs.</li> </ul> </li> <li>Allow to test the horizontal scaling.</li> </ul> <p>What main issues do we want to solve?</p> <ul> <li>Adding hardware easily.</li> <li>Disk space and availability:   a. Scaling with S3.   b. Software: distinguish cold and hot pictures:      - some pictures are never asked   c. Deduplicate.      - first mesure impact      - excellent topic for a contributor   d. When croped images are not the latest one, only keep the crop.   e. Convert JPEG to webp (-50%).   f. Decrease resolution for 48Mpixel images (=&gt; 12 Mpixel)</li> </ul> <ul> <li>Priority for disk space:<ul> <li>f, e</li> <li>b for the long term</li> </ul> </li> </ul> <ul> <li>CPU.</li> <li>What is scaling horizontally?<ul> <li>MongoDB: OK.</li> <li>Apache: OK.</li> <li>Images?<ul> <li>Use software routing to manage data with or without images or STO.</li> </ul> </li> <li>STO?</li> </ul> </li> </ul>"},{"location":"reports/2022-07-11-infra-workshop/#short-term","title":"Short term","text":"<ul> <li>Disk space: 6 month to solve this issue.<ul> <li>Very short term: x3 is possible but doesn\u2019t solve the issue in the long term.</li> <li> <ol> <li>Against rsync! =&gt; ZFS synchronization</li> <li>OFF1 &amp; OFF2 disks to ZFS</li> <li>buy 2x2 equivalent disks (+1 in case of emergency).</li> </ol> </li> </ul> </li> <li>Ask Syl20 for MongoDB optimisation.</li> </ul> <p>Focus on IO:</p> <ul> <li>on off1 rsync are impacting performance</li> <li>on off2 at night it gets slow</li> </ul> <p>Product use cases:</p> <ul> <li>scan speed</li> <li>facet speed</li> <li>advanced search speed</li> <li>nightly data exports</li> <li># of concurrent scan requests</li> </ul> <p>Increase modularity: being able to upgrade with confidence</p> <p>being able to deploy regularly, and with confidence</p> <ul> <li>what kind of evolutions to achieve these goals?</li> <li>what kind of hardware evolutions to follow: 1. products' growth, 2. resource consumption growth (API, web, etc.) and 3. software evolution.</li> <li>what short-term evolutions to manage products' growth (6-18 months).</li> </ul>"},{"location":"reports/2022-07-11-infra-workshop/#decrease-the-barrier-to-contribute-for-people-who-want-to-train-their-models","title":"Decrease the barrier to contribute for people who want to train their models","text":"<ul> <li>64 GB RAM, GPU for training,<ul> <li>Tool server with OSM?</li> </ul> </li> </ul>"},{"location":"reports/2022-07-11-infra-workshop/#potential-resources","title":"Potential resources","text":"<p>https://sre.google/books/</p>"},{"location":"reports/2022-07-11-infra-workshop/#small-tasks","title":"Small tasks","text":"<ul> <li>Document Munin in openfoodfacts-infrastructure</li> <li>Document what to look for in Munin</li> <li>Document monitoring usage</li> <li>Add API documentation for the reuse of Robotoff</li> <li>Plan de reprise de Robotoff</li> </ul>"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/","title":"Kibana down because of Elasticsearch circuit_breaking_exception","text":""},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#symptoms","title":"Symptoms","text":"<p>We got a lot of alerts in infrastructure-alerts slack channel for:</p>  <p>Service probe on URL 'https://kibana.openfoodfacts.org/status' failed for more than 5 minutes.</p>  <p>Going to the status url we got a \"server error\".</p>"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#trying-to-diagnose-and-remedy","title":"Trying to diagnose and remedy","text":""},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#base-problem","title":"Base problem","text":"<p>On the machine, looking at kibana logs, while doing a request</p> <p><pre><code>docker-compose logs --tail=0 -f kibana\n</code></pre> we see <pre><code>kibana_1                  | {\"type\":\"log\",\"@timestamp\":\"2022-07-27T12:36:07+00:00\",\"tags\":[\"error\",\"plugins\",\"security\",\"authentication\"],\"pid\":1216,\"message\":\"License is not available, authentication is not possible.\"}\nkibana_1                  | {\"type\":\"log\",\"@timestamp\":\"2022-07-27T12:36:07+00:00\",\"tags\":[\"warning\",\"plugins\",\"licensing\"],\"pid\":1216,\"message\":\"License information could not be obtained from Elasticsearch due to {\\\"error\\\":{\\\"root_cause\\\":[{\\\"type\\\":\\\"circuit_breaking_exception\\\",\\\"reason\\\":\\\"[parent] Data too large, data for [&lt;http_request&gt;] would be [1028220976/980.5mb], which is larger than the limit of [1020054732/972.7mb], real usage: [1028220976/980.5mb], new bytes reserved: [0/0b], usages [request=0/0b, fielddata=0/0b, in_flight_requests=0/0b, model_inference=0/0b, eql_sequence=0/0b, accounting=76757928/73.2mb]\\\",\\\"bytes_wanted\\\":1028220976,\\\"bytes_limit\\\":1020054732,\\\"durability\\\":\\\"PERMANENT\\\"}],\\\"type\\\":\\\"circuit_breaking_exception\\\",\\\"reason\\\":\\\"[parent] Data too large, data for [&lt;http_request&gt;] would be [1028220976/980.5mb], which is larger than the limit of [1020054732/972.7mb], real usage: [1028220976/980.5mb], new bytes reserved: [0/0b], usages [request=0/0b, fielddata=0/0b, in_flight_requests=0/0b, model_inference=0/0b, eql_sequence=0/0b, accounting=76757928/73.2mb]\\\",\\\"bytes_wanted\\\":1028220976,\\\"bytes_limit\\\":1020054732,\\\"durability\\\":\\\"PERMANENT\\\"},\\\"status\\\":429} error\"}\n</code></pre></p>"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#more-memory","title":"More memory","text":"<p>The day before, I gave more memory to ES to see if it was the problem, but it's not (see openfoodfacts-monitoring:#54).</p>"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#continue-diagnosis","title":"Continue diagnosis","text":"<p>If we go on kibana container we can reproduce error:</p> <pre><code>docker-compose exec kibana bash\n</code></pre> <p>ES is there: <pre><code>curl -XGET http://elasticsearch:9200\n</code></pre></p> <p>but we get the circuit_breaking_exception while querying <pre><code>curl -XGET http://elasticsearch:9200/_license?pretty=true\n{\n  \"error\" : {\n    \"root_cause\" : [\n      {\n        \"type\" : \"circuit_breaking_exception\",\n        \"reason\" : \"[parent] Data too large, data for [&lt;http_request&gt;] would be [1027528328/979.9mb], which is larger than the limit of [1020054732/972.7mb], real usage: [1027528328/979.9mb], new bytes reserved: [0/0b], usages [request=0/0b, fielddata=0/0b, in_flight_requests=0/0b, model_inference=0/0b, eql_sequence=0/0b, accounting=76607288/73mb]\",\n        \"bytes_wanted\" : 1027528328,\n        \"bytes_limit\" : 1020054732,\n        \"durability\" : \"PERMANENT\"\n      }\n    ],\n    \"type\" : \"circuit_breaking_exception\",\n    \"reason\" : \"[parent] Data too large, data for [&lt;http_request&gt;] would be [1027528328/979.9mb], which is larger than the limit of [1020054732/972.7mb], real usage: [1027528328/979.9mb], new bytes reserved: [0/0b], usages [request=0/0b, fielddata=0/0b, in_flight_requests=0/0b, model_inference=0/0b, eql_sequence=0/0b, accounting=76607288/73mb]\",\n    \"bytes_wanted\" : 1027528328,\n    \"bytes_limit\" : 1020054732,\n    \"durability\" : \"PERMANENT\"\n  },\n  \"status\" : 429\n}\n</code></pre></p> <p>trying to get stats for breaker: <pre><code>curl -XGET elasticsearch:9200/_nodes/stats/breaker?pretty=true\n</code></pre> we see: <pre><code>\"parent\" : {\n          \"limit_size_in_bytes\" : 1020054732,\n          \"limit_size\" : \"972.7mb\",\n          \"estimated_size_in_bytes\" : 1030370648,\n          \"estimated_size\" : \"982.6mb\",\n          \"overhead\" : 1.0,\n          \"tripped\" : 39108\n        }\n</code></pre></p> <p>Parent is the responsible for all memory according to amazon help center :</p>  <p>The parent circuit breaker (a circuit breaker type) is responsible for the overall memory usage of your cluster.</p>  <p>I tried to clear out <code>fielddata</code> cache:</p> <p><pre><code>curl -XPOST http://elasticsearch:9200/*/_cache/clear?fielddata=true\n</code></pre> but it does not solve the problem.</p>"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#tunning-parent-circuit-breaker","title":"Tunning parent circuit breaker","text":"<p>Comparing to reference documentation, our settings seems a bit low compared to defaults.</p> <p>I will try to augment the parent size for now, and set it to real memory tracking.</p> <p>Edited the docker-compose to add configuration (through environment variables):</p> <pre><code>  elasticsearch:\n  ...\n    environment:\n    ...\n      - \"indices.breaker.total.use_real_memory=true\"\n      - \"indices.breaker.request.limit=95%\"\n</code></pre> <p>and restarted the container <pre><code>docker-compose restart elasticsearch\n</code></pre></p> <p>It's not enough.</p> <p>The issue might be that we have too much  data (17Gb indexes)</p>"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#garbage-collection","title":"Garbage collection ?","text":"<p>Is that maybe related to Garbage collection ? Listing <code>/usr/share/elasticsearch/config/jvm.options</code> in the container shows that G1GC is in use:</p> <pre><code>## GC configuration\n8-13:-XX:+UseConcMarkSweepGC\n8-13:-XX:CMSInitiatingOccupancyFraction=75\n8-13:-XX:+UseCMSInitiatingOccupancyOnly\n\n## G1GC Configuration\n# NOTE: G1 GC is only supported on JDK version 10 or later\n# to use G1GC, uncomment the next two lines and update the version on the\n# following three lines to your version of the JDK\n# 10-13:-XX:-UseConcMarkSweepGC\n# 10-13:-XX:-UseCMSInitiatingOccupancyOnly\n14-:-XX:+UseG1GC\n</code></pre> <p>line <code>14-:</code> applies for we are on version 16 of the jdk:</p> <pre><code># jdk/bin/java --version\nopenjdk 16.0.2 2021-07-20\n</code></pre>"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#data-size","title":"Data size","text":"<p>In elasticsearch container: <pre><code>curl localhost:9200/_cluster/stats?pretty=true\n</code></pre></p> <p>We got <pre><code> ...\n \"segments\" : {\n      \"count\" : 2390,\n      \"memory_in_bytes\" : 76423432,\n      \"terms_memory_in_bytes\" : 65353984,\n      \"stored_fields_memory_in_bytes\" : 1225392,\n      \"term_vectors_memory_in_bytes\" : 0,\n      \"norms_memory_in_bytes\" : 9117696,\n      \"points_memory_in_bytes\" : 0,\n      \"doc_values_memory_in_bytes\" : 726360,\n      \"index_writer_memory_in_bytes\" : 35328144,\n      \"version_map_memory_in_bytes\" : 0,\n      \"fixed_bit_set_memory_in_bytes\" : 960,\n      \"max_unsafe_auto_id_timestamp\" : 1658927252002,\n      \"file_sizes\" : { }\n    },\n ...\n}\n</code></pre> other memory values (for <code>querycache</code> or <code>fielddata</code>) ar <code>0</code> or near <code>0</code>.</p> <p>This is 188,175,008 bytes in total, which is 180m.</p> <p>If we look at nodes using cat nodes API:</p> <p><pre><code>curl -XGET \"localhost:9200/_cat/nodes?v=true&amp;h=name,node*,heap*\"\nname         id   node.role   heap.current heap.percent heap.max\n0665175f0369 3vOq cdfhilmrstw      978.5mb           95      1gb\n</code></pre> we see two potential problems: * heap size is only 1gb (it should be 2 according to our heap settings) * our a node use 95% memory</p>"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#too-many-shards","title":"Too many shards ?","text":"<p>following Es blog on memory troubleshooting</p> <p>Over sharding is a usual suspect, let see:</p> <pre><code># curl -XGET \"localhost:9200/_cluster/health?filter_path=status,*_shards&amp;pretty=true\"\n{\n  \"status\" : \"yellow\",\n  \"active_primary_shards\" : 317,\n  \"active_shards\" : 317,\n  \"relocating_shards\" : 0,\n  \"initializing_shards\" : 0,\n  \"unassigned_shards\" : 302,\n  \"delayed_unassigned_shards\" : 0\n}\n</code></pre> <p>while recommanded for our setting (2Gb) is 40 shards ! Also having <code>unassigned_shards</code> seems a bad news !</p> <p>The problems comes from the fact that we have many indices. The ILM (Index Life Cycle Management) should prevent that, but it does not.</p>"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#repairing-backup-then-remove-old-indices","title":"Repairing (backup then remove old indices)","text":""},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#more-memory_1","title":"More memory","text":"<p>First I need to snapshot to eventually clear some indices. But with the exception I can't do it, I can't even check a snapshot repository already exists:</p> <pre><code>$ curl -XGET 127.0.0.1:9200/_snapshot\n{\"error\":{\"root_cause\":[{\"type\":\"circuit_breaking_exception\",\"reason\":\"[parent] Data too large, data for [&lt;http_request&gt;] would be [1026619992/979mb], which is larger than the limit of [1020054732/972.7mb], real usage: [1026619992/979mb], new bytes reserved: [0/0b], usages [request=0/0b, fielddata=815/815b, in_flight_request...\n</code></pre> <p>No choice then, we have to give ES enough memory to be able to handle current indices\u2026 we go for a hype of 8G. I change the <code>docker-compose.yml</code>: <pre><code>  elasticsearch:\n    ...\n    environment:\n      ...\n      - \"ES_JAVA_OPTS=-Xms8048m -Xmx8048m\"\n    ...\n    mem_limit: 9g\n</code></pre></p> <p>But then again after a while we are blocked by the exception again :-( Even stranger, we have a larger than the limit of [1020054732/972.7mb]</p> <p>circuit breaker limit was for request not total ! changed <code>indices.breaker.request.limit=95%</code> to <code>indices.breaker.total.limit=95%</code>. ... same result !</p>"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#snapshoting-backup","title":"Snapshoting (backup)","text":"<p>Trying to backup:</p> <ul> <li> <p>modified docker-compose.yml to add</p> <p><pre><code>elasticsearch\n    ...\n    environment:\n    ...\n    - \"path.repo=/opt/elasticsearch/backups\"\n    ...\n    volumes:\n    - elasticsearch-backup:/opt/elasticsearch\n\nvolumes:\n...\nelasticsearch-backup:\n</code></pre> - Create directories</p> <pre><code>docker-compose run --rm elasticsearch bash\ncd /opt/elasticsearch\nmkdir backups\nchown elasticsearch /opt/elasticsearch/ -R\n</code></pre> </li> </ul> <ul> <li> <p>Recreated container</p> <pre><code>docker-compose rm -sf elasticsearch\ndocker-compose up -d elasticsearch\n</code></pre> </li> </ul> <ul> <li>Created the snapshot repository   <pre><code>curl -X PUT \"http://localhost:9200/_snapshot/backups?pretty\" -H 'Content-Type: application/json'     -d'\n{\n  \"type\": \"fs\",\n  \"settings\": {\n    \"location\": \"/opt/elasticsearch/backups\"\n  }\n}\n'\n</code></pre> Note: (we had to retry several times because of circuit breaking exceptions)</li> </ul> <ul> <li>get all indices names<pre><code>curl -XGET \"http://localhost:9200/*?pretty=true\"|grep '^  \"logs'\n</code></pre> </li> </ul> <ul> <li> <p>make a global snapshot, in a screen !!!</p> <p><pre><code>screen\n...\ncurl -X PUT \"localhost:9200/_snapshot/backups/2022-07-31?wait_for_completion=true&amp;pretty\" -H   'Content-Type: application/json' -d'\n{\n  \"indices\": [],\n  \"ignore_unavailable\": true,\n  \"include_global_state\": false,\n  \"metadata\": {\n    \"taken_by\": \"alex\",\n    \"taken_because\": \"backup before removing some indices\"\n  }\n}\n'\n</code></pre> And it's a success !</p> <pre><code>{\n\"snapshot\" : {\n  \"snapshot\" : \"2022-07-31\",\n  \"uuid\" : \"kHQXBTGhQb-4-jH0m5mMfg\",\n  \"repository\" : \"backups\",\n  \"version_id\" : 7140199,\n  ...\n  \"state\" : \"SUCCESS\",\n  \"start_time\" : \"2022-07-31T18:06:51.220Z\",\n  \"start_time_in_millis\" : 1659290811220,\n  \"end_time\" : \"2022-07-31T18:15:08.301Z\",\n  \"end_time_in_millis\" : 1659291308301,\n  \"duration_in_millis\" : 497081,\n  \"failures\" : [ ],\n  \"shards\" : {\n    \"total\" : 322,\n    \"failed\" : 0,\n    \"successful\" : 322\n  },\n  \"feature_states\" : [ ]\n}\n}\n</code></pre> </li> </ul> <p>So we start removing indices !</p>"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#removing-indices","title":"Removing indices","text":"<p>Let's remove 2021 logs</p> <pre><code>curl -X DELETE \"localhost:9200/logs-2021.*?pretty=true\"\n</code></pre> <p>Is hard to pass, so we restart elasticsearch and try again\u2026 success.</p> <p>Lets remove 2022.01\u2026 until 04</p> <p>Finally it works !</p>"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#fixing-snapshot-policy-and-ilm-policy","title":"Fixing Snapshot policy and ILM policy","text":"<p>It is a temporary fix\u2026 now we have to use kibana to setup a better policy (and maybe take ES memory down a bit again ?)</p>"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#create-snapshot-policy","title":"Create snapshot policy","text":"<p>In Kibana, go to Management -&gt; Stack Management -&gt; Snapshot and restore</p> <p>In \"Repositories\" we see the <code>backups</code> repository that was created before, pointing to <code>/opt/elasticsearch/backups</code></p> <p>Remember that we had to edit the <code>docker-compose.yml</code> file to add the <code>path.repo</code> environment variable and the <code>elasticsearch-backup</code> volume.</p> <p>In \"Policies\", we create a policy named <code>weekly-snapshots</code> as follow: - Snapshot name: <code>&lt;weekly-snap-{now/d}&gt;</code> - Repository: backups - Schedule : weekly - Data streams and indices: All indices - Ignore unavailable indices: No - Allow partial shards: No - Include global state: Yes - Retention: delete after 360d - Min count: 20</p>"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#create-index-lifecycle-management-policy","title":"Create Index Lifecycle Management policy","text":"<p>This part is very important. Logs are creating a lot of indices (as logrotate would) and so we have to automatically manage what happens with old indices so that we do not go beyond hardware capabilities (too much indices takes too much memory). ES has a mechanism for that called Index Lifecycle Management (ILM).</p> <p>In Kibana, go to Management -&gt; Stack Management -&gt; Index Lifecycle Policy</p> <p>There is already a log policy. Edit it to have this:</p> <ul> <li>Hot phase (the phase where the index is the active index for logs)<ul> <li>Roll over : use recommended defaults</li> <li>set index priority to 100</li> </ul> </li> <li>Warm phase (the phase where the index is history of logs, you want to search)<ul> <li>Move data into phase when 58 days old</li> <li>replicas: 0</li> <li>Shrink: to 1</li> <li>Force merge to 1 + compress</li> <li>mark read only</li> <li>index priority: 50</li> </ul> </li> <li>Cold phase (historical data, last phase before removal)<ul> <li>Move data into phase when: 119 days old</li> <li>Do not make them searchable</li> </ul> </li> <li>Delete phase:<ul> <li>move data intor phase when 240 days old</li> <li>Wait for snapshot policy: weekly-snapshots</li> </ul> </li> </ul>"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#attach-policy-to-index-template","title":"Attach policy to index template","text":"<p>Attach the logs policy you edited to the logs index template.</p> <p>In Management -&gt; Stack Management -&gt; Index Lifecycle Policy,</p> <ul> <li>On the line corresponding to logs policy, use Action button + Add policy to index template</li> <li>Set template to logs</li> <li>Set alias to logs-current</li> </ul> <p>We notice that logs template pattern was not coherent, as it was <code>logs-*-*</code>, so we also add <code>logs-*.*.*</code> that match our indexes names. To do this goes to Management -&gt; Stack Management -&gt; Index Management -&gt; Index Templates, and edit the logs template.</p> <p>Now we have to create the active alias. It should correspond to our last index. This was <code>logs-2022.08.18</code> at time of writing.</p> <p>In the Dev Tool -&gt; Console, we run:</p> <pre><code>POST _aliases\n{\n  \"actions\": [\n    {\n      \"add\": {\n        \"index\": \"logs-2022.08.18\",\n        \"alias\": \"logs-current\",\n         \"is_write_index\": true\n      }\n    }\n  ]\n}\n</code></pre> <p>We then go to  Management -&gt; Stack Management -&gt; Index Management</p> <p>Select logs-2022.08.18 and use \"Manage\" apply log policy, with: * policy: logs * alias: logs-current</p>"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#attach-old-index-to-a-policy","title":"Attach old index to a policy","text":"<p>This will add it for future indices, but we want to add it to existing indices (we follow ES doc, section Manage existing indices).</p> <ul> <li>going to Management -&gt; Stack Management -&gt; Index Lifecycle Policy   we create a policy that is exactly the same as logs policy but:<ul> <li>name is logs-old</li> <li>rollover is disabled</li> </ul> </li> <li>we apply it to 05 logs, in Dev Tool -&gt; Console, by running:   <pre><code>PUT logs-2022.05.*/_settings \n{\n  \"index\": {\n    \"lifecycle\": {\n      \"name\": \"logs-old\"\n    }\n  }\n}\n</code></pre></li> <li>we do the  same for every monthes until 08, but beware not to apply it to 2022.08.18 !</li> </ul>"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#useful-resources","title":"Useful resources","text":"<p>About circuit breaker exception: - https://aws.amazon.com/premiumsupport/knowledge-center/opensearch-circuit-breaker-exception/ - https://www.elastic.co/blog/improving-node-resiliency-with-the-real-memory-circuit-breaker - https://www.elastic.co/guide/en/elasticsearch/reference/7.0/circuit-breaker.html#parent-circuit-breaker</p> <p>About index lifecycle management (ILM): * https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started-index-lifecycle-management.html * https://www.elastic.co/guide/en/elasticsearch/reference/current/index-lifecycle-management.html</p>"},{"location":"reports/2022-08-17-openfoodfacts-net-unreachable/","title":"2022-08-17 openfoodfacts.net unreachable","text":""},{"location":"reports/2022-08-17-openfoodfacts-net-unreachable/#what-happens","title":"What happens","text":"<ul> <li>day before stagging (preprod) environment was down due to a bug introduced in the code</li> <li>A PR #openfoodfacts-server:7214 was submitted and merged to fix it</li> <li>github action went bogus, but was reruned later and passed</li> <li>though openfoodfacts.net was unreachable</li> </ul>"},{"location":"reports/2022-08-17-openfoodfacts-net-unreachable/#why-this-happens","title":"Why this happens","text":"<ul> <li>the day before, Charles saw that wiki and some other services were down and had to reboot proxy1</li> <li>openfoodfacts.net was still using the old way to redirect:<ul> <li>redirect to ovh1.openfoodfacts</li> <li>ip filters would then forward to internal ip 101 (aka proxy VM)</li> </ul> </li> <li>this kind of forwarding did not work any more but I don't know why</li> <li>Also this way of doing as the drawback that nginx can't see the correct incoming address, but see instead 10.0.0.1</li> </ul>"},{"location":"reports/2022-08-17-openfoodfacts-net-unreachable/#what-was-done","title":"What was done","text":""},{"location":"reports/2022-08-17-openfoodfacts-net-unreachable/#resolving-by-pointing-to-proxy1-instead-of-ovh1","title":"Resolving by pointing to proxy1 instead of ovh1","text":"<p>I tested that using proxy1 address would work, using:</p> <pre><code>curl  --connect-to world.openfoodfacts.net:443:193.70.55.124:443 https://off:***@world.openfoodfacts.net\n</code></pre> <p>Go on OVH interface, web cloud, domain names, openfoodfacts.net, dns zones</p> <p>Change <code>CNAME</code> and <code>A</code> previously pointing to ovh1 to point to proxy1</p> <ul> <li><code>A</code> rule for: <code>openfoodfacts.net</code> and <code>*.openfoodfacts.net</code></li> <li><code>CNAME</code> rule for: <code>events.openfoodfacts.net</code>, <code>productopener.openfoodfacts.net</code>, <code>robotoff.openfoodfacts.net</code></li> </ul> <p>We had to wait for propagation...</p> <p>On the <code>A</code> rule I changed DNS TTL to 360s as it is a test environment... (suggested by Benoit382 on slack)</p>"},{"location":"reports/2022-08-17-openfoodfacts-net-unreachable/#trying-to-understand-why-ovh1-forwarding-is-down","title":"Trying to understand why ovh1 forwarding is down","text":"<p>Forwarding rule and Masquerading rule seems there:</p> <pre><code>sudo iptables -t nat -L --verbose --line-numbers\n\nChain PREROUTING (policy ACCEPT 3135 packets, 201K bytes)\nnum   pkts bytes target     prot opt in     out     source               destination         \n1    3016K  180M DNAT       tcp  --  any    any     anywhere             ovh1.openfoodfacts.org  tcp dpt:https to:10.1.0.101:443\n2     318K   18M DNAT       tcp  --  any    any     anywhere             ovh1.openfoodfacts.org  tcp dpt:http to:10.1.0.101:80\n(...)\nChain POSTROUTING (policy ACCEPT 0 packets, 0 bytes)\nnum   pkts bytes target     prot opt in     out     source               destination         \n(...)\n3    2203K  150M MASQUERADE  all  --  any    vmbr1   10.1.0.0/16          anywhere            \n4      10M  697M MASQUERADE  all  --  any    any     anywhere             anywhere            \n5        0     0 MASQUERADE  all  --  any    vmbr1   10.1.0.0/16          anywhere            \n6        0     0 MASQUERADE  all  --  any    vmbr1   10.1.0.0/16          anywhere    \n</code></pre> <p>But the proxy nginx does not seems accessible to ovh1:</p> <p><code>nc -vz 10.1.0.101 443</code> and <code>nc -vz 10.1.0.101 80</code> are not responding</p> <p>While on the proxy itself, it is:</p> <pre><code># nc -vz 10.1.0.101 443\nproxy [10.1.0.101] 443 (https) open\n</code></pre> <p>This is linked to previous problem resolution, see Reverse proxy down on 18 f\u00e9v 2022, Final resolution.</p> <p>Solution: on proxy add a rule for routing 10.1.0.xxx packets: <pre><code>sudo ip route add 10.0.0.0/8 dev eth0\n</code></pre></p> <p>Now <code>nc -vz 10.1.0.101 443</code> works from ovh1 and service is up again.</p>"}]}