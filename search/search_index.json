{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OpenFoodFacts Infrastructure # Sysadmin repository for the various parts of the Open Food Facts infrastructure. We have a specific repository regarding monitoring Documentation # Link to Github Page The infrastructure documentation is as follows: Mail - servers mail setup Linux Server - servers general setup Proxmox - about proxmox management CICD - continuous integration and deployment Observability - doc on monitoring / logs / etc. Docker Onboarding Docker Infrastructure Virtual Machines Some services: Discourse for forum NGINX reverse proxy the reverse proxy for OVH services Folksonomy user editable labels and values Matomo for web analytics Producers sftp to push product updates on producer platform Zammad for support Also look at all install and post-mortem reports in docs/reports Requests # Virtual Machines # Title State OS CPU # RAM SSD (Local) HDD (Remote) Services Monitoring - VM (QEMU host for docker) [#159] open Debian * 4 CPUs * 12G for we have influxdb and elastic-search that needs memory * 30 Go disk (it is currently around 14G, but this will grow because we want to harvest more logs and more metrics) * 50Go for ES backups Docker, docker-compose CT for new blog engine [#80] open Debian stable. 3 CPU. 2 GB. 10 GB -- LAMP + wordpress. CT for Folksonomy Engine API dev [#76] open Default to Debian last Stable. 2 1 GB 12 GB. - PostgreSQL, Python3. Wild School Eco-Score project [#37] open Debian 10 4 16 Gb 30 Gb 0 MongoDB slack-org [#36] open Debian 10 1 1 Gb 10 Gb None Node.js adminer-org [#29] open Debian 10 2 512 Mb. 4 Gb or even less. 0 Nginx, PHP, Adminer. Containers (x2) to build a replica set for OFF database [#28] open Debian 10 4 32 GB 50 GB (DB = 20 GB). 0 Mongodb. feedme-org [#27] open Debian 10 3 3 Gb. 15 Gb. 0 PostgreSQL, Node.js, Nginx. off-wiki-org [#21] open Debian 10 2 3 Gb 14 Gb. 14 Gb Apache, PHP, MySQL, Mediawiki. VM for the Community Portal [#124] closed Debian last Stable. [Explain if > 4.] [Explain if > 4 Gb.] [Explain if > 32 Gb.] [Explain if > 1 Tb.] Python/Django, probably PostgreSQL, probably Apache and all Dockerized VM for the Taxonomy Editor [#123] closed Debian last Stable. [Explain if > 4.] [Explain if > 4 Gb.] [Explain if > 32 Gb.] [Explain if > 1 Tb.] Python, probably PostgreSQL, probably Apache for lightweight API serving from Docker New VM QEMU for prod docker containers [#71] closed Debian 11 (stable) 8 24 GB 256 GB. - Services deployed in production: monitoring [#59] closed Debian 11 4 32GB 64GB 500GB (ovh3 mount) Docker: ElasticSearch (Kibana?, Logstash?), Grafana, InfluxDB, Prometheus, Alertmanager impactestimator-net [#55] closed Debian 11 1 1GB 1Gb 0 https://github.com/openfoodfacts/impactestimator robotoff-ml [#53] closed Debian 11 8 96GB (Tensorflow, ANN) 192GB [ML models] 100GB Tensorflow + ElasticSearch robotoff-net [#51] closed Debian 11 4 16GB (DB 4GB, Services 8GB) 92GB 0GB Robotoff API + Schedulers + Workers, PostgreSQL DB mongo-dev [#45] closed Debian 10 2 16GB 40GB MongoDB running in Docker off-net [#41] closed Debian 10 4 16GB (PO needs > 6GB) 192GB 0GB ProductOpener frontend + backend, MongoDB, PostgreSQL, Memcached robotoff-dev [#40] closed Debian 10 4 8 Gb 32 Gb 100 Gb robotoff, elastic search, tensorflow, postgresql Matomo [#24] closed Debian 10 No idea. No idea. No idea. No idea. LAMP robotoff-org [#20] closed Debian 10 4 8 Gb 32 Gb 100 Gb robotoff, elastic search, tensorflow, postgresql Request a VM","title":"OpenFoodFacts Infrastructure"},{"location":"#openfoodfacts-infrastructure","text":"Sysadmin repository for the various parts of the Open Food Facts infrastructure. We have a specific repository regarding monitoring","title":"OpenFoodFacts Infrastructure"},{"location":"#documentation","text":"Link to Github Page The infrastructure documentation is as follows: Mail - servers mail setup Linux Server - servers general setup Proxmox - about proxmox management CICD - continuous integration and deployment Observability - doc on monitoring / logs / etc. Docker Onboarding Docker Infrastructure Virtual Machines Some services: Discourse for forum NGINX reverse proxy the reverse proxy for OVH services Folksonomy user editable labels and values Matomo for web analytics Producers sftp to push product updates on producer platform Zammad for support Also look at all install and post-mortem reports in docs/reports","title":"Documentation"},{"location":"#requests","text":"","title":"Requests"},{"location":"#virtual-machines","text":"Title State OS CPU # RAM SSD (Local) HDD (Remote) Services Monitoring - VM (QEMU host for docker) [#159] open Debian * 4 CPUs * 12G for we have influxdb and elastic-search that needs memory * 30 Go disk (it is currently around 14G, but this will grow because we want to harvest more logs and more metrics) * 50Go for ES backups Docker, docker-compose CT for new blog engine [#80] open Debian stable. 3 CPU. 2 GB. 10 GB -- LAMP + wordpress. CT for Folksonomy Engine API dev [#76] open Default to Debian last Stable. 2 1 GB 12 GB. - PostgreSQL, Python3. Wild School Eco-Score project [#37] open Debian 10 4 16 Gb 30 Gb 0 MongoDB slack-org [#36] open Debian 10 1 1 Gb 10 Gb None Node.js adminer-org [#29] open Debian 10 2 512 Mb. 4 Gb or even less. 0 Nginx, PHP, Adminer. Containers (x2) to build a replica set for OFF database [#28] open Debian 10 4 32 GB 50 GB (DB = 20 GB). 0 Mongodb. feedme-org [#27] open Debian 10 3 3 Gb. 15 Gb. 0 PostgreSQL, Node.js, Nginx. off-wiki-org [#21] open Debian 10 2 3 Gb 14 Gb. 14 Gb Apache, PHP, MySQL, Mediawiki. VM for the Community Portal [#124] closed Debian last Stable. [Explain if > 4.] [Explain if > 4 Gb.] [Explain if > 32 Gb.] [Explain if > 1 Tb.] Python/Django, probably PostgreSQL, probably Apache and all Dockerized VM for the Taxonomy Editor [#123] closed Debian last Stable. [Explain if > 4.] [Explain if > 4 Gb.] [Explain if > 32 Gb.] [Explain if > 1 Tb.] Python, probably PostgreSQL, probably Apache for lightweight API serving from Docker New VM QEMU for prod docker containers [#71] closed Debian 11 (stable) 8 24 GB 256 GB. - Services deployed in production: monitoring [#59] closed Debian 11 4 32GB 64GB 500GB (ovh3 mount) Docker: ElasticSearch (Kibana?, Logstash?), Grafana, InfluxDB, Prometheus, Alertmanager impactestimator-net [#55] closed Debian 11 1 1GB 1Gb 0 https://github.com/openfoodfacts/impactestimator robotoff-ml [#53] closed Debian 11 8 96GB (Tensorflow, ANN) 192GB [ML models] 100GB Tensorflow + ElasticSearch robotoff-net [#51] closed Debian 11 4 16GB (DB 4GB, Services 8GB) 92GB 0GB Robotoff API + Schedulers + Workers, PostgreSQL DB mongo-dev [#45] closed Debian 10 2 16GB 40GB MongoDB running in Docker off-net [#41] closed Debian 10 4 16GB (PO needs > 6GB) 192GB 0GB ProductOpener frontend + backend, MongoDB, PostgreSQL, Memcached robotoff-dev [#40] closed Debian 10 4 8 Gb 32 Gb 100 Gb robotoff, elastic search, tensorflow, postgresql Matomo [#24] closed Debian 10 No idea. No idea. No idea. No idea. LAMP robotoff-org [#20] closed Debian 10 4 8 Gb 32 Gb 100 Gb robotoff, elastic search, tensorflow, postgresql Request a VM","title":"Virtual Machines"},{"location":"cicd/","text":"Continous Integration and Continuous Delivery # This document presents the Continous Integration and Continous Delivery (CICD) process at Open Food Facts. The information below is valid for most OFF repositories containing apps deployed on OFF servers. A summary table is given at the end to get the status of the deployment / test automation across different OFF repositories. Technology Stack # This section gives an overview on the technologies used to automate the CI and CD process at Open Food Facts. Feel free to skip it if you already know these technologies ! Docker # See docker Makefile ( uniformity ) # A Makefile proves very useful for wrapping up and centralizing all the commands we run (locally or on remote environments) and have a lighter development and deployment process using simpler aliases. Open Food Facts contributors should know that the Makefile is the simplest entrypoint for collaborating to Open Food Facts repos, although they are not mandatory if the user have a good knowledge of the application at hand. Makefile s should stay away from complexities when possible and be streamlined enough that we can easily understand what the commands stand for. It is important to be able to switch between the different Open Food Facts repositories but keep the same interface to set up our local developer workflow. Most of the existing OFF repos try to have the commands below in their Makefile : make dev is the only command needed to set up a complete developer environment after cloning the repo. It should hopefully never fail, but if it does anyway please open an issue to track it. make up , make down , make hdown , make restart , make status map exactly to docker-compose commands, respectively docker-compose up , docker-compose down , docker-compose down -v , docker-compose restart and docker-compose ps . Using a different .env file (e.g: .env.test ) is supported by setting the env variable ENV_FILE=.env.test so that the Make commands still work. GitHub Actions # We use GitHub Actions to automatically run tests pull requests (unit / integration / lint / performance), but also to build and deploy Docker containers to pre-production and production environments. GitHub actions workflows are stored in .github/workflows in each repository. In order to ease the deployments of new repositories and have uniform deployments across OFF apps, 2 GitHub Actions workflow templates were created, which can be setup by going to the Actions tab on the GitHub repo and selecting the \"Docker image build\" and \"Docker Compose Deployment\" actions: Continuous Integration # Continous integration (CI) is the practice of automating the integration of code changes from multiple contributors into a single software project. It is thus essential in the DevOps space, as it allows developers to frequently merge code changes into a central repository where builds and tests are run. A good CI process consists of the following: On pull requests , run style checks as well as unit , integration , and performance tests. On merge to main branch , deploy to a live environment and run integration tests on it. Continuous Delivery # Continuous Delivery (CD) is the process of automatically deploying build artifacts (Docker containers, tars, static assets, data, etc\u2026) to the target environment servers. Deployment model # The diagram below represents a standard development git tree and how the deployment process wraps up around it. It shows the developers workflow to get a change into net and org environments, with the following principles in mind: A pull request needs to be tested automatically before an administrator can merge it. Additionally, an administrator can deploy it to pre-production by pushing the PR branch to a new branch called deploy-<something> . Any change needs to be successfully deployed to pre-production before it is deployed to production. No humans should have to worry about making releases. The process should be fully automated. The following diagram represents the same process, but seen from a persona perspective (Developer, Maintainer, Release Administrator): Summary: On pull requests : run the pull_request.yml Github Action workflow that builds and runs unit / integration / load tests locally. On commit to master / main : run the release-please.yml workflow that will create a release branch or add the commit to an existing release branch ; run the container-build.yml workflow that will build the container image and tag it with the merge commit SHA run the container-deploy.yml workflow that will deploy the container image to the pre-production .net environment . On merge of branches matching release-v*.*.* , run the release-please.yml workflow that will create the v*.*.* tag. On push to tags matching v*.*.* , run the container-deploy.yml workflow that will reload the container image to the off-org environment. The version tag is automatically created when merging a release branch. On push to branches matching deploy-* , run the container-deploy.yml workflow that will push the image to the pre-production ( .net ) environment. This is useful to quickly test a pull request in the pre-prod environment to see if it breaks anything. Notes: deployment process to pre-prod .net and production .org environments is identical ( container-deploy.yml ), as pre-production should be as close as possible to the production environment to avoid any pitfalls when pushing a release to production. the special branches ( deploy-* and release-v*.*.* ) MUST be protected branches . deploying to production .org can also be done manually by pushing a tag to the repository that follows semantic versioning: git tag v1.1.0tc1 && git push --tags although this is not recommended as it contradicts with the automated deployment workflow. release please has to use a user PAT (Access Token) to be able to run release please. See #84 use github SECRETS only for real secrets ! To set environment variables that depends on the deploy target, use environment modification with a if directive. Rollbacks # In the advent where pre-production or production environments are broken by a 'bad' change, it is important to be able to rollback to the previous version. Automated rollbacks are tricky with docker-compose (a discussion to migrate to docker swarm should be envisioned), but manual rollbacks are easily done. The steps for executing a manual rollback are as follow: SSH to the QEMU VM (either pre-production or production) and go to the deployment folder (usually named after the GitHub environment we are deploying to) Replace the TAG variable by sha-<COMMIT_SHA> (where COMMIT_SHA is the last 'good' commit) in the .env file and restore it in the checked out repository. Copy the .env file outside of the checked out repository so that it can be restored later. Run git checkout -qf <COMMIT_SHA> of the last 'good' commit. move the .env file from previous step into the directory Note that since deployments are automated, the following alternative also exists and is safer, although it can be a bit longer considering the git process: Revert the 'bad' commit ( git revert <COMMIT_SHA> ) and make a pull request Push to a branch called deploy-<something> to deploy to pre-prod Merge the pull request : the release workflow runs and creates a new release branch. Merge the release branch : the revert will be deployed in production. CICD status # The current status of the automation of the deployment and testing processes across Open Food Facts repositories is as follows: Repository Continuous Testing Continuous Deployment Pre-production deployment Production deployment Release automation openfoodfacts-server :heavy_exclamation_mark: Weak (lint, unit) :heavy_check_mark: Good :heavy_check_mark: Automated :heavy_exclamation_mark: Manual :heavy_check_mark: Automated robotoff :heavy_exclamation_mark: Weak (lint, unit) :heavy_check_mark: Good :heavy_check_mark: Automated :heavy_check_mark: Automated :heavy_check_mark: Automated robotoff-ann :heavy_exclamation_mark: Weak (lint, unit) :heavy_check_mark: Good :heavy_check_mark: Automated :heavy_check_mark: Automated :heavy_check_mark: Automated impactestimator :heavy_exclamation_mark: Weak :heavy_check_mark: Good :heavy_check_mark: Automated :heavy_check_mark: Automated disabled :heavy_check_mark: Automated disabled openfoodfacts-monitoring None :heavy_check_mark: Good :heavy_check_mark: Automated :heavy_check_mark: Automated :heavy_check_mark: Automated smooth-app :heavy_exclamation_mark: Weak (lint, flutter) :heavy_check_mark: Good None :heavy_check_mark: Automated (deployment to Android + IOS stores) :heavy_check_mark: Automated FIXME: add taxonomy-editor, openfoodfacts-events, facets-knowledge-panels, robotoff-ml Q&A # Container deployment is failing, how do I fix it ? # Have a look at the Actions tab in the GitHub repository and finds out why it is failing. If the process has trouble checking out the appropriate commit sha, you might have to ssh to the machine, bring down the deployment ( make down ) and delete the repository folder. It will be automatically re-created for the next deployment. I forgot to set an env variable on GitHub, can I re-trigger the deployment ? # Yes, simply make an empty commit to your deployment branch: git commit --allow-empty -m \"trigger\" && git push . You can also re-trigger a deployment in the repo's Actions tab, assuming you are a repo maintainer. How do I set up a deployment on a new repository ? # Go to the Actions tab and click on New workflow : scroll down to Workflows created by Open Food Facts and click on Set up this workflow for both Docker image build and Docker Compose Deployment workflows. It will generate pre-configured workflow files in .github/workflows that you can then tweak to your needs and commit to the repository. You will also need to create two GitHub environments (in Settings > Environments) and set up a few secrets needed by the deployment, mainly HOST , SSH_PRIVATE_KEY , PROXY_HOST and USERNAME . What do I do if the deployment fails after merging my PR to the main or master branch ? # Contact an OFF administrator to analyze why it is failing: the admin might have to revert your PR to restore the previous working version in pre-production; you can then continue to work on your branch to fix the problem, and make another PR. Ask the OFF administrator to deploy your PR before merging it, so that it is known ahead of time if the PR will break the pre-production environment. I don't have much confidence in the next release, can I make a release candidate before publishing the official release ? # Yes, assuming your next version is v1.1.0 , just create a git tag following semantic versioning using git tag v1.1.0rc1 && git push --tags and the automated process will deploy this release candidate to production.","title":"Continous Integration and Continuous Delivery"},{"location":"cicd/#continous-integration-and-continuous-delivery","text":"This document presents the Continous Integration and Continous Delivery (CICD) process at Open Food Facts. The information below is valid for most OFF repositories containing apps deployed on OFF servers. A summary table is given at the end to get the status of the deployment / test automation across different OFF repositories.","title":"Continous Integration and Continuous Delivery"},{"location":"cicd/#technology-stack","text":"This section gives an overview on the technologies used to automate the CI and CD process at Open Food Facts. Feel free to skip it if you already know these technologies !","title":"Technology Stack"},{"location":"cicd/#docker","text":"See docker","title":"Docker"},{"location":"cicd/#makefile-uniformity","text":"A Makefile proves very useful for wrapping up and centralizing all the commands we run (locally or on remote environments) and have a lighter development and deployment process using simpler aliases. Open Food Facts contributors should know that the Makefile is the simplest entrypoint for collaborating to Open Food Facts repos, although they are not mandatory if the user have a good knowledge of the application at hand. Makefile s should stay away from complexities when possible and be streamlined enough that we can easily understand what the commands stand for. It is important to be able to switch between the different Open Food Facts repositories but keep the same interface to set up our local developer workflow. Most of the existing OFF repos try to have the commands below in their Makefile : make dev is the only command needed to set up a complete developer environment after cloning the repo. It should hopefully never fail, but if it does anyway please open an issue to track it. make up , make down , make hdown , make restart , make status map exactly to docker-compose commands, respectively docker-compose up , docker-compose down , docker-compose down -v , docker-compose restart and docker-compose ps . Using a different .env file (e.g: .env.test ) is supported by setting the env variable ENV_FILE=.env.test so that the Make commands still work.","title":"Makefile (uniformity)"},{"location":"cicd/#github-actions","text":"We use GitHub Actions to automatically run tests pull requests (unit / integration / lint / performance), but also to build and deploy Docker containers to pre-production and production environments. GitHub actions workflows are stored in .github/workflows in each repository. In order to ease the deployments of new repositories and have uniform deployments across OFF apps, 2 GitHub Actions workflow templates were created, which can be setup by going to the Actions tab on the GitHub repo and selecting the \"Docker image build\" and \"Docker Compose Deployment\" actions:","title":"GitHub Actions"},{"location":"cicd/#continuous-integration","text":"Continous integration (CI) is the practice of automating the integration of code changes from multiple contributors into a single software project. It is thus essential in the DevOps space, as it allows developers to frequently merge code changes into a central repository where builds and tests are run. A good CI process consists of the following: On pull requests , run style checks as well as unit , integration , and performance tests. On merge to main branch , deploy to a live environment and run integration tests on it.","title":"Continuous Integration"},{"location":"cicd/#continuous-delivery","text":"Continuous Delivery (CD) is the process of automatically deploying build artifacts (Docker containers, tars, static assets, data, etc\u2026) to the target environment servers.","title":"Continuous Delivery"},{"location":"cicd/#deployment-model","text":"The diagram below represents a standard development git tree and how the deployment process wraps up around it. It shows the developers workflow to get a change into net and org environments, with the following principles in mind: A pull request needs to be tested automatically before an administrator can merge it. Additionally, an administrator can deploy it to pre-production by pushing the PR branch to a new branch called deploy-<something> . Any change needs to be successfully deployed to pre-production before it is deployed to production. No humans should have to worry about making releases. The process should be fully automated. The following diagram represents the same process, but seen from a persona perspective (Developer, Maintainer, Release Administrator): Summary: On pull requests : run the pull_request.yml Github Action workflow that builds and runs unit / integration / load tests locally. On commit to master / main : run the release-please.yml workflow that will create a release branch or add the commit to an existing release branch ; run the container-build.yml workflow that will build the container image and tag it with the merge commit SHA run the container-deploy.yml workflow that will deploy the container image to the pre-production .net environment . On merge of branches matching release-v*.*.* , run the release-please.yml workflow that will create the v*.*.* tag. On push to tags matching v*.*.* , run the container-deploy.yml workflow that will reload the container image to the off-org environment. The version tag is automatically created when merging a release branch. On push to branches matching deploy-* , run the container-deploy.yml workflow that will push the image to the pre-production ( .net ) environment. This is useful to quickly test a pull request in the pre-prod environment to see if it breaks anything. Notes: deployment process to pre-prod .net and production .org environments is identical ( container-deploy.yml ), as pre-production should be as close as possible to the production environment to avoid any pitfalls when pushing a release to production. the special branches ( deploy-* and release-v*.*.* ) MUST be protected branches . deploying to production .org can also be done manually by pushing a tag to the repository that follows semantic versioning: git tag v1.1.0tc1 && git push --tags although this is not recommended as it contradicts with the automated deployment workflow. release please has to use a user PAT (Access Token) to be able to run release please. See #84 use github SECRETS only for real secrets ! To set environment variables that depends on the deploy target, use environment modification with a if directive.","title":"Deployment model"},{"location":"cicd/#rollbacks","text":"In the advent where pre-production or production environments are broken by a 'bad' change, it is important to be able to rollback to the previous version. Automated rollbacks are tricky with docker-compose (a discussion to migrate to docker swarm should be envisioned), but manual rollbacks are easily done. The steps for executing a manual rollback are as follow: SSH to the QEMU VM (either pre-production or production) and go to the deployment folder (usually named after the GitHub environment we are deploying to) Replace the TAG variable by sha-<COMMIT_SHA> (where COMMIT_SHA is the last 'good' commit) in the .env file and restore it in the checked out repository. Copy the .env file outside of the checked out repository so that it can be restored later. Run git checkout -qf <COMMIT_SHA> of the last 'good' commit. move the .env file from previous step into the directory Note that since deployments are automated, the following alternative also exists and is safer, although it can be a bit longer considering the git process: Revert the 'bad' commit ( git revert <COMMIT_SHA> ) and make a pull request Push to a branch called deploy-<something> to deploy to pre-prod Merge the pull request : the release workflow runs and creates a new release branch. Merge the release branch : the revert will be deployed in production.","title":"Rollbacks"},{"location":"cicd/#cicd-status","text":"The current status of the automation of the deployment and testing processes across Open Food Facts repositories is as follows: Repository Continuous Testing Continuous Deployment Pre-production deployment Production deployment Release automation openfoodfacts-server :heavy_exclamation_mark: Weak (lint, unit) :heavy_check_mark: Good :heavy_check_mark: Automated :heavy_exclamation_mark: Manual :heavy_check_mark: Automated robotoff :heavy_exclamation_mark: Weak (lint, unit) :heavy_check_mark: Good :heavy_check_mark: Automated :heavy_check_mark: Automated :heavy_check_mark: Automated robotoff-ann :heavy_exclamation_mark: Weak (lint, unit) :heavy_check_mark: Good :heavy_check_mark: Automated :heavy_check_mark: Automated :heavy_check_mark: Automated impactestimator :heavy_exclamation_mark: Weak :heavy_check_mark: Good :heavy_check_mark: Automated :heavy_check_mark: Automated disabled :heavy_check_mark: Automated disabled openfoodfacts-monitoring None :heavy_check_mark: Good :heavy_check_mark: Automated :heavy_check_mark: Automated :heavy_check_mark: Automated smooth-app :heavy_exclamation_mark: Weak (lint, flutter) :heavy_check_mark: Good None :heavy_check_mark: Automated (deployment to Android + IOS stores) :heavy_check_mark: Automated FIXME: add taxonomy-editor, openfoodfacts-events, facets-knowledge-panels, robotoff-ml","title":"CICD status"},{"location":"cicd/#qa","text":"","title":"Q&amp;A"},{"location":"cicd/#container-deployment-is-failing-how-do-i-fix-it","text":"Have a look at the Actions tab in the GitHub repository and finds out why it is failing. If the process has trouble checking out the appropriate commit sha, you might have to ssh to the machine, bring down the deployment ( make down ) and delete the repository folder. It will be automatically re-created for the next deployment.","title":"Container deployment is failing, how do I fix it ?"},{"location":"cicd/#i-forgot-to-set-an-env-variable-on-github-can-i-re-trigger-the-deployment","text":"Yes, simply make an empty commit to your deployment branch: git commit --allow-empty -m \"trigger\" && git push . You can also re-trigger a deployment in the repo's Actions tab, assuming you are a repo maintainer.","title":"I forgot to set an env variable on GitHub, can I re-trigger the deployment ?"},{"location":"cicd/#how-do-i-set-up-a-deployment-on-a-new-repository","text":"Go to the Actions tab and click on New workflow : scroll down to Workflows created by Open Food Facts and click on Set up this workflow for both Docker image build and Docker Compose Deployment workflows. It will generate pre-configured workflow files in .github/workflows that you can then tweak to your needs and commit to the repository. You will also need to create two GitHub environments (in Settings > Environments) and set up a few secrets needed by the deployment, mainly HOST , SSH_PRIVATE_KEY , PROXY_HOST and USERNAME .","title":"How do I set up a deployment on a new repository ?"},{"location":"cicd/#what-do-i-do-if-the-deployment-fails-after-merging-my-pr-to-the-main-or-master-branch","text":"Contact an OFF administrator to analyze why it is failing: the admin might have to revert your PR to restore the previous working version in pre-production; you can then continue to work on your branch to fix the problem, and make another PR. Ask the OFF administrator to deploy your PR before merging it, so that it is known ahead of time if the PR will break the pre-production environment.","title":"What do I do if the deployment fails after merging my PR to the main or master branch ?"},{"location":"cicd/#i-dont-have-much-confidence-in-the-next-release-can-i-make-a-release-candidate-before-publishing-the-official-release","text":"Yes, assuming your next version is v1.1.0 , just create a git tag following semantic versioning using git tag v1.1.0rc1 && git push --tags and the automated process will deploy this release candidate to production.","title":"I don't have much confidence in the next release, can I make a release candidate before publishing the official release ?"},{"location":"discourse/","text":"Discourse # Discourse is a forum application. It is installed on our proxmox infrastructure. FIXME: more details ? Mail # Mail is very important as a lot of notifications are sent by the forum. Mail can be tested at https://forum.openfoodfacts.org/admin/email We use promox mail gateway . \u26a0 Warning: the sender email have to be on main domain , NOT forum.openfoodfacts.org.","title":"Discourse"},{"location":"discourse/#discourse","text":"Discourse is a forum application. It is installed on our proxmox infrastructure. FIXME: more details ?","title":"Discourse"},{"location":"discourse/#mail","text":"Mail is very important as a lot of notifications are sent by the forum. Mail can be tested at https://forum.openfoodfacts.org/admin/email We use promox mail gateway . \u26a0 Warning: the sender email have to be on main domain , NOT forum.openfoodfacts.org.","title":"Mail"},{"location":"docker/","text":"Docker at Open Food Facts # Technology stack # See also: Continous Integration and Continuous Delivery Docker ( idempotency ) # The process of dockerizing applications is an important step towards achieving a modern-day great Continuous Integration and Continous Delivery process. Dockerization avoids common pitfalls in deployment processes, such as having to write idempotent deployment scripts to deploy an application. A Docker container build can be run many times producing each time the same resulting image. Most of Open Food Facts git repositories have a Dockerfile that is used both to test changes locally, but also to ease automated testing and automated deployments through idempotency (a.k.a repeatabilty, or the ability to re-run a deployment X times without problems). Docker-Compose ( orchestration ) # We use docker-compose to deploy our applications to our servers: it is a simple orchestator that can deploy to a single machine at once. An alternative like docker swarm or kubernetes could be considered in the future to deploy to multiple machines at scale, but it currently does not make much sense considering the small amount of servers used to run Open Food Facts. Env file ( secret management ) # Every OFF repo has a .env file that contains the secrets needed by the application to run properly. The .env file is loaded by the docker-compose commands. The default .env file in the repo is ready for local development and should rarely be modified. In pre-production and production, the .env file is populated by the GitHub action (using GitHub environment secrets) before deploying to the target environment. Warnings: * The default .env file should rarely change. If you need a different environment locally, create a new env file (e.g .env.test ) and set ENV_FILE=.env.test before running the Makefile commands. * Do not commit your env files to the repos ! * you may use direnv to override some variables on a folder basis. See how-to for openfoodfacts-server Best Practice for Docker containers # Here are some important rules. The document also explain why we follow those rules. From time to time you might have good reason to bend or break the rules, but only do it if needed. Rules also enables having a consistent experiences between projects. Images # If possible use an official image. If you use another image take a look on how it's built. It's important to be future proof and to be able to rely on a good base. We try to favor images based on debian, if really needed you can use arch or other architecture. This is to keep consistent and manageable to admin and developers to debug images. Enable configuration through environment # We really want to be able to run the same project multiple time on same machine / server. For that we need to ensure that we can configure the docker-compose project. You have two mechanism to configure the docker-compose: - docker-compose file composition, use it for structural changes - .env is the prefered way to change configuration (but can't solve it all) Avoid too generic name for services. Like postgresql it's better to use myproject_db . Every public network should have a configurable name. To be able to run the project more than once, to also be able to connect docker-compose between them. Every port exposure should be changeable through env. We want to be able to change port (run multiple time same project), and to keep exposure to localhost on dev (avoid exposure on public wifi). Never use container_name (let docker-compose build the name) Never user static names for volumes, let docker-compose add a prefix try to stitch to the default network and setup a network with a configurable name for exchanges with other projects services (that is located in other docker-compose). restart directive should always be configurable. While we want auto-start in production, we don't want it on dev machines. always prefer prod defaults for variable, or safe default. For example it's better to only expose to localhost. And if a variable is missing in prod it should never create a disaster. Dev config # The docker-compose.yml should be as close as possible to production. Put specific configurations in a docker/dev.yml The build part should only be in dev docker-compose. (see why we use images only in prod) use a USER_UID / USER_GID parameter to align docker user with host user uid. This avoid having problems with file permissions. bind mount code so that it's easy to develop. make it possible to connect the project between them on dev, as if it was on production. This enables manual integration testing of all the project all together. Prod config # Here I talk about production, but staging is as much possible identical to prod. There should be no build in production, containers should be defined by their images. We want to be able to redeploy easily only depending on the container registry, not external packages repositories and so on. every volume containing production data should be external (to avoid a docker-compose down fatality if -v is added). The Makefile should contain a creation target ( create_external_volumes ) shared network name should have a prefix which reflect the environment: like stagging / prod COMPOSE_PROJECT_NAME should use _: like po_stagging, po_prod, ... see also https://github.com/openfoodfacts/openfoodfacts-infrastructure/issues/146 Security # Try hard not to use root in docker images. (it's ok if root is only used to launch a service that immediately drops privileges) for containers that contains code, or elements that are edited by developers and bind mounted at dev time expose to localhost only whenever possible. Only expose to all interfaces when needed be aware that docker use an alternative table for ip tables. A blocking INPUT or OUTPUT rule won't apply to docker exposed port. You can instead add rules to DOCKER-USER chain.","title":"Docker at Open Food Facts"},{"location":"docker/#docker-at-open-food-facts","text":"","title":"Docker at Open Food Facts"},{"location":"docker/#technology-stack","text":"See also: Continous Integration and Continuous Delivery","title":"Technology stack"},{"location":"docker/#docker-idempotency","text":"The process of dockerizing applications is an important step towards achieving a modern-day great Continuous Integration and Continous Delivery process. Dockerization avoids common pitfalls in deployment processes, such as having to write idempotent deployment scripts to deploy an application. A Docker container build can be run many times producing each time the same resulting image. Most of Open Food Facts git repositories have a Dockerfile that is used both to test changes locally, but also to ease automated testing and automated deployments through idempotency (a.k.a repeatabilty, or the ability to re-run a deployment X times without problems).","title":"Docker (idempotency)"},{"location":"docker/#docker-compose-orchestration","text":"We use docker-compose to deploy our applications to our servers: it is a simple orchestator that can deploy to a single machine at once. An alternative like docker swarm or kubernetes could be considered in the future to deploy to multiple machines at scale, but it currently does not make much sense considering the small amount of servers used to run Open Food Facts.","title":"Docker-Compose (orchestration)"},{"location":"docker/#env-file-secret-management","text":"Every OFF repo has a .env file that contains the secrets needed by the application to run properly. The .env file is loaded by the docker-compose commands. The default .env file in the repo is ready for local development and should rarely be modified. In pre-production and production, the .env file is populated by the GitHub action (using GitHub environment secrets) before deploying to the target environment. Warnings: * The default .env file should rarely change. If you need a different environment locally, create a new env file (e.g .env.test ) and set ENV_FILE=.env.test before running the Makefile commands. * Do not commit your env files to the repos ! * you may use direnv to override some variables on a folder basis. See how-to for openfoodfacts-server","title":"Env file (secret management)"},{"location":"docker/#best-practice-for-docker-containers","text":"Here are some important rules. The document also explain why we follow those rules. From time to time you might have good reason to bend or break the rules, but only do it if needed. Rules also enables having a consistent experiences between projects.","title":"Best Practice for Docker containers"},{"location":"docker/#images","text":"If possible use an official image. If you use another image take a look on how it's built. It's important to be future proof and to be able to rely on a good base. We try to favor images based on debian, if really needed you can use arch or other architecture. This is to keep consistent and manageable to admin and developers to debug images.","title":"Images"},{"location":"docker/#enable-configuration-through-environment","text":"We really want to be able to run the same project multiple time on same machine / server. For that we need to ensure that we can configure the docker-compose project. You have two mechanism to configure the docker-compose: - docker-compose file composition, use it for structural changes - .env is the prefered way to change configuration (but can't solve it all) Avoid too generic name for services. Like postgresql it's better to use myproject_db . Every public network should have a configurable name. To be able to run the project more than once, to also be able to connect docker-compose between them. Every port exposure should be changeable through env. We want to be able to change port (run multiple time same project), and to keep exposure to localhost on dev (avoid exposure on public wifi). Never use container_name (let docker-compose build the name) Never user static names for volumes, let docker-compose add a prefix try to stitch to the default network and setup a network with a configurable name for exchanges with other projects services (that is located in other docker-compose). restart directive should always be configurable. While we want auto-start in production, we don't want it on dev machines. always prefer prod defaults for variable, or safe default. For example it's better to only expose to localhost. And if a variable is missing in prod it should never create a disaster.","title":"Enable configuration through environment"},{"location":"docker/#dev-config","text":"The docker-compose.yml should be as close as possible to production. Put specific configurations in a docker/dev.yml The build part should only be in dev docker-compose. (see why we use images only in prod) use a USER_UID / USER_GID parameter to align docker user with host user uid. This avoid having problems with file permissions. bind mount code so that it's easy to develop. make it possible to connect the project between them on dev, as if it was on production. This enables manual integration testing of all the project all together.","title":"Dev config"},{"location":"docker/#prod-config","text":"Here I talk about production, but staging is as much possible identical to prod. There should be no build in production, containers should be defined by their images. We want to be able to redeploy easily only depending on the container registry, not external packages repositories and so on. every volume containing production data should be external (to avoid a docker-compose down fatality if -v is added). The Makefile should contain a creation target ( create_external_volumes ) shared network name should have a prefix which reflect the environment: like stagging / prod COMPOSE_PROJECT_NAME should use _: like po_stagging, po_prod, ... see also https://github.com/openfoodfacts/openfoodfacts-infrastructure/issues/146","title":"Prod config"},{"location":"docker/#security","text":"Try hard not to use root in docker images. (it's ok if root is only used to launch a service that immediately drops privileges) for containers that contains code, or elements that are edited by developers and bind mounted at dev time expose to localhost only whenever possible. Only expose to all interfaces when needed be aware that docker use an alternative table for ip tables. A blocking INPUT or OUTPUT rule won't apply to docker exposed port. You can instead add rules to DOCKER-USER chain.","title":"Security"},{"location":"docker_architecture/","text":"Docker architecture # Below is a diagram of how the various OFF apps interact within the Docker environments: Docker server for staging # The 200 VM on ovh2 is the serveur hosting the docker for stagging. Docker server for prod # The 201 VM on ovh2 is the serveur hosting the docker for production. Useful commands # List all mapped ports on a VM: docker ps --format 'table {{.Names}}\\t{{.Ports}}' | grep '\\->'","title":"Docker architecture"},{"location":"docker_architecture/#docker-architecture","text":"Below is a diagram of how the various OFF apps interact within the Docker environments:","title":"Docker architecture"},{"location":"docker_architecture/#docker-server-for-staging","text":"The 200 VM on ovh2 is the serveur hosting the docker for stagging.","title":"Docker server for staging"},{"location":"docker_architecture/#docker-server-for-prod","text":"The 201 VM on ovh2 is the serveur hosting the docker for production.","title":"Docker server for prod"},{"location":"docker_architecture/#useful-commands","text":"List all mapped ports on a VM: docker ps --format 'table {{.Names}}\\t{{.Ports}}' | grep '\\->'","title":"Useful commands"},{"location":"docker_onboarding/","text":"Onboarding # Focusing on the developer onboarding process into a project is a very important step to make a repository popular: no one wants to struggle for hours to setup an application and start developing on it. A streamlined and easy setup process is thus critical to having meaningful contributions on an open-source repository. Rules of the OFF on-boarding process # The setup steps should be as simple as possible and stick by the following rules: Respect the user's time : reduce the amount of time needed to setup a project to the least amount possible. but also make it as fast as possible to take into account code modifications (ideally live reload, if needed a container restart) Make it easy for non-developers to contribute : the dev setup should not require a high comprehension of the application at hand. Repeatable and tested developer workflow : the dev setup should be automated and tested with every pull request to ensure that it does not break accidentally when making changes. Make it possible to reach a wide audience : try to make the dev deployment platform agnostic, at least for commands used to develop in a normal process. On windows, git comes with git bash which should be priviledge as a console. You can use symlink on windows. The make dev command should work across all repos to streamline the applications setup process.","title":"Onboarding"},{"location":"docker_onboarding/#onboarding","text":"Focusing on the developer onboarding process into a project is a very important step to make a repository popular: no one wants to struggle for hours to setup an application and start developing on it. A streamlined and easy setup process is thus critical to having meaningful contributions on an open-source repository.","title":"Onboarding"},{"location":"docker_onboarding/#rules-of-the-off-on-boarding-process","text":"The setup steps should be as simple as possible and stick by the following rules: Respect the user's time : reduce the amount of time needed to setup a project to the least amount possible. but also make it as fast as possible to take into account code modifications (ideally live reload, if needed a container restart) Make it easy for non-developers to contribute : the dev setup should not require a high comprehension of the application at hand. Repeatable and tested developer workflow : the dev setup should be automated and tested with every pull request to ensure that it does not break accidentally when making changes. Make it possible to reach a wide audience : try to make the dev deployment platform agnostic, at least for commands used to develop in a normal process. On windows, git comes with git bash which should be priviledge as a console. You can use symlink on windows. The make dev command should work across all repos to streamline the applications setup process.","title":"Rules of the OFF on-boarding process"},{"location":"folksonomy/","text":"Folksonomy API # Folksonomy is a service to allow contributors to freely add labels and values to products. The code is at https://github.com/openfoodfacts/folksonomy_api/ Deployment # Folksonomy is deployed on a LXC container. (108 at the time of writing) Code is in /home/folksonomy/folksonomy_api It is started thanks to a systemd unit: folksonomy.service (config at /etc/systemd/system/folksonomy.service ) Server is running uvicorn on port 8000 with user folksonomy. It is served behind the NGINX reverse proxy Useful commands # Status (reload/restart/etc.): systemctl status folksonomy See service logs: sudo journalctl -u folksonomy","title":"Folksonomy API"},{"location":"folksonomy/#folksonomy-api","text":"Folksonomy is a service to allow contributors to freely add labels and values to products. The code is at https://github.com/openfoodfacts/folksonomy_api/","title":"Folksonomy API"},{"location":"folksonomy/#deployment","text":"Folksonomy is deployed on a LXC container. (108 at the time of writing) Code is in /home/folksonomy/folksonomy_api It is started thanks to a systemd unit: folksonomy.service (config at /etc/systemd/system/folksonomy.service ) Server is running uvicorn on port 8000 with user folksonomy. It is served behind the NGINX reverse proxy","title":"Deployment"},{"location":"folksonomy/#useful-commands","text":"Status (reload/restart/etc.): systemctl status folksonomy See service logs: sudo journalctl -u folksonomy","title":"Useful commands"},{"location":"linux-server/","text":"Linux server # Here are some guidelines for linux servers. Note that we have some servers (which are bare metal installs. While others are proxmox hosts . On proxmox some VM are lxc containers, while other are QEMU VM. Every server is referenced in CT and VM list of OFF infrastructure Etckeeper # We use etckeeper with git backend on as much server as possible. See https://etckeeper.branchable.com/README/ So whenever you make a change to /etc . When possible before making your change, as root, do a git status and then etckeeper commit \"save before changes\" . And after, do a etckeeper commit \"<a descriptive message>\" afterwards. Email # We use either postfix or exim as a satellite of a smart_host. Every outgoing mail must pass through the proxmox mail gateway, which is registered in spf record and adds DKIM signature. For configuration, see mail - Servers Iptables # We use iptables on a lot of servers (generally host servers). We use iptables-persistent to save rules, and restore them at startup. On ovh servers, rules are in /etc/iptables/rule.v{4,6} On free servers, rules are in /etc/iptables.up.rules Remember, that docker as it's own chains that are not affected by INPUT and OUTPUT rules. So it won't block a port exposed by docker. Use DOCKER-USER chain for that. see https://docs.docker.com/network/iptables/","title":"Linux server"},{"location":"linux-server/#linux-server","text":"Here are some guidelines for linux servers. Note that we have some servers (which are bare metal installs. While others are proxmox hosts . On proxmox some VM are lxc containers, while other are QEMU VM. Every server is referenced in CT and VM list of OFF infrastructure","title":"Linux server"},{"location":"linux-server/#etckeeper","text":"We use etckeeper with git backend on as much server as possible. See https://etckeeper.branchable.com/README/ So whenever you make a change to /etc . When possible before making your change, as root, do a git status and then etckeeper commit \"save before changes\" . And after, do a etckeeper commit \"<a descriptive message>\" afterwards.","title":"Etckeeper"},{"location":"linux-server/#email","text":"We use either postfix or exim as a satellite of a smart_host. Every outgoing mail must pass through the proxmox mail gateway, which is registered in spf record and adds DKIM signature. For configuration, see mail - Servers","title":"Email"},{"location":"linux-server/#iptables","text":"We use iptables on a lot of servers (generally host servers). We use iptables-persistent to save rules, and restore them at startup. On ovh servers, rules are in /etc/iptables/rule.v{4,6} On free servers, rules are in /etc/iptables.up.rules Remember, that docker as it's own chains that are not affected by INPUT and OUTPUT rules. So it won't block a port exposed by docker. Use DOCKER-USER chain for that. see https://docs.docker.com/network/iptables/","title":"Iptables"},{"location":"mail/","text":"Mail on Open Food Facts infrastructure # Because mail is difficult to setup, we use Proxmox Mail Gateway as a relay to all servers. It ensure correct SPF, but also it adds DKIM signature. No server receive mail, but they should be able to send them. \ud83d\udcddNote: We ONLY support emails address on primary domain ( openfoodfacts.org ) and we DO NOT support emails on sub domains (aka xxx.openfoodfacts.org ). Proxmox Mail Gateway # This is the pmg lxc VM (aka 102 ) currently on ovh1.openfoodfacts.org . The install follows procedure starting from a debian distribution . Administration # You can access the administration of pmg by using a tunnel on ovh1: ssh -L 8006:10.1.0.102:8006 ovh1.openfoodfacts.org -N then connect to https://localhost:8006 (beware the s of https !) You must have an account to connect to the service. The most important tools you will find is Administration where you can see queues of deferred mails or logs (on the administration entry). See https://pmg.proxmox.com/pmg-docs/pmg-admin-guide.html#_administration Notable configurations options # First bare in mind that even if proxmox mail gateway is built to handle outgoing and incoming emails, we want to use it only for outgoing emails. We registered an letsencrypt account with tech@openfoodfacts.org Administrator email is tech@openfoodfacts.org Relay is set to smtp-relay.gmail.com, although we should not need it (this is for incoming mails). SMTP private port is set to 25 and public port to 26 In networks tab, there are all servers ips, though because of redirects, PMG sees each incoming request as issued by 10.1.0.1 . TLS (TLS tab) is enabled DKIM is configured with label pmg-openfoodfacts , with a key size of 2048, the public key was reported in DNS configuration see also installation report redirects # On DNS: * pmg.openfoodfacts.org is a CNAME to ovh1.openfoodfacts.org * ovh1 ip address is reported in the spf entry of the DNS * there is an entry for the dkim key An iptable rule on host ( ovh1 ) redirects our server incoming ips to the VM smtp port (25). Private network address (corresponding to VM and docker ranges) are also redirected. To add a new machine sudo iptables -t nat -A PREROUTING -s 213.36.253.206,213.36.253.208,146.59.148.140,51.210.154.203,1.210.32.79 -d pmg.openfoodfacts.org -p tcp --dport 25 -j DNAT --to 10.1.0.102:25 don't forget to save iptables (a generic masquerading rule for VM also exists) Note that the port 25 is in fact the private (trusted) port (not the public one, as we are not receiving emails). This is a tweak in default config. Also the nginx reverse proxy (VM 101 on ovh1 ) proxies requests to pmg.openfoodfacts.org on port 80, to the proxmox mail gateway VM ( 102 ), this is needed for certificate generation through letsencrypt by the gateway. Testing that the gateway is well configured # To do a simple test, you may use: echo \"Subject: mail gateway test\" | sudo sendmail -f alex@openfoodfacts.org -v alex@openfoodfacts.org directly on the pmg server. To test quality of the configuration we can send a mail to a service such as https://www.mail-tester.com/ To test using this service: - I temporarily added an iptables rule to enable my personal ip address to be forwarded to the mail gateway (as for servers) - I added pmg.openfoodfacts.org as an smtp server in thunderbird and configured my personal address to use it - I sent a mail as requested by the service - I could then check the result - I undo iptables and thunderbird config Servers # On client servers (be it a VM, a host or a standalone server), we use either exim4 or postfix. Email aliases # We normally keeps a standard /etc/aliases . We have specific groups to receive emails: root@openfoodfacts.org and off@openfoodfacts.org You may add some redirections for non standard users to one of those groups. Do not forget to run newaliases , and etckeeper . Postfix configuration # Run: dpkg-reconfigure postfix : configuration type : satellite system mail name: openfoodfacts.org relayhost: pmg.openfoodfacts.org (with an exception for ovh1: 10.1.0.2) * mail for root: tech@openfoodfacts.org * other dest: blank * sync: no * local network: leave default * use procmail: no * default for the rest Exim4 configuration # Run: dpkg-reconfigure exim4-config : mail sent by smarthost; no local mail mail name: openfoodfacts.org listen: 127.0.0.1 other dest: off1.free.org visible domain name : openfoodfacts.org IP address smarthost: pmg.openfoodfacts.org keep DNS queries minimal (dial-up): no local: maildir format in home split config : no Testing # To test that mail is well configured, you can use: echo \"Subject: sendmail test xxx\" | sudo sendmail -f alex@openfoodfacts.org -v root or, with the mailutils package installed: echo \"test message from xxx\" | mail.mailutils -s \"test root xxx\" -r alex@openfoodfacts.org root If you do not receive the email on expected group, here are some checks: * look at logs on your server: * /var/log/exim/ if you use exim4 * /var/log/mail* if you use postfix * look at logs on pmg VM: * /var/log/mail* * ping pmg.openfoodfacts.org to verify network * nc -vz pmg.openfoodfacts.org 25 suceed, else you might have firewall problems: verify iptables on your machine and on ovh1. (nc command belongs to netcat-traditional or netcat-openbsd packages) * you can even try to send a mail manualy using smtp protocol using nc pmg.openfoodfacts.org and following a tutorial about sending email with netcat If it suceed this may mean you have a problem in username to email address translation. * /etc/mailname contains openfoodfacts.org (see also debian wiki ) * check /etc/aliases and /etc/email-adresses if you use exim4 References # Debian reference Debian wiki Postfix Debian wiki Exim","title":"Mail on Open Food Facts infrastructure"},{"location":"mail/#mail-on-open-food-facts-infrastructure","text":"Because mail is difficult to setup, we use Proxmox Mail Gateway as a relay to all servers. It ensure correct SPF, but also it adds DKIM signature. No server receive mail, but they should be able to send them. \ud83d\udcddNote: We ONLY support emails address on primary domain ( openfoodfacts.org ) and we DO NOT support emails on sub domains (aka xxx.openfoodfacts.org ).","title":"Mail on Open Food Facts infrastructure"},{"location":"mail/#proxmox-mail-gateway","text":"This is the pmg lxc VM (aka 102 ) currently on ovh1.openfoodfacts.org . The install follows procedure starting from a debian distribution .","title":"Proxmox Mail Gateway"},{"location":"mail/#administration","text":"You can access the administration of pmg by using a tunnel on ovh1: ssh -L 8006:10.1.0.102:8006 ovh1.openfoodfacts.org -N then connect to https://localhost:8006 (beware the s of https !) You must have an account to connect to the service. The most important tools you will find is Administration where you can see queues of deferred mails or logs (on the administration entry). See https://pmg.proxmox.com/pmg-docs/pmg-admin-guide.html#_administration","title":"Administration"},{"location":"mail/#notable-configurations-options","text":"First bare in mind that even if proxmox mail gateway is built to handle outgoing and incoming emails, we want to use it only for outgoing emails. We registered an letsencrypt account with tech@openfoodfacts.org Administrator email is tech@openfoodfacts.org Relay is set to smtp-relay.gmail.com, although we should not need it (this is for incoming mails). SMTP private port is set to 25 and public port to 26 In networks tab, there are all servers ips, though because of redirects, PMG sees each incoming request as issued by 10.1.0.1 . TLS (TLS tab) is enabled DKIM is configured with label pmg-openfoodfacts , with a key size of 2048, the public key was reported in DNS configuration see also installation report","title":"Notable configurations options"},{"location":"mail/#redirects","text":"On DNS: * pmg.openfoodfacts.org is a CNAME to ovh1.openfoodfacts.org * ovh1 ip address is reported in the spf entry of the DNS * there is an entry for the dkim key An iptable rule on host ( ovh1 ) redirects our server incoming ips to the VM smtp port (25). Private network address (corresponding to VM and docker ranges) are also redirected. To add a new machine sudo iptables -t nat -A PREROUTING -s 213.36.253.206,213.36.253.208,146.59.148.140,51.210.154.203,1.210.32.79 -d pmg.openfoodfacts.org -p tcp --dport 25 -j DNAT --to 10.1.0.102:25 don't forget to save iptables (a generic masquerading rule for VM also exists) Note that the port 25 is in fact the private (trusted) port (not the public one, as we are not receiving emails). This is a tweak in default config. Also the nginx reverse proxy (VM 101 on ovh1 ) proxies requests to pmg.openfoodfacts.org on port 80, to the proxmox mail gateway VM ( 102 ), this is needed for certificate generation through letsencrypt by the gateway.","title":"redirects"},{"location":"mail/#testing-that-the-gateway-is-well-configured","text":"To do a simple test, you may use: echo \"Subject: mail gateway test\" | sudo sendmail -f alex@openfoodfacts.org -v alex@openfoodfacts.org directly on the pmg server. To test quality of the configuration we can send a mail to a service such as https://www.mail-tester.com/ To test using this service: - I temporarily added an iptables rule to enable my personal ip address to be forwarded to the mail gateway (as for servers) - I added pmg.openfoodfacts.org as an smtp server in thunderbird and configured my personal address to use it - I sent a mail as requested by the service - I could then check the result - I undo iptables and thunderbird config","title":"Testing that the gateway is well configured"},{"location":"mail/#servers","text":"On client servers (be it a VM, a host or a standalone server), we use either exim4 or postfix.","title":"Servers"},{"location":"mail/#email-aliases","text":"We normally keeps a standard /etc/aliases . We have specific groups to receive emails: root@openfoodfacts.org and off@openfoodfacts.org You may add some redirections for non standard users to one of those groups. Do not forget to run newaliases , and etckeeper .","title":"Email aliases"},{"location":"mail/#postfix-configuration","text":"Run: dpkg-reconfigure postfix : configuration type : satellite system mail name: openfoodfacts.org relayhost: pmg.openfoodfacts.org (with an exception for ovh1: 10.1.0.2) * mail for root: tech@openfoodfacts.org * other dest: blank * sync: no * local network: leave default * use procmail: no * default for the rest","title":"Postfix configuration"},{"location":"mail/#exim4-configuration","text":"Run: dpkg-reconfigure exim4-config : mail sent by smarthost; no local mail mail name: openfoodfacts.org listen: 127.0.0.1 other dest: off1.free.org visible domain name : openfoodfacts.org IP address smarthost: pmg.openfoodfacts.org keep DNS queries minimal (dial-up): no local: maildir format in home split config : no","title":"Exim4 configuration"},{"location":"mail/#testing","text":"To test that mail is well configured, you can use: echo \"Subject: sendmail test xxx\" | sudo sendmail -f alex@openfoodfacts.org -v root or, with the mailutils package installed: echo \"test message from xxx\" | mail.mailutils -s \"test root xxx\" -r alex@openfoodfacts.org root If you do not receive the email on expected group, here are some checks: * look at logs on your server: * /var/log/exim/ if you use exim4 * /var/log/mail* if you use postfix * look at logs on pmg VM: * /var/log/mail* * ping pmg.openfoodfacts.org to verify network * nc -vz pmg.openfoodfacts.org 25 suceed, else you might have firewall problems: verify iptables on your machine and on ovh1. (nc command belongs to netcat-traditional or netcat-openbsd packages) * you can even try to send a mail manualy using smtp protocol using nc pmg.openfoodfacts.org and following a tutorial about sending email with netcat If it suceed this may mean you have a problem in username to email address translation. * /etc/mailname contains openfoodfacts.org (see also debian wiki ) * check /etc/aliases and /etc/email-adresses if you use exim4","title":"Testing"},{"location":"mail/#references","text":"Debian reference Debian wiki Postfix Debian wiki Exim","title":"References"},{"location":"matomo/","text":"Matomo # Matomo is the web analytics platform. Available at: https://analytics.openfoodfacts.org/ You must have a user account to access it (hopefully !). Ask for an admin to create you an account if you need it (Beware, there are personal information in the sense of GDPR like ip addresses). Ask for it to contact email. See also Install log Site setup # goto manage / websites and add a website GDPR # To be GDPR compliant (and user friendly) 1 : in your Matomo Tag, you can check the option \u00ab Disable cookies \u00bb which will disable all first party tracking cookies for Matomo. 2 To ensure that you do not store the visitor IP, which is Personally Identifiable Information (PII), please go to Administration > Privacy > Anonimyze data, to enable IP anonymization, and check you have 2 bytes or 3 bytes masked from the IP address. 3 In productopener # We use the $google_analytics variable in config to add the javascript snippet for Matomo. https://fr.matomo.org/blog/2018/04/how-to-make-matomo-gdpr-compliant-in-12-steps/ \u21a9 https://fr.matomo.org/faq/general/faq_157/ \u21a9 https://matomo.org/faq/general/configure-privacy-settings-in-matomo/#step-1-automatically-anonymize-visitor-ips \u21a9","title":"Matomo"},{"location":"matomo/#matomo","text":"Matomo is the web analytics platform. Available at: https://analytics.openfoodfacts.org/ You must have a user account to access it (hopefully !). Ask for an admin to create you an account if you need it (Beware, there are personal information in the sense of GDPR like ip addresses). Ask for it to contact email. See also Install log","title":"Matomo"},{"location":"matomo/#site-setup","text":"goto manage / websites and add a website","title":"Site setup"},{"location":"matomo/#gdpr","text":"To be GDPR compliant (and user friendly) 1 : in your Matomo Tag, you can check the option \u00ab Disable cookies \u00bb which will disable all first party tracking cookies for Matomo. 2 To ensure that you do not store the visitor IP, which is Personally Identifiable Information (PII), please go to Administration > Privacy > Anonimyze data, to enable IP anonymization, and check you have 2 bytes or 3 bytes masked from the IP address. 3","title":"GDPR"},{"location":"matomo/#in-productopener","text":"We use the $google_analytics variable in config to add the javascript snippet for Matomo. https://fr.matomo.org/blog/2018/04/how-to-make-matomo-gdpr-compliant-in-12-steps/ \u21a9 https://fr.matomo.org/faq/general/faq_157/ \u21a9 https://matomo.org/faq/general/configure-privacy-settings-in-matomo/#step-1-automatically-anonymize-visitor-ips \u21a9","title":"In productopener"},{"location":"nginx-reverse-proxy/","text":"NGINX Reverse proxy (OVH) # At OVH we have a lxc container dedicated to reverse proxy http/https applications. Network specific interface # It as a specific network configurations with two ethernet address: * one internal, to communicate with other VMs * one which is bridged on host network card, with ip fail over mechanism. Important : only the public ip should have a gateway 1 Configuring a new service # To make a new service, hosted on proxmox, available you needs to: have this service available on proxmox internal network in the DNS, CNAME you service name to proxy1.openfoodfacts.org write a configuration on nginx for this service eventually add https Steps to create Nginx configuration # we will imagine we configure my-service.openfoodfacts.net You will have to be root to do that. Login on container (101) and start a root bash. Create the basic configuration file for your service in /etc/nginx/conf.d named my-service.openfoodfacts.net.conf on machine 222 , port 8888 . Important : your file has to ends with .conf to be taken into account. It's a good idea to first test it exists, using nc and curl: nc -vz 10 .1.0.222 8888 curl http://10.1.0.222:8888 Create a config, say: server { listen 80 ; listen [::]:80 ; server_name my-service.openfoodfacts.net ; access_log /var/log/nginx/my-service.off.net.log main ; error_log /var/log/nginx/my-service.off.net.err ; location / { proxy_pass http://10.1.0.222:8888 $request_uri ; proxy_set_header Host $host ; proxy_set_header X-Real-IP $remote_addr ; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for ; proxy_set_header X-Forwarded-Proto https ; proxy_read_timeout 90 ; client_max_body_size 512M ; } } If you need more nginx docs are here We test nginx configuration is ok ( Mandatory ) 2 : $ nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful if it's ok, we reload. $ systemctl reload nginx Next step is probably to setup https (putting something in http should be an exception, with good reason for that !) Otherwise jump to EtcKeeper Adding https # Most of the time https certificates are managed on the nginx reverse proxy VM. Here is how to configure them to enable https. We use certbot to manage certificates. First prepare the service definition to answer on port 443 and 80. That is: change your current configuration to listen on 443: { server { listen 80 ; listen [::]:80 ; server_name my-service.openfoodfacts.net ; \u2026 but after it, add a new bare section for port 80: server { listen 80 ; listen [::]:80 ; server_name my-service.openfoodfacts.net ; } We will first validate our config using --test-cert option ( WARNING only skip this part if you really are used to certbot, as after too much invalid tentatives, we won't be able to renew any certificates on the domain for several hours from this IP, see letsencrypt Rate limits ) $ certbot --test-cert -d my-service.openfoodfacts.net \u2026 Enter email address (used for urgent renewal and security notices) (Enter 'c' to cancel): root@openfoodfacts.org \u2026 terms of service\u2026 (A)gree/(C)ancel: A \u2026 share email\u2026 (Y)es/(N)o: n Obtaining a new certificate Performing the following challenges: http-01 challenge for my-service.openfoodfacts.net Waiting for verification... Cleaning up challenges Deploying Certificate to VirtualHost /etc/nginx/conf.d/my-service.openfoodfacts.net.conf Note: verify it's the right file which has been impacted ! If it's not you may have the option to restore the file with git checkout wrong-touched-file (but look before with git status ) Test it's working (apart from the security alert in your browser, because certificate is from an unknown issuer). Then install the real certificate (you get the first section if you first did a test certificate): $ certbot -d my-service.openfoodfacts.net 1: Attempt to reinstall this existing certificate 2: Renew & replace the cert (limit ~5 per 7 days) Select \u2026 [enter] (press 'c' to cancel): 2 \u2026 Enter email address (used for urgent renewal and security notices) (Enter 'c' to cancel): root@openfoodfacts.org \u2026 terms of service\u2026 (A)gree/(C)ancel: A \u2026 share email\u2026 (Y)es/(N)o: n Obtaining a new certificate Performing the following challenges: http-01 challenge for my-service.openfoodfacts.net Waiting for verification... Cleaning up challenges Deploying Certificate to VirtualHost /etc/nginx/conf.d/my-service.openfoodfacts.net.conf Test again if it's working Etc Keeper # We use etckeeper Do not forget to commit your changes: etckeeper commit -m \"Configured my-service.openfoodfacts.net\" Now we are done \ud83c\udf89 The default proxmox interface does not offer options to indicate which gateway should be the default gateway, and the public ip needs to have its gateway as the default one, and there is no trivial way to achieve this reliably and elegantly, thus the best solution is to have only one gateway. See also ovh reverse proxy incident of 2022-02-18 \u21a9 the nginx script will normally do the check before trying to restart nginx, but this way you are able to also see warnings. \u21a9","title":"NGINX Reverse proxy (OVH)"},{"location":"nginx-reverse-proxy/#nginx-reverse-proxy-ovh","text":"At OVH we have a lxc container dedicated to reverse proxy http/https applications.","title":"NGINX Reverse proxy (OVH)"},{"location":"nginx-reverse-proxy/#network-specific-interface","text":"It as a specific network configurations with two ethernet address: * one internal, to communicate with other VMs * one which is bridged on host network card, with ip fail over mechanism. Important : only the public ip should have a gateway 1","title":"Network specific interface"},{"location":"nginx-reverse-proxy/#configuring-a-new-service","text":"To make a new service, hosted on proxmox, available you needs to: have this service available on proxmox internal network in the DNS, CNAME you service name to proxy1.openfoodfacts.org write a configuration on nginx for this service eventually add https","title":"Configuring a new service"},{"location":"nginx-reverse-proxy/#steps-to-create-nginx-configuration","text":"we will imagine we configure my-service.openfoodfacts.net You will have to be root to do that. Login on container (101) and start a root bash. Create the basic configuration file for your service in /etc/nginx/conf.d named my-service.openfoodfacts.net.conf on machine 222 , port 8888 . Important : your file has to ends with .conf to be taken into account. It's a good idea to first test it exists, using nc and curl: nc -vz 10 .1.0.222 8888 curl http://10.1.0.222:8888 Create a config, say: server { listen 80 ; listen [::]:80 ; server_name my-service.openfoodfacts.net ; access_log /var/log/nginx/my-service.off.net.log main ; error_log /var/log/nginx/my-service.off.net.err ; location / { proxy_pass http://10.1.0.222:8888 $request_uri ; proxy_set_header Host $host ; proxy_set_header X-Real-IP $remote_addr ; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for ; proxy_set_header X-Forwarded-Proto https ; proxy_read_timeout 90 ; client_max_body_size 512M ; } } If you need more nginx docs are here We test nginx configuration is ok ( Mandatory ) 2 : $ nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful if it's ok, we reload. $ systemctl reload nginx Next step is probably to setup https (putting something in http should be an exception, with good reason for that !) Otherwise jump to EtcKeeper","title":"Steps to create Nginx configuration"},{"location":"nginx-reverse-proxy/#adding-https","text":"Most of the time https certificates are managed on the nginx reverse proxy VM. Here is how to configure them to enable https. We use certbot to manage certificates. First prepare the service definition to answer on port 443 and 80. That is: change your current configuration to listen on 443: { server { listen 80 ; listen [::]:80 ; server_name my-service.openfoodfacts.net ; \u2026 but after it, add a new bare section for port 80: server { listen 80 ; listen [::]:80 ; server_name my-service.openfoodfacts.net ; } We will first validate our config using --test-cert option ( WARNING only skip this part if you really are used to certbot, as after too much invalid tentatives, we won't be able to renew any certificates on the domain for several hours from this IP, see letsencrypt Rate limits ) $ certbot --test-cert -d my-service.openfoodfacts.net \u2026 Enter email address (used for urgent renewal and security notices) (Enter 'c' to cancel): root@openfoodfacts.org \u2026 terms of service\u2026 (A)gree/(C)ancel: A \u2026 share email\u2026 (Y)es/(N)o: n Obtaining a new certificate Performing the following challenges: http-01 challenge for my-service.openfoodfacts.net Waiting for verification... Cleaning up challenges Deploying Certificate to VirtualHost /etc/nginx/conf.d/my-service.openfoodfacts.net.conf Note: verify it's the right file which has been impacted ! If it's not you may have the option to restore the file with git checkout wrong-touched-file (but look before with git status ) Test it's working (apart from the security alert in your browser, because certificate is from an unknown issuer). Then install the real certificate (you get the first section if you first did a test certificate): $ certbot -d my-service.openfoodfacts.net 1: Attempt to reinstall this existing certificate 2: Renew & replace the cert (limit ~5 per 7 days) Select \u2026 [enter] (press 'c' to cancel): 2 \u2026 Enter email address (used for urgent renewal and security notices) (Enter 'c' to cancel): root@openfoodfacts.org \u2026 terms of service\u2026 (A)gree/(C)ancel: A \u2026 share email\u2026 (Y)es/(N)o: n Obtaining a new certificate Performing the following challenges: http-01 challenge for my-service.openfoodfacts.net Waiting for verification... Cleaning up challenges Deploying Certificate to VirtualHost /etc/nginx/conf.d/my-service.openfoodfacts.net.conf Test again if it's working","title":"Adding https"},{"location":"nginx-reverse-proxy/#etc-keeper","text":"We use etckeeper Do not forget to commit your changes: etckeeper commit -m \"Configured my-service.openfoodfacts.net\" Now we are done \ud83c\udf89 The default proxmox interface does not offer options to indicate which gateway should be the default gateway, and the public ip needs to have its gateway as the default one, and there is no trivial way to achieve this reliably and elegantly, thus the best solution is to have only one gateway. See also ovh reverse proxy incident of 2022-02-18 \u21a9 the nginx script will normally do the check before trying to restart nginx, but this way you are able to also see warnings. \u21a9","title":"Etc Keeper"},{"location":"observability/","text":"Observability # This document describes the observability stack used at Open Food Facts to monitor applications. Having a good observability stack is critical to spend less time when debugging failures, to have a comprehension of how applications behave over time, and to have the ability to compare a software version with the previously deployed one. The observability stack used in the OFF stack is comprised of the following applications: Filebeat as a logs collection agent deployed on each QEMU VM with Docker containers. ElasticSearch for centralized storage and indexing of logs collected from Docker. Kibana UI to visualize and use logs collected by ElasticSearch. (official doc) Prometheus for scraping metrics from Prometheus exporters' /metrics endpoint, running as sidecar containers of the applications. AlertManager to send alerts based on Prometheus metrics, integrated with dedicated Slack channels. InfluxDB is the storage backend for data harvested by prometheus Grafana for visualizing Prometheus metrics, InfluxDB and other metrics; and create dashboards. (official doc) Prometheus exporters such as the Apache Prometheus Exporter , which collect metrics from applications and expose them on a port in the Prometheus metric format. Some applications natively export Prometheus metrics and do not need additional exporters. The observability stack diagram is as follows:","title":"Observability"},{"location":"observability/#observability","text":"This document describes the observability stack used at Open Food Facts to monitor applications. Having a good observability stack is critical to spend less time when debugging failures, to have a comprehension of how applications behave over time, and to have the ability to compare a software version with the previously deployed one. The observability stack used in the OFF stack is comprised of the following applications: Filebeat as a logs collection agent deployed on each QEMU VM with Docker containers. ElasticSearch for centralized storage and indexing of logs collected from Docker. Kibana UI to visualize and use logs collected by ElasticSearch. (official doc) Prometheus for scraping metrics from Prometheus exporters' /metrics endpoint, running as sidecar containers of the applications. AlertManager to send alerts based on Prometheus metrics, integrated with dedicated Slack channels. InfluxDB is the storage backend for data harvested by prometheus Grafana for visualizing Prometheus metrics, InfluxDB and other metrics; and create dashboards. (official doc) Prometheus exporters such as the Apache Prometheus Exporter , which collect metrics from applications and expose them on a port in the Prometheus metric format. Some applications natively export Prometheus metrics and do not need additional exporters. The observability stack diagram is as follows:","title":"Observability"},{"location":"odoo/","text":"Odoo: our relationship management (connect.openfoodfacts.org) # Odoo is a very rich tool: we need to think a little bit before doing things. Some actions can't be canceled; for example: some modules, when installed, can't be removed. Quick guidelines to follow: 1. Technical and functional admins rights should be separated. 2. Usages should be clearly expressed and modules should be discussed before implemented. An issue have to be open before each module or group of module installation. 3. Tests need to be made in a staging environement. Install # The current test instance (Odoo 15) have been installed with the following commands. Install Debian 11 apt install postgresql -y apt install wkhtmltopdf apt install gnupg gnupg1 gnupg2 wget -O - https://nightly.odoo.com/odoo.key | apt-key add - echo \"deb http://nightly.odoo.com/15.0/nightly/deb/ ./\" >> /etc/apt/sources.list.d/odoo.list apt-get update && apt-get install odoo Setup Nginx proxy Start/stop # sudo systemctl start odoo # (starts service) sudo systemctl stop odoo # (stops service) sudo systemctl restart odoo # (restarts Service) sudo systemctl status odoo # (status of service) sudo systemctl enable odoo # (starts service at boot) sudo systemctl disable odoo # (disables service at boot) Install a new module # Examples: cd /usr/lib/python3/dist-packages/odoo/addons wget https://apps.odoo.com/loempia/download/formio/15.0/formio.zip unzip formio.zip cd /usr/lib/python3/dist-packages/odoo/addons wget https://apps.odoo.com/loempia/download/mass_editing/15.0/mass_editing.zip unzip formio.zip Contributed modules from OCA store # OCA hosts hundreds of modules. Those ones are disseminated into dozens of git repositories. For example, the Mass Editing module can be find inside the https://github.com/OCA/server-ux repository. Note: In github, always search for the 15.0 branch (if 15.0 is your version) because the front page might be that of 14.0 and mislead you on supported versions. In this case we will add all addons of the repository: cd /usr/lib/python3/dist-packages/odoo/addons # for odoo version 15.0: adapt according to desired version git clone https://github.com/OCA/server-ux --branch 15 .0 Update the addon path in your /etc/odoo/odoo.conf file to add our new directory addons_path = /usr/lib/python3/dist-packages/odoo/addons,/usr/lib/python3/dist-packages/odoo/addons/server-ux Look at the __manifest__.py , for the product you want to install, and see if there are python modules to be installed (python dependencies). Also do that for any product, it depends on. Restart Odoo: systemctl restart odoo Then, as an admin, in Odoo: * pass in developer mode (ctrl+K debug:) * Apps menu * Update Apps List sub-menu * then find the app in the search field (eventually remove the \"app\" filter if you installed a utility) Create a test environment from production instance # # previously verify that CT 112 is currently connect-staging, and delete it before renewing it pct snapshot 110 temp # create a \"temp\" named snapshot of CT with ID 120 (production) pct clone 110 112 --hostname connect-staging --snapname temp # take the snapshot and create a new CT (112) named connect-staging pct delsnapshot 110 temp # del production snapshot # New CT configuration pct set 112 --cores 2 --memory 4096 --net0 name = eth0,bridge = vmbr0,gw = 10 .0.0.1,ip = 10 .1.0.112/24 pct start 112 # pct exec 112 .................. # a way to execute things on a CT from host # TO BE CONTINUED","title":"Odoo: our relationship management (connect.openfoodfacts.org)"},{"location":"odoo/#odoo-our-relationship-management-connectopenfoodfactsorg","text":"Odoo is a very rich tool: we need to think a little bit before doing things. Some actions can't be canceled; for example: some modules, when installed, can't be removed. Quick guidelines to follow: 1. Technical and functional admins rights should be separated. 2. Usages should be clearly expressed and modules should be discussed before implemented. An issue have to be open before each module or group of module installation. 3. Tests need to be made in a staging environement.","title":"Odoo: our relationship management (connect.openfoodfacts.org)"},{"location":"odoo/#install","text":"The current test instance (Odoo 15) have been installed with the following commands. Install Debian 11 apt install postgresql -y apt install wkhtmltopdf apt install gnupg gnupg1 gnupg2 wget -O - https://nightly.odoo.com/odoo.key | apt-key add - echo \"deb http://nightly.odoo.com/15.0/nightly/deb/ ./\" >> /etc/apt/sources.list.d/odoo.list apt-get update && apt-get install odoo Setup Nginx proxy","title":"Install"},{"location":"odoo/#startstop","text":"sudo systemctl start odoo # (starts service) sudo systemctl stop odoo # (stops service) sudo systemctl restart odoo # (restarts Service) sudo systemctl status odoo # (status of service) sudo systemctl enable odoo # (starts service at boot) sudo systemctl disable odoo # (disables service at boot)","title":"Start/stop"},{"location":"odoo/#install-a-new-module","text":"Examples: cd /usr/lib/python3/dist-packages/odoo/addons wget https://apps.odoo.com/loempia/download/formio/15.0/formio.zip unzip formio.zip cd /usr/lib/python3/dist-packages/odoo/addons wget https://apps.odoo.com/loempia/download/mass_editing/15.0/mass_editing.zip unzip formio.zip","title":"Install a new module"},{"location":"odoo/#contributed-modules-from-oca-store","text":"OCA hosts hundreds of modules. Those ones are disseminated into dozens of git repositories. For example, the Mass Editing module can be find inside the https://github.com/OCA/server-ux repository. Note: In github, always search for the 15.0 branch (if 15.0 is your version) because the front page might be that of 14.0 and mislead you on supported versions. In this case we will add all addons of the repository: cd /usr/lib/python3/dist-packages/odoo/addons # for odoo version 15.0: adapt according to desired version git clone https://github.com/OCA/server-ux --branch 15 .0 Update the addon path in your /etc/odoo/odoo.conf file to add our new directory addons_path = /usr/lib/python3/dist-packages/odoo/addons,/usr/lib/python3/dist-packages/odoo/addons/server-ux Look at the __manifest__.py , for the product you want to install, and see if there are python modules to be installed (python dependencies). Also do that for any product, it depends on. Restart Odoo: systemctl restart odoo Then, as an admin, in Odoo: * pass in developer mode (ctrl+K debug:) * Apps menu * Update Apps List sub-menu * then find the app in the search field (eventually remove the \"app\" filter if you installed a utility)","title":"Contributed modules from OCA store"},{"location":"odoo/#create-a-test-environment-from-production-instance","text":"# previously verify that CT 112 is currently connect-staging, and delete it before renewing it pct snapshot 110 temp # create a \"temp\" named snapshot of CT with ID 120 (production) pct clone 110 112 --hostname connect-staging --snapname temp # take the snapshot and create a new CT (112) named connect-staging pct delsnapshot 110 temp # del production snapshot # New CT configuration pct set 112 --cores 2 --memory 4096 --net0 name = eth0,bridge = vmbr0,gw = 10 .0.0.1,ip = 10 .1.0.112/24 pct start 112 # pct exec 112 .................. # a way to execute things on a CT from host # TO BE CONTINUED","title":"Create a test environment from production instance"},{"location":"producers_sftp/","text":"Producers SFTP # We have a producer SFTP which is part of the producer platform. This sftp is used by producers who send files for regular automated updates of their products. The sftp is located on off1.openfoodfacts.org The /home/sftp folder links to /srv/sftp/ and contains home for sftp users. Adding a new sftp user # Use the script add_sftp_user.pl (present in /home/script ) with user root.","title":"Producers SFTP"},{"location":"producers_sftp/#producers-sftp","text":"We have a producer SFTP which is part of the producer platform. This sftp is used by producers who send files for regular automated updates of their products. The sftp is located on off1.openfoodfacts.org The /home/sftp folder links to /srv/sftp/ and contains home for sftp users.","title":"Producers SFTP"},{"location":"producers_sftp/#adding-a-new-sftp-user","text":"Use the script add_sftp_user.pl (present in /home/script ) with user root.","title":"Adding a new sftp user"},{"location":"promox/","text":"Proxmox # On ovh1 and ovh2 we use proxmox to manage VMs. TODO this page is really incomplete ! HTTP Reverse Proxy # The VM 101 is a http / https proxy to all services. It has it's own bridge interface with a public facing ip. See Nginx reverse proxy Storage # We use two type of storage: the NVME and zfs storage. There are also mounts of zfs storage from ovh3. TODO tell much more Creating a new VM # TODO (see wiki page) Loggin to a container or VM # Most of the time we use ssh to connect to containers and VM. The mkuser script helps you create users using github keys. For the happy few sudoers on the host, they can attach to containers using lxc-attach -n <num> where <num> is the VM number. This gives a root console in the container.","title":"Proxmox"},{"location":"promox/#proxmox","text":"On ovh1 and ovh2 we use proxmox to manage VMs. TODO this page is really incomplete !","title":"Proxmox"},{"location":"promox/#http-reverse-proxy","text":"The VM 101 is a http / https proxy to all services. It has it's own bridge interface with a public facing ip. See Nginx reverse proxy","title":"HTTP Reverse Proxy"},{"location":"promox/#storage","text":"We use two type of storage: the NVME and zfs storage. There are also mounts of zfs storage from ovh3. TODO tell much more","title":"Storage"},{"location":"promox/#creating-a-new-vm","text":"TODO (see wiki page)","title":"Creating a new VM"},{"location":"promox/#loggin-to-a-container-or-vm","text":"Most of the time we use ssh to connect to containers and VM. The mkuser script helps you create users using github keys. For the happy few sudoers on the host, they can attach to containers using lxc-attach -n <num> where <num> is the VM number. This gives a root console in the container.","title":"Loggin to a container or VM"},{"location":"zammad/","text":"Zammad # Zammad is a tool for support. It is installed in a VM at CQuest home (we should migrate it), in an LXC container. It is exposed at https://support.openfoodfacts.org/ It was setup using zammad package (see install docs ). It has different services, all begining with the name zammad- . It uses postgresql. It also uses Elasticsearch. Because we are in a container, heap memory size has to be configured manually through a file in /etc/elasticsearch/jvm.options.d/memory.options . 600m seems like a good size.","title":"Zammad"},{"location":"zammad/#zammad","text":"Zammad is a tool for support. It is installed in a VM at CQuest home (we should migrate it), in an LXC container. It is exposed at https://support.openfoodfacts.org/ It was setup using zammad package (see install docs ). It has different services, all begining with the name zammad- . It uses postgresql. It also uses Elasticsearch. Because we are in a container, heap memory size has to be configured manually through a file in /etc/elasticsearch/jvm.options.d/memory.options . 600m seems like a good size.","title":"Zammad"},{"location":"reports/2021-02-22-matomo-install/","text":"Matomo install # see: https://github.com/openfoodfacts/openfoodfacts-infrastructure/issues/24 Install log # Nginx and SSL configuration for https://analytics.openfoodfacts.org/: $ ssh -i ~/.ssh/id_rsa -J CharlesNepote@ovh1.openfoodfacts.org CharlesNepote@10.1.0.101 $ sudo cp /etc/nginx/conf.d/default.conf /etc/nginx/conf.d/analytics.conf $ sudo nano /etc/nginx/conf.d/analytics.conf access_log /var/log/nginx/analytics.access.log main; server_name analytics.openfoodfacts.org; $ sudo systemctl restart nginx $ sudo certbot https://analytics.openfoodfacts.org/ make a snapshot before https://www.atechtown.com/install-nginx-and-php-on-debian-10/ $ ssh -i ~/.ssh/id_rsa -J CharlesNepote@ovh1.openfoodfacts.org CharlesNepote@10.1.0.107 $ sudo apt install nginx php7.3-{fpm,cli,curl,gd,imap,json,mbstring,mysql,xml,zip} mariadb-server $ sudo chmod 755 -R /var/www/html/ $ sudo chown www-data:www-data -R /var/www/html/ $ sudo nano /etc/nginx/sites-available/default location ~ \\.php$ { include snippets/fastcgi-php.conf; With php-fpm (or other unix sockets): fastcgi_pass unix:/var/run/php/php7.3-fpm.sock; # With php-cgi (or other tcp sockets): # fastcgi_pass 127.0.0.1:9000; } $ sudo systemctl restart nginx $ echo \"<?php phpinfo(); ?>\" | sudo -u www-data tee /var/www/html/test.php https://analytics.openfoodfacts.org/test.php sudo rm /var/www/html/test.php $ sudo systemctl enable mariadb $ sudo mysql_secure_installation # https://matomo.org/faq/how-to-install/faq_23484/ $ sudo mysql -e \"CREATE DATABASE matomo_db;\" $ read matomopass # enter the matomo user password $ sudo mysql -e \"CREATE USER 'matomo'@'localhost' IDENTIFIED BY '$matomopass';\" $ sudo mysql -e \"GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, INDEX, DROP, ALTER, CREATE TEMPORARY TABLES, LOCK TABLES ON matomo_db.* TO 'matomo'@'localhost';\" $ sudo mysql -e \"GRANT FILE ON *.* TO 'matomo'@'localhost';\" $ unset matomopass $ cd /var/www/html $ sudo -u www-data wget https://builds.matomo.org/matomo.zip $ sudo apt install unzip $ sudo -u www-data unzip matomo.zip $ sudo sed -i \"s|root /var/www/html|/var/www/html/matomo|\" /etc/nginx/sites-available/default https://github.com/matomo-org/matomo-nginx/blob/master/sites-available/matomo.conf $ sudo sed -i \"s|index index.html index.htm index.nginx-debian.html;|index index.html index.htm index.nginx-debian.html index.php;|\" /etc/nginx/sites-available/default # non! $ sudo systemctl restart nginx https://analytics.openfoodfacts.org Activate reverse proxy mode (2022-05-09) # We have to configure matomo to take into account it is behind a reverse proxy. We follow https://matomo.org/faq/how-to-install/faq_98/ And on matomo machine (container 107): edit /var/www/html/matomo/config/config.ini to add : [General] ... force_ssl = 1 assume_secure_protocol = 1 ... proxy_client_headers[] = HTTP_X_FORWARDED_FOR proxy_host_headers[] = HTTP_X_FORWARDED_HOST restart pfm to take this into account: systemctl restart php7.3-fpm.service On proxy machine (container 101), we insure the header is passed by nginx: edit /etc/nginx/conf.d/analytics.conf server { server_name analytics.openfoodfacts.org; ... location / { ... proxy_set_header X-Forwarded-For $remote_addr; reload nginx config: systemctl reload nginx Verify it's working by looking at real time traffic on matomo. Trigger archival offline - 2022-05-10 # By default matomo tries to run archival on analytics visit. But our website as a lot of analytics so it's important to run it on a regular basis and out of a request (which will timeout too soon). We follow https://matomo.org/faq/troubleshooting/faq_19489/ and thus https://matomo.org/faq/on-premise/how-to-set-up-auto-archiving-of-your-reports/ On analytics (container 107): ensure mail is setup correctly on the server create directory for logs: mkdir /var/log/matomo/ chown www-data:www-data /var/log/matomo/ edit /etc/cron.d/matomo-archive MAILTO=\"root@openfoodfacts.org\" 5 * * * * www-data /usr/bin/php /var/www/html/matomo/console core:archive --url=http://analytics.openfoodfacts.org/ >> /var/log/matomo/matomo-archive.log 2>>/var/log/matomo/matomo-archive-err.log add files to logrotate, by adding /etc/logrotate.d/matomo with /var/log/matomo/*.log { daily missingok rotate 14 compress delaycompress notifempty create 0640 www-data adm sharedscripts } tweak /etc/php/7.3/cli/php.ini (this correspond to settings for php command), to have: max_execution_time = 3000 ... memory_limit = -1 In matomo (https://analytics.openfoodfacts.org), click on Administration \u2192 System \u2192 General Settings , and select: Archive reports when viewed from the browser: No Archive reports at most every X seconds : 3600 seconds Performance settings # I tweak /etc/php/7.3/fpm/php.ini to have: max_execution_time = 120 ... memory_limit = 250M then reload to take the change into account: systemctl reload php7.3-fpm.service","title":"Matomo install"},{"location":"reports/2021-02-22-matomo-install/#matomo-install","text":"see: https://github.com/openfoodfacts/openfoodfacts-infrastructure/issues/24","title":"Matomo install"},{"location":"reports/2021-02-22-matomo-install/#install-log","text":"Nginx and SSL configuration for https://analytics.openfoodfacts.org/: $ ssh -i ~/.ssh/id_rsa -J CharlesNepote@ovh1.openfoodfacts.org CharlesNepote@10.1.0.101 $ sudo cp /etc/nginx/conf.d/default.conf /etc/nginx/conf.d/analytics.conf $ sudo nano /etc/nginx/conf.d/analytics.conf access_log /var/log/nginx/analytics.access.log main; server_name analytics.openfoodfacts.org; $ sudo systemctl restart nginx $ sudo certbot https://analytics.openfoodfacts.org/ make a snapshot before https://www.atechtown.com/install-nginx-and-php-on-debian-10/ $ ssh -i ~/.ssh/id_rsa -J CharlesNepote@ovh1.openfoodfacts.org CharlesNepote@10.1.0.107 $ sudo apt install nginx php7.3-{fpm,cli,curl,gd,imap,json,mbstring,mysql,xml,zip} mariadb-server $ sudo chmod 755 -R /var/www/html/ $ sudo chown www-data:www-data -R /var/www/html/ $ sudo nano /etc/nginx/sites-available/default location ~ \\.php$ { include snippets/fastcgi-php.conf; With php-fpm (or other unix sockets): fastcgi_pass unix:/var/run/php/php7.3-fpm.sock; # With php-cgi (or other tcp sockets): # fastcgi_pass 127.0.0.1:9000; } $ sudo systemctl restart nginx $ echo \"<?php phpinfo(); ?>\" | sudo -u www-data tee /var/www/html/test.php https://analytics.openfoodfacts.org/test.php sudo rm /var/www/html/test.php $ sudo systemctl enable mariadb $ sudo mysql_secure_installation # https://matomo.org/faq/how-to-install/faq_23484/ $ sudo mysql -e \"CREATE DATABASE matomo_db;\" $ read matomopass # enter the matomo user password $ sudo mysql -e \"CREATE USER 'matomo'@'localhost' IDENTIFIED BY '$matomopass';\" $ sudo mysql -e \"GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, INDEX, DROP, ALTER, CREATE TEMPORARY TABLES, LOCK TABLES ON matomo_db.* TO 'matomo'@'localhost';\" $ sudo mysql -e \"GRANT FILE ON *.* TO 'matomo'@'localhost';\" $ unset matomopass $ cd /var/www/html $ sudo -u www-data wget https://builds.matomo.org/matomo.zip $ sudo apt install unzip $ sudo -u www-data unzip matomo.zip $ sudo sed -i \"s|root /var/www/html|/var/www/html/matomo|\" /etc/nginx/sites-available/default https://github.com/matomo-org/matomo-nginx/blob/master/sites-available/matomo.conf $ sudo sed -i \"s|index index.html index.htm index.nginx-debian.html;|index index.html index.htm index.nginx-debian.html index.php;|\" /etc/nginx/sites-available/default # non! $ sudo systemctl restart nginx https://analytics.openfoodfacts.org","title":"Install log"},{"location":"reports/2021-02-22-matomo-install/#activate-reverse-proxy-mode-2022-05-09","text":"We have to configure matomo to take into account it is behind a reverse proxy. We follow https://matomo.org/faq/how-to-install/faq_98/ And on matomo machine (container 107): edit /var/www/html/matomo/config/config.ini to add : [General] ... force_ssl = 1 assume_secure_protocol = 1 ... proxy_client_headers[] = HTTP_X_FORWARDED_FOR proxy_host_headers[] = HTTP_X_FORWARDED_HOST restart pfm to take this into account: systemctl restart php7.3-fpm.service On proxy machine (container 101), we insure the header is passed by nginx: edit /etc/nginx/conf.d/analytics.conf server { server_name analytics.openfoodfacts.org; ... location / { ... proxy_set_header X-Forwarded-For $remote_addr; reload nginx config: systemctl reload nginx Verify it's working by looking at real time traffic on matomo.","title":"Activate reverse proxy mode (2022-05-09)"},{"location":"reports/2021-02-22-matomo-install/#trigger-archival-offline-2022-05-10","text":"By default matomo tries to run archival on analytics visit. But our website as a lot of analytics so it's important to run it on a regular basis and out of a request (which will timeout too soon). We follow https://matomo.org/faq/troubleshooting/faq_19489/ and thus https://matomo.org/faq/on-premise/how-to-set-up-auto-archiving-of-your-reports/ On analytics (container 107): ensure mail is setup correctly on the server create directory for logs: mkdir /var/log/matomo/ chown www-data:www-data /var/log/matomo/ edit /etc/cron.d/matomo-archive MAILTO=\"root@openfoodfacts.org\" 5 * * * * www-data /usr/bin/php /var/www/html/matomo/console core:archive --url=http://analytics.openfoodfacts.org/ >> /var/log/matomo/matomo-archive.log 2>>/var/log/matomo/matomo-archive-err.log add files to logrotate, by adding /etc/logrotate.d/matomo with /var/log/matomo/*.log { daily missingok rotate 14 compress delaycompress notifempty create 0640 www-data adm sharedscripts } tweak /etc/php/7.3/cli/php.ini (this correspond to settings for php command), to have: max_execution_time = 3000 ... memory_limit = -1 In matomo (https://analytics.openfoodfacts.org), click on Administration \u2192 System \u2192 General Settings , and select: Archive reports when viewed from the browser: No Archive reports at most every X seconds : 3600 seconds","title":"Trigger archival offline - 2022-05-10"},{"location":"reports/2021-02-22-matomo-install/#performance-settings","text":"I tweak /etc/php/7.3/fpm/php.ini to have: max_execution_time = 120 ... memory_limit = 250M then reload to take the change into account: systemctl reload php7.3-fpm.service","title":"Performance settings"},{"location":"reports/2021-10-03-network-down/","text":"[Postmortem] OpenFoodFacts.net down (#1) # Date : 03/10/2021 Authors : ocervello, Status : Complete, action items in progress Summary : openfoodfacts.net down after Docker storage driver configuration change. Proxmox containers off-net , robotoff-net , robotoff-dev , mongo-dev , and monitoring are unreachable. Impact : Integration tests failing on openfoodfacts-dart ( example ), pre-prod environment (openfoodfacts.net) down. Root Causes : Cascading failure probably due to Docker storage driver configuration change from vfs to fuse-overlayfs , and probably an incompatibility between LXC, Docker and fuse-overlayfs , causing containers to crash, and unable to SSH. Exact root cause is still unknown , as some containers using fuse-overlayfs have not crashed. Trigger : Unknown. First outage happened 3 days after the Docker storage driver change. Resolution # Short term: revert Docker storage driver configuration from fuse-overlayfs to vfs . Long term: run Docker containers in a QEMU host instead. Detection : message on Slack #infrastructure channel + openfoodfacts-dart integration tests failing with timeouts. Action Items # Action Item Type Owner Status Revert Docker storage driver to vfs mitigate olivier DONE Snapshot off-net CT and start a new CT from the snapshot mitigate charles FAILED Create vanilla CT with storage driver vfs and re-deploy openfoodfacts.net on it + ZFS Mounts + NGINX config change mitigate charles,olivier,stephane,christian IN PROGRESS Open ticket to Proxmox forums to investigate the crash process charles TODO Run all crashed Docker containers on QEMU VM for stability + ZFS mounts + NGINX configuration prevent olivier,charles,stephane,christian https://github.com/openfoodfacts/openfoodfacts-infrastructure/issues/62 Lessons Learned # What went well # Community + staff got quickly alerted of openfoodfacts.net being down Worked together to solve the issues What went wrong # No explicit alert message was sent to productopener-alerts Slack channel \u2192 need integration tests on openfoodfacts-server repository Too many CTs brought down simultaneously - should have done the storage engine change on only 1 host and wait Proxmox container cloning failed, increasing the ETTR (Estimated Time To Repair) Proxmox container failed to reboot, increasing the ETTR Too much noise on productopener-alerts , failed deployments were missed. Sysadmins were not aware about all the impacts of off-net downtime. No single point to track the investigation and resolution (e.g. GitHub issue) Where we got lucky # Did not bring down production as it is still running on the Free machines. Automated deployments allowed us to re-deploy openfoodfacts.net pretty fast The right people were available. What we learned # Assuming the root cause is correct: Proxmox LXC + Docker + ZFS + fuse-overlayfs storage driver can trigger severe issues where even Proxmox administration tools do not work (clones, snapshots, etc\u2026) Timeline # 29-09-2021 (All times CEST) # 15:46 ROOT CAUSE \u2014 Docker storage driver switch from vfs to fuse-overlayfs made on all CTs w/ Docker deployments. 03-10-2021 (All times CEST) # 02:17 OUTAGE BEGINS \u2014 Automated message on #infrastructure-alerts Slack channel about timeouts when trying to access world.openfoodfacts.net 19:16 OUTAGE BEGINS \u2014 Manual message by contributor on #infrastructure Slack channel about timeouts when trying to access world.openfoodfacts.net 04-10-2021 (All times CEST) # 9:23 Message on #infrastructure Slack channel that multiple containers are unresponsive. 06-10-2021 (All times CEST) # 14:36 OUTAGE MITIGATED , deployed openfoodfacts.net and a new machine. Mounts are still missing on disk. 14:45 Decision taken to switch Docker containers to QEMU VM. 15:36 Creation of QEMU VM 128GB RAM, 8 cores, 196GB drive. 07-10-2021 (All times CEST) # 09:00 Starting to manually deploy openfoodfacts-server, robotoff, robotoff-ann and monitoring containers on QEMU VM 09:30 Openfoodfacts server is deployed on QEMU VM 10:20 Robotoff deployment is blocked by a CPU flag issue (avx flag needed for Tensorflow library) Supporting information: # Document a clear realistic \u201cacceptable downtime\u201d for each CT/VM/machines we manage (using the existing spreadsheet ). Document the main owner and his/her co-owner (?) of each machine, ie people able to restore a service within the \u201cacceptable downtime\u201d and owning this responsibility. Decide how we document the infrastructure (not well decided yet). Is it possible to publish only real alerts in #infrastructure-alerts? Eg, only publish alerts if the machine is down for more than 15 minutes. Most of the alerts seems to be false positives. Define a process to resolve future incidents (e.g. should we systematically file a github issue for each incident?)","title":"[Postmortem] OpenFoodFacts.net down (#1)"},{"location":"reports/2021-10-03-network-down/#postmortem-openfoodfactsnet-down-1","text":"Date : 03/10/2021 Authors : ocervello, Status : Complete, action items in progress Summary : openfoodfacts.net down after Docker storage driver configuration change. Proxmox containers off-net , robotoff-net , robotoff-dev , mongo-dev , and monitoring are unreachable. Impact : Integration tests failing on openfoodfacts-dart ( example ), pre-prod environment (openfoodfacts.net) down. Root Causes : Cascading failure probably due to Docker storage driver configuration change from vfs to fuse-overlayfs , and probably an incompatibility between LXC, Docker and fuse-overlayfs , causing containers to crash, and unable to SSH. Exact root cause is still unknown , as some containers using fuse-overlayfs have not crashed. Trigger : Unknown. First outage happened 3 days after the Docker storage driver change.","title":"[Postmortem] OpenFoodFacts.net down (#1)"},{"location":"reports/2021-10-03-network-down/#resolution","text":"Short term: revert Docker storage driver configuration from fuse-overlayfs to vfs . Long term: run Docker containers in a QEMU host instead. Detection : message on Slack #infrastructure channel + openfoodfacts-dart integration tests failing with timeouts.","title":"Resolution"},{"location":"reports/2021-10-03-network-down/#action-items","text":"Action Item Type Owner Status Revert Docker storage driver to vfs mitigate olivier DONE Snapshot off-net CT and start a new CT from the snapshot mitigate charles FAILED Create vanilla CT with storage driver vfs and re-deploy openfoodfacts.net on it + ZFS Mounts + NGINX config change mitigate charles,olivier,stephane,christian IN PROGRESS Open ticket to Proxmox forums to investigate the crash process charles TODO Run all crashed Docker containers on QEMU VM for stability + ZFS mounts + NGINX configuration prevent olivier,charles,stephane,christian https://github.com/openfoodfacts/openfoodfacts-infrastructure/issues/62","title":"Action Items"},{"location":"reports/2021-10-03-network-down/#lessons-learned","text":"","title":"Lessons Learned"},{"location":"reports/2021-10-03-network-down/#what-went-well","text":"Community + staff got quickly alerted of openfoodfacts.net being down Worked together to solve the issues","title":"What went well"},{"location":"reports/2021-10-03-network-down/#what-went-wrong","text":"No explicit alert message was sent to productopener-alerts Slack channel \u2192 need integration tests on openfoodfacts-server repository Too many CTs brought down simultaneously - should have done the storage engine change on only 1 host and wait Proxmox container cloning failed, increasing the ETTR (Estimated Time To Repair) Proxmox container failed to reboot, increasing the ETTR Too much noise on productopener-alerts , failed deployments were missed. Sysadmins were not aware about all the impacts of off-net downtime. No single point to track the investigation and resolution (e.g. GitHub issue)","title":"What went wrong"},{"location":"reports/2021-10-03-network-down/#where-we-got-lucky","text":"Did not bring down production as it is still running on the Free machines. Automated deployments allowed us to re-deploy openfoodfacts.net pretty fast The right people were available.","title":"Where we got lucky"},{"location":"reports/2021-10-03-network-down/#what-we-learned","text":"Assuming the root cause is correct: Proxmox LXC + Docker + ZFS + fuse-overlayfs storage driver can trigger severe issues where even Proxmox administration tools do not work (clones, snapshots, etc\u2026)","title":"What we learned"},{"location":"reports/2021-10-03-network-down/#timeline","text":"","title":"Timeline"},{"location":"reports/2021-10-03-network-down/#29-09-2021-all-times-cest","text":"15:46 ROOT CAUSE \u2014 Docker storage driver switch from vfs to fuse-overlayfs made on all CTs w/ Docker deployments.","title":"29-09-2021 (All times CEST)"},{"location":"reports/2021-10-03-network-down/#03-10-2021-all-times-cest","text":"02:17 OUTAGE BEGINS \u2014 Automated message on #infrastructure-alerts Slack channel about timeouts when trying to access world.openfoodfacts.net 19:16 OUTAGE BEGINS \u2014 Manual message by contributor on #infrastructure Slack channel about timeouts when trying to access world.openfoodfacts.net","title":"03-10-2021 (All times CEST)"},{"location":"reports/2021-10-03-network-down/#04-10-2021-all-times-cest","text":"9:23 Message on #infrastructure Slack channel that multiple containers are unresponsive.","title":"04-10-2021 (All times CEST)"},{"location":"reports/2021-10-03-network-down/#06-10-2021-all-times-cest","text":"14:36 OUTAGE MITIGATED , deployed openfoodfacts.net and a new machine. Mounts are still missing on disk. 14:45 Decision taken to switch Docker containers to QEMU VM. 15:36 Creation of QEMU VM 128GB RAM, 8 cores, 196GB drive.","title":"06-10-2021 (All times CEST)"},{"location":"reports/2021-10-03-network-down/#07-10-2021-all-times-cest","text":"09:00 Starting to manually deploy openfoodfacts-server, robotoff, robotoff-ann and monitoring containers on QEMU VM 09:30 Openfoodfacts server is deployed on QEMU VM 10:20 Robotoff deployment is blocked by a CPU flag issue (avx flag needed for Tensorflow library)","title":"07-10-2021 (All times CEST)"},{"location":"reports/2021-10-03-network-down/#supporting-information","text":"Document a clear realistic \u201cacceptable downtime\u201d for each CT/VM/machines we manage (using the existing spreadsheet ). Document the main owner and his/her co-owner (?) of each machine, ie people able to restore a service within the \u201cacceptable downtime\u201d and owning this responsibility. Decide how we document the infrastructure (not well decided yet). Is it possible to publish only real alerts in #infrastructure-alerts? Eg, only publish alerts if the machine is down for more than 15 minutes. Most of the alerts seems to be false positives. Define a process to resolve future incidents (e.g. should we systematically file a github issue for each incident?)","title":"Supporting information:"},{"location":"reports/2021-12-21-disk-extension/","text":"Disk extension for preprod and containers prod (ovh2) # We got alerts on low disk space on preprod VM (dockers (200) on ovh2) We added disk space in proxmox to this machine and the container machine but we still add to make it available on the system side. Operation was done on ovh2, VM dockers (200) and VM dockers-prod (201) Snapshoting # Before this delicate operation, we snapshoted the VM in proxmox. Extending partition size # This is the commands we run to make it happens. Install tools: apt install parted etckeeper The swap partition is after the main partition, so we will have to remove it, resize main partition and recreate it. Turn swap off: swapoff -a Edit partition. parted /dev/sda # change unit to sectors (parted) u Unit? [compact]? s # print partition table (parted) p Number Start End Size Type File system Flags 1 1049kB 274GB 274GB primary ext4 boot 2 274GB 275GB 1022MB extended 5 274GB 275GB 1022MB logical linux-swap(v1) # note tha swap partition is to 534872062 536868863 (1996802 sectores in size) # remove swap partitions (parted) rm 5 (parted) rm 2 # print to check (parted) p Number Start End Size Type File system Flags 1 2048s 534870015s 534867968s primary ext4 boot # resize partition 1, leaving space for swap (parted) resizepart 1 -1996801 Warning: Partition /dev/sda1 is being used. Are you sure you want to continue? Yes/No? y # print umber Start End Size Type File system Flags 1 2048s 627148799s 627146752s primary ext4 boot # recreate swap (Start is end of part 1 + 1) (parted) mkpart Partition type? primary/extended? primary File system type? [ext2]? linux-swap Start? 836864000 End? -1s (parted) quit :pencil: Note: 1. we switch units to sectors because it helps having better aligned partitions (by multiples of 2048 in this case) 2. The swap partition was on an extended partition, but we put it back as a simple partition We recreated swap, now we have to format it: mkswap /dev/sda2 show new uids: blkid /dev/sda1: UUID=\"082b4523-f4d6-4d39-b5dd-48c5bdba2541\" BLOCK_SIZE=\"4096\" TYPE=\"ext4\" PARTUUID=\"1a39b366-01\" /dev/sr0: BLOCK_SIZE=\"2048\" UUID=\"2021-08-14-10-10-00-00\" LABEL=\"Debian 11.0.0 amd64 n\" TYPE=\"iso9660\" PTUUID=\"3c15dbf8\" PTTYPE=\"dos\" /dev/sda2: UUID=\"f8e99f04-88eb-4550-9308-10a470175e45\" TYPE=\"swap\" PARTUUID=\"1a39b366-02\" Change /etc/fstab accordingly ( sda1 UUID did not change, only swap changed): # / was on /dev/sda1 during installation UUID=082b4523-f4d6-4d39-b5dd-48c5bdba2541 / ext4 errors=remount-ro 0 1 # swap was on /dev/sda2 during installation UUID=f8e99f04-88eb-4550-9308-10a470175e45 none swap sw 0 0 We should have booked our operation in etckeeper (not done on dockers-prod, etckeeper was not yet installed) etckeeper commit \"changed partition size\" Now we resized the partition, but we have to resize the filesystem. Let's resize sda1: resize2fs /dev/sda1 Reactivate swap: swapon -a Let's see our changes: df -h / Filesystem Size Used Avail Use% Mounted on /dev/sda1 294G 187G 93G 67% /","title":"Disk extension for preprod and containers prod (ovh2)"},{"location":"reports/2021-12-21-disk-extension/#disk-extension-for-preprod-and-containers-prod-ovh2","text":"We got alerts on low disk space on preprod VM (dockers (200) on ovh2) We added disk space in proxmox to this machine and the container machine but we still add to make it available on the system side. Operation was done on ovh2, VM dockers (200) and VM dockers-prod (201)","title":"Disk extension for preprod and containers prod (ovh2)"},{"location":"reports/2021-12-21-disk-extension/#snapshoting","text":"Before this delicate operation, we snapshoted the VM in proxmox.","title":"Snapshoting"},{"location":"reports/2021-12-21-disk-extension/#extending-partition-size","text":"This is the commands we run to make it happens. Install tools: apt install parted etckeeper The swap partition is after the main partition, so we will have to remove it, resize main partition and recreate it. Turn swap off: swapoff -a Edit partition. parted /dev/sda # change unit to sectors (parted) u Unit? [compact]? s # print partition table (parted) p Number Start End Size Type File system Flags 1 1049kB 274GB 274GB primary ext4 boot 2 274GB 275GB 1022MB extended 5 274GB 275GB 1022MB logical linux-swap(v1) # note tha swap partition is to 534872062 536868863 (1996802 sectores in size) # remove swap partitions (parted) rm 5 (parted) rm 2 # print to check (parted) p Number Start End Size Type File system Flags 1 2048s 534870015s 534867968s primary ext4 boot # resize partition 1, leaving space for swap (parted) resizepart 1 -1996801 Warning: Partition /dev/sda1 is being used. Are you sure you want to continue? Yes/No? y # print umber Start End Size Type File system Flags 1 2048s 627148799s 627146752s primary ext4 boot # recreate swap (Start is end of part 1 + 1) (parted) mkpart Partition type? primary/extended? primary File system type? [ext2]? linux-swap Start? 836864000 End? -1s (parted) quit :pencil: Note: 1. we switch units to sectors because it helps having better aligned partitions (by multiples of 2048 in this case) 2. The swap partition was on an extended partition, but we put it back as a simple partition We recreated swap, now we have to format it: mkswap /dev/sda2 show new uids: blkid /dev/sda1: UUID=\"082b4523-f4d6-4d39-b5dd-48c5bdba2541\" BLOCK_SIZE=\"4096\" TYPE=\"ext4\" PARTUUID=\"1a39b366-01\" /dev/sr0: BLOCK_SIZE=\"2048\" UUID=\"2021-08-14-10-10-00-00\" LABEL=\"Debian 11.0.0 amd64 n\" TYPE=\"iso9660\" PTUUID=\"3c15dbf8\" PTTYPE=\"dos\" /dev/sda2: UUID=\"f8e99f04-88eb-4550-9308-10a470175e45\" TYPE=\"swap\" PARTUUID=\"1a39b366-02\" Change /etc/fstab accordingly ( sda1 UUID did not change, only swap changed): # / was on /dev/sda1 during installation UUID=082b4523-f4d6-4d39-b5dd-48c5bdba2541 / ext4 errors=remount-ro 0 1 # swap was on /dev/sda2 during installation UUID=f8e99f04-88eb-4550-9308-10a470175e45 none swap sw 0 0 We should have booked our operation in etckeeper (not done on dockers-prod, etckeeper was not yet installed) etckeeper commit \"changed partition size\" Now we resized the partition, but we have to resize the filesystem. Let's resize sda1: resize2fs /dev/sda1 Reactivate swap: swapon -a Let's see our changes: df -h / Filesystem Size Used Avail Use% Mounted on /dev/sda1 294G 187G 93G 67% /","title":"Extending partition size"},{"location":"reports/2021-12-22-preprod-crash/","text":"Preprod crash 2021 12 # What happens # zfs storage on OVH2 was full this blocks the restart of two VM : dockers (200) and dockers-prod (201) Why this happens # probably because of resize of partition the day before on both machine, this makes snapshot diverge and take much more space. (see 2021-12-21-disk-extension ) out of space on zfs, blocks machine because proxmox needs to snapshot memory What was done # The same day: rollback VM dockers (200) to its snapshoted version removed dockers-prod (201) snapshot extension of disk space for this VM dockers (200) was reduced from 400G to 300G (this is still a 50G improvment over the previous size for dockers VM) The day after: removed the efficientnet.tar.gz in /home/off/robotoff-ann-net as it was already untared in the ann_data folder hard reboot of VM dockers (200) resized partition to 300G following previous operating mode","title":"Preprod crash 2021 12"},{"location":"reports/2021-12-22-preprod-crash/#preprod-crash-2021-12","text":"","title":"Preprod crash 2021 12"},{"location":"reports/2021-12-22-preprod-crash/#what-happens","text":"zfs storage on OVH2 was full this blocks the restart of two VM : dockers (200) and dockers-prod (201)","title":"What happens"},{"location":"reports/2021-12-22-preprod-crash/#why-this-happens","text":"probably because of resize of partition the day before on both machine, this makes snapshot diverge and take much more space. (see 2021-12-21-disk-extension ) out of space on zfs, blocks machine because proxmox needs to snapshot memory","title":"Why this happens"},{"location":"reports/2021-12-22-preprod-crash/#what-was-done","text":"The same day: rollback VM dockers (200) to its snapshoted version removed dockers-prod (201) snapshot extension of disk space for this VM dockers (200) was reduced from 400G to 300G (this is still a 50G improvment over the previous size for dockers VM) The day after: removed the efficientnet.tar.gz in /home/off/robotoff-ann-net as it was already untared in the ann_data folder hard reboot of VM dockers (200) resized partition to 300G following previous operating mode","title":"What was done"},{"location":"reports/2022-02-18-ovh-reverse-proxy-down/","text":"[Postmortem] Reverse proxy down # Date : 2022-02-18 Authors : Alex alex@openfoodfacts.org Summary : wiki.openfoodfacts.net down after ovh1 reboot, as well as all services using a CNAME on proxy1.openfoodfacts.org or proxy.openfoodfacts.org. Impact : Tools / services unreachable : feedme, wiki, slack page, etc. Root Causes : Network configuration on proxy (VM 101) did not return to a correct state after reboot, because of two default gateway resulting in a race condition, and in an incorrect default gateway for public ip (fail over ip) of proxy. Trigger : Reboot of ovh1, to be able to cleanup some old VM, blocked by stalled processes (uninterruptible sleep) Diagnosis # We got a problem after rebooting ovh1 : the proxy was not responding on it's public ip. On proxy VM aka 101 . We had two IPs, one internal (NAT mode) eth0 ( 10.1.0.101 ) and one public (bridged) sharing a virtual bridge with host: eth1 193.70.55.124 . The problem was that nated requests (passing through ovh1 ip and forwarded by iptables) did work but not requests to public ip. * ping to ip did work * nc -vz 193.70.55.124 80 did succeed * lsof -i tcp in proxy did show nginx listening on all interfaces * but doing HTTP requests manually using nc , did not succeed (it was working only from inside the VM) proxy: root@proxy:/etc/nginx# nc -v 193 .70.55.124 80 proxy1.openfoodfacts.org [ 193 .70.55.124 ] 80 ( http ) open GET / <html> <head><title>301 Moved Permanently</title></head> my computer: alex@tignasse:~$ nc -v 193 .70.55.124 80 Connection to 193 .70.55.124 80 port [ tcp/http ] succeeded! GET / HTTP/1.1 Long story short: the problem was that default route was passing through eth0, and thus packets did not return. # ip route list default via 10 .0.0.1 dev eth0 10 .0.0.1 dev eth0 scope link 10 .1.0.0/24 dev eth0 proto kernel scope link src 10 .1.0.101 193 .70.55.1 dev eth1 scope link Using: ip route del default via 10 .0.0.1 dev eth0 ip route add default via 193 .70.55.1 dev eth1 We get to: # ip route list default via 193 .70.55.1 dev eth1 10 .0.0.1 dev eth0 scope link 10 .1.0.0/24 dev eth0 proto kernel scope link src 10 .1.0.101 193 .70.55.1 dev eth1 scope link And everything works. Resolution tentative 1 # The problem: proxmox do control the interfaces settings and in /etc/network/interfaces , and route are set there using post-up and pre-down rules. But as this part is managed by Proxmox, hence this is a bit hard to modify. And at boot time, eth cards are in competition to have their route the global default and eth0 wins over eth1, leading to the non working situation. I did try to add this a script in /etc/network/if-up.d/90-insure-eth1-default-route #!/usr/bin/env bash # this script tries to ensure defaults route is on eth1 # this is important, otherwise apps served on eth1 won't be able to respond to incoming packets # if eth1 is up # it should be in /etc/network/if-up.d/ # ..note: this is a bit fragile for we hardcode the gateway here # but greping for right gateway in interfaces seems a bit overkill.. if ( ip address show eth1 | grep \"state UP\" ) then # and default route not via eth1 if ! ( ip route show default | grep eth1 ) then > & 2 echo \"Setting up default route via eth1\" # eventually remove eth0 route ip route del default via 10 .0.0.1 dev eth0 # add eth1 route ip route add default via 193 .70.55.1 dev eth1 \\ || > & 2 echo \"Error while adding default route via eth1\" fi fi this is a bit complicated though and hard code ip, while we would have to keep them in sync with proxmox settings. Note: to use a script in /etc/network/if-up.d/ , remember to chmod +x but also do not use any extension (a .sh extension makes run-parts ignore it !). Final Resolution # Christian (@cquest) proposes a more radical solution: remove gateway for eth0, and only keep it for eth1 . This has the advantage of being driven from proxmox. Before: After: This works but then we cannot use forwarding requests through ovh1 ip + iptables. But that's fine, as this was not a portable configuration however (if we have to move proxy, there would have been no ip failover). Still we add the gateway for eth0 with ip route, until we moved all CNAME to the public ip proxy1.openfoodfacts.org. So a second action was to move CNAME for all services behind proxy from ovh1.openfoodfacts.org to proxy1.openfoodfacts.org","title":"[Postmortem] Reverse proxy down"},{"location":"reports/2022-02-18-ovh-reverse-proxy-down/#postmortem-reverse-proxy-down","text":"Date : 2022-02-18 Authors : Alex alex@openfoodfacts.org Summary : wiki.openfoodfacts.net down after ovh1 reboot, as well as all services using a CNAME on proxy1.openfoodfacts.org or proxy.openfoodfacts.org. Impact : Tools / services unreachable : feedme, wiki, slack page, etc. Root Causes : Network configuration on proxy (VM 101) did not return to a correct state after reboot, because of two default gateway resulting in a race condition, and in an incorrect default gateway for public ip (fail over ip) of proxy. Trigger : Reboot of ovh1, to be able to cleanup some old VM, blocked by stalled processes (uninterruptible sleep)","title":"[Postmortem] Reverse proxy down"},{"location":"reports/2022-02-18-ovh-reverse-proxy-down/#diagnosis","text":"We got a problem after rebooting ovh1 : the proxy was not responding on it's public ip. On proxy VM aka 101 . We had two IPs, one internal (NAT mode) eth0 ( 10.1.0.101 ) and one public (bridged) sharing a virtual bridge with host: eth1 193.70.55.124 . The problem was that nated requests (passing through ovh1 ip and forwarded by iptables) did work but not requests to public ip. * ping to ip did work * nc -vz 193.70.55.124 80 did succeed * lsof -i tcp in proxy did show nginx listening on all interfaces * but doing HTTP requests manually using nc , did not succeed (it was working only from inside the VM) proxy: root@proxy:/etc/nginx# nc -v 193 .70.55.124 80 proxy1.openfoodfacts.org [ 193 .70.55.124 ] 80 ( http ) open GET / <html> <head><title>301 Moved Permanently</title></head> my computer: alex@tignasse:~$ nc -v 193 .70.55.124 80 Connection to 193 .70.55.124 80 port [ tcp/http ] succeeded! GET / HTTP/1.1 Long story short: the problem was that default route was passing through eth0, and thus packets did not return. # ip route list default via 10 .0.0.1 dev eth0 10 .0.0.1 dev eth0 scope link 10 .1.0.0/24 dev eth0 proto kernel scope link src 10 .1.0.101 193 .70.55.1 dev eth1 scope link Using: ip route del default via 10 .0.0.1 dev eth0 ip route add default via 193 .70.55.1 dev eth1 We get to: # ip route list default via 193 .70.55.1 dev eth1 10 .0.0.1 dev eth0 scope link 10 .1.0.0/24 dev eth0 proto kernel scope link src 10 .1.0.101 193 .70.55.1 dev eth1 scope link And everything works.","title":"Diagnosis"},{"location":"reports/2022-02-18-ovh-reverse-proxy-down/#resolution-tentative-1","text":"The problem: proxmox do control the interfaces settings and in /etc/network/interfaces , and route are set there using post-up and pre-down rules. But as this part is managed by Proxmox, hence this is a bit hard to modify. And at boot time, eth cards are in competition to have their route the global default and eth0 wins over eth1, leading to the non working situation. I did try to add this a script in /etc/network/if-up.d/90-insure-eth1-default-route #!/usr/bin/env bash # this script tries to ensure defaults route is on eth1 # this is important, otherwise apps served on eth1 won't be able to respond to incoming packets # if eth1 is up # it should be in /etc/network/if-up.d/ # ..note: this is a bit fragile for we hardcode the gateway here # but greping for right gateway in interfaces seems a bit overkill.. if ( ip address show eth1 | grep \"state UP\" ) then # and default route not via eth1 if ! ( ip route show default | grep eth1 ) then > & 2 echo \"Setting up default route via eth1\" # eventually remove eth0 route ip route del default via 10 .0.0.1 dev eth0 # add eth1 route ip route add default via 193 .70.55.1 dev eth1 \\ || > & 2 echo \"Error while adding default route via eth1\" fi fi this is a bit complicated though and hard code ip, while we would have to keep them in sync with proxmox settings. Note: to use a script in /etc/network/if-up.d/ , remember to chmod +x but also do not use any extension (a .sh extension makes run-parts ignore it !).","title":"Resolution tentative 1"},{"location":"reports/2022-02-18-ovh-reverse-proxy-down/#final-resolution","text":"Christian (@cquest) proposes a more radical solution: remove gateway for eth0, and only keep it for eth1 . This has the advantage of being driven from proxmox. Before: After: This works but then we cannot use forwarding requests through ovh1 ip + iptables. But that's fine, as this was not a portable configuration however (if we have to move proxy, there would have been no ip failover). Still we add the gateway for eth0 with ip route, until we moved all CNAME to the public ip proxy1.openfoodfacts.org. So a second action was to move CNAME for all services behind proxy from ovh1.openfoodfacts.org to proxy1.openfoodfacts.org","title":"Final Resolution"},{"location":"reports/2022-02-proxmox-mail-gateway-install/","text":"Install of proxmox gateway server # Note : Remember this documents keeps track of installation, it might not be complete or up-to-date See also: the mail documentation Proxmox Mail Gateway install # Following https://www.proxmox.com/en/proxmox-mail-gateway/get-started VM is already there as Christian as started I take at Install Proxmox Mail Gateway on Debian, see https://pmg.proxmox.com/pmg-docs/pmg-admin-guide.html#pmg_install_on_debian I started with apt update && apt upgrade * I had to re-install package manager version of config file for clamav * I also installed vim ;-) * I edited /etc/locale.gen to add fr_FR.UTF-8 then I run locale-gen Then apt install proxmox-mailgateway reading /etc/network/interfaces show me we have a static address, this is ok /etc/apt/sources.list.d/pmg-enterprise.list is ok section 3.5 . Also gpg key is ok I skipped the non-free section, we do not care about rar files. To get graphical user interface: ssh -v -L 8006:10.1.0.102:8006 ovh1.openfoodfacts.org then https://localhost:8006/, I had to accept risk (auto-signed certificate) to proceed. I edit /etc/pmg/user.conf following section 16.3 . first create a password hash: openssl passwd -5 -stdin -salt xxxxxx ****** where xxxx was randomly generated (using pwgen 8 ) **** is the password. Then add this line to /etc/pmg/user.conf : alex:1:0:$5$xxxxxx$xxxxxxxxxx:admin:alex@openfoodfacts.org:Alex:Garel:: \ud83d\udcdd\ufe0f Note: you need the '::' at the end contrarily of what's written in documentation (there is a column after keys) see bug https://bugzilla.proxmox.com/show_bug.cgi?id=3879 \u2757\ufe0fWarning: '+' is not an acceptable sign in password for it will be converted to space by http !!! Following section 4.5.2 Following section 4.6.3 Prepared: - In ovh web DNS , I registered pmg.openfoodfacts.org as a CNAME to ovh1.openfoodfacts.org - I changed the configuration on nginx proxy (lxc 101) to redirect to pmg (so that certbot could work) : ``` root@proxy:/etc/nginx/conf.d# cat pmg.openfoodfacts.org.conf # PMG stands for Promox Mail Gateway # We need to redirect port 80, for letsencrypt's certificate management server { listen 80; listen [::]:80; server_name pmg.openfoodfacts.org; access_log /var/log/nginx/pmg.off.log main; error_log /var/log/nginx/pmg.off_errors.log; location / { proxy_pass http://10.1.0.102:80$request_uri; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto https; proxy_read_timeout 90; client_max_body_size 512M; } } root@proxy:/etc/nginx/conf.d# nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful root@proxy:/etc/nginx/conf.d# systemctl reload nginx ``` On the pmg machine: root@mail-gateway:/home/alex# pmgconfig acme account register default tech@openfoodfacts.org Directory endpoints: 0) Let's Encrypt V2 (https://acme-v02.api.letsencrypt.org/directory) 1) Let's Encrypt V2 Staging (https://acme-staging-v02.api.letsencrypt.org/directory) 2) Custom Enter selection: 0 Attempting to fetch Terms of Service from 'https://acme-v02.api.letsencrypt.org/directory'.. Terms of Service: https://letsencrypt.org/documents/LE-SA-v1.2-November-15-2017.pdf Do you agree to the above terms? [y|N]: y Attempting to register account with 'https://acme-v02.api.letsencrypt.org/directory'.. Registering new ACME account.. Registration successful, account URL: 'https://acme-v02.api.letsencrypt.org/acme/acct/403308140' Task OK On the pmg web interface, in Certificates section, in ACME accounts / challenges I can see my account, http challenge plugin is already configured. Proxmox Mail Gateway configuration # In section Configuration, options tab, set Administrator Email to tech@openfoodfacts.org . Following 4.7.1 Relaying put default relay to smtp-relay.gmail.com . In our case, normally we should not relay (it's for incoming mails) Removed any relay domains Ports: I move public port to 24 and private port to 25, this is because we will only join PMV from trusted servers 4.7.8 Networks : - In networks tab I put the address of off1 and off2 without any mask. - Same for ovh1/2/3. - I also added usual private networks: (see https://en.wikipedia.org/wiki/Reserved_IP_addresses ): 10.0.0.0/8 , 172.16.0.0/12 , 192.168.0.0/16 4.7.9 TLS - In TLS tab: I enables TLS 4.7.10 DKIM - I added selector with pmg-openfoodfacts value, key size 2048 - Then I activated DKIM and tell to sign all outgoing mail - I clicked \"View DNS record\" - In ovh I then have to use the DKIM record type and use the form to create a similar looking entry: checked version / Algorithm checked hash 256 / checked key type / public key (I add to join the string) / service type none / checked key valid for subdomains I added domain openfoodfacts.org (still in DKIM tab) At this point I did a backup (Configuration : backup / restore) I tested it directly on the PMG machine: echo \"Subject: sendmail test\" | sudo sendmail -f alex@openfoodfacts.org -v alex@openfoodfacts.org iptables on ovh1 # FIXME: I stopped postfix on ovh1 sudo systemctl stop postfix sudo iptables -t nat -L shows me that masquerading is already configured: Chain POSTROUTING (policy ACCEPT) target prot opt source destination MASQUERADE all -- 10.1.0.0/16 anywhere MASQUERADE all -- 10.1.0.0/16 anywhere MASQUERADE all -- 10.1.0.0/16 anywhere MASQUERADE all -- anywhere anywhere MASQUERADE all -- 10.1.0.0/16 anywhere I configure NAT to postfix ~~ sudo iptables -t nat -A PREROUTING -p tcp --dport 25 -j DNAT --to 10.1.0.102:25 ~~ Note that, because we use nating, this means that pmg sees all incoming mails as coming from 10.1.0.1 thus avoiding PMG to filter out addresses So finally we have to limit to certain machines directly in iptables: sudo iptables -t nat -A PREROUTING -s 10.0.0.0/8,172.16.0.0/12,192.168.0.0/16 -d pmg.openfoodfacts.org -p tcp --dport 25 -j DNAT --to 10.1.0.102:25 sudo iptables -t nat -A PREROUTING -s 213.36.253.206,213.36.253.208,146.59.148.140,51.210.154.203,1.210.32.79 -d pmg.openfoodfacts.org -p tcp --dport 25 -j DNAT --to 10.1.0.102:25 \u2757\ufe0fNote : important to set destination, otherwise pmg won't be able to reach external servers (because nat rule will apply to it's outgoing requests !) To save rules (ovh1 use netfilter-persistent): cd /etc git status iptables-save -f iptables/rules.v4 etckeeper commit \"iptables rules to redirect port 25 to PMG proxmox mail gateway\" configuring servers # on dockers (aka preprod) # Configuration VM200 to send emails (this is a good candidates for tests): sudo apt intall postfix * configuration type : satellite system * mail name: openfoodfacts.org * relayhost: pmg.openfoodfacts.org * mail for root: tech@openfoodfacts.org * other dest: blank * sync: no * local network: leave default * use procmail: no * default for the rest test on preprod: echo \"Subject: sendmail test\" | sendmail -f alex@openfoodfacts.org -v alex@openfoodfacts.org on off1 # dpkg-reconfigure exim4-config * mail sent by smarthost; no local mail * mail name: openfoodfacts.org * listen: 127.0.0.1 * other dest: off1.free.org * visible domain name : openfoodfacts.org * IP address smarthost: pmg.openfoodfacts.org * keep DNS queries minimal (dial-up): no * local: maildir format in home * split config : no on off2 # Same config of exim as on off1. But Initially I cannot send ! It's an iptables rule directly on the machine that block OUTGOING smtp requests: REJECT tcp -- anywhere anywhere tcp dpt:smtp reject-with icmp-port-unreachable` I removed this rule and put instead a rule to limit destination: iptables -A OUTPUT -p tcp '!' -d '146.59.148.140' --dport smtp -j REJECT --reject-with icmp-port-unreachable I don't see any difference in iptables on ovh1. I've tried to log on ovh1: sudo iptables -I INPUT -s 213.36.253.208,213.36.253.206 -j LOG --log-prefix '** ALEX DEBUG **' sudo iptables -t nat -I PREROUTING -s 213.36.253.208,213.36.253.206 -j LOG --log-prefix '** ALEX DEBUG PRE **' but I don't see any incoming request ! Trying to get root mails: I look into masquerading and see Step 4 \u2014 Forwarding System Mail https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-postfix-as-a-send-only-smtp-server-on-debian-10 https://debian-facile.org/doc:reseau:exim4:redirection-mails-locaux but on debian /etc/mailname also does the job see https://wiki.debian.org/Postfix and https://wiki.debian.org/EtcMailName but I'm not able to have alias work with root: root@openfoodfacts.org on ovh3 # strange configuration in aliases, sending emails to admins@openfoodfacts.org ! When I unblocked the mails, I got an incredible of mails sent to root@openfoodfacts.org ! (because of a munin error, running in cron every 5 minute and sending a mail every time). I add to suppress whole queue in PMG admin\u2026 (because it does not allow to bulk delete in queue). I received emails for more than 2 hours (fortunately gmail did some throttling) Munin error was: There is nothing to do here, since there are no nodes with any plugins . I did a munin-node-configure --suggest --shell without success. Finally changing munin.conf to use 127.0.0.1 for ovh3 node (instead of 10.1.0.3) fixed it ! Testing mail quality # in thunderbird I setup a smtp using pmg.openfoodfacts.org I added an ip rule to forward if source is my ip address (from home) i sent a mail, result: https://www.mail-tester.com/test-kuklm574u then https://www.mail-tester.com/test-g6jk6pdto (after fixing name)","title":"Install of proxmox gateway server"},{"location":"reports/2022-02-proxmox-mail-gateway-install/#install-of-proxmox-gateway-server","text":"Note : Remember this documents keeps track of installation, it might not be complete or up-to-date See also: the mail documentation","title":"Install of proxmox gateway server"},{"location":"reports/2022-02-proxmox-mail-gateway-install/#proxmox-mail-gateway-install","text":"Following https://www.proxmox.com/en/proxmox-mail-gateway/get-started VM is already there as Christian as started I take at Install Proxmox Mail Gateway on Debian, see https://pmg.proxmox.com/pmg-docs/pmg-admin-guide.html#pmg_install_on_debian I started with apt update && apt upgrade * I had to re-install package manager version of config file for clamav * I also installed vim ;-) * I edited /etc/locale.gen to add fr_FR.UTF-8 then I run locale-gen Then apt install proxmox-mailgateway reading /etc/network/interfaces show me we have a static address, this is ok /etc/apt/sources.list.d/pmg-enterprise.list is ok section 3.5 . Also gpg key is ok I skipped the non-free section, we do not care about rar files. To get graphical user interface: ssh -v -L 8006:10.1.0.102:8006 ovh1.openfoodfacts.org then https://localhost:8006/, I had to accept risk (auto-signed certificate) to proceed. I edit /etc/pmg/user.conf following section 16.3 . first create a password hash: openssl passwd -5 -stdin -salt xxxxxx ****** where xxxx was randomly generated (using pwgen 8 ) **** is the password. Then add this line to /etc/pmg/user.conf : alex:1:0:$5$xxxxxx$xxxxxxxxxx:admin:alex@openfoodfacts.org:Alex:Garel:: \ud83d\udcdd\ufe0f Note: you need the '::' at the end contrarily of what's written in documentation (there is a column after keys) see bug https://bugzilla.proxmox.com/show_bug.cgi?id=3879 \u2757\ufe0fWarning: '+' is not an acceptable sign in password for it will be converted to space by http !!! Following section 4.5.2 Following section 4.6.3 Prepared: - In ovh web DNS , I registered pmg.openfoodfacts.org as a CNAME to ovh1.openfoodfacts.org - I changed the configuration on nginx proxy (lxc 101) to redirect to pmg (so that certbot could work) : ``` root@proxy:/etc/nginx/conf.d# cat pmg.openfoodfacts.org.conf # PMG stands for Promox Mail Gateway # We need to redirect port 80, for letsencrypt's certificate management server { listen 80; listen [::]:80; server_name pmg.openfoodfacts.org; access_log /var/log/nginx/pmg.off.log main; error_log /var/log/nginx/pmg.off_errors.log; location / { proxy_pass http://10.1.0.102:80$request_uri; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto https; proxy_read_timeout 90; client_max_body_size 512M; } } root@proxy:/etc/nginx/conf.d# nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful root@proxy:/etc/nginx/conf.d# systemctl reload nginx ``` On the pmg machine: root@mail-gateway:/home/alex# pmgconfig acme account register default tech@openfoodfacts.org Directory endpoints: 0) Let's Encrypt V2 (https://acme-v02.api.letsencrypt.org/directory) 1) Let's Encrypt V2 Staging (https://acme-staging-v02.api.letsencrypt.org/directory) 2) Custom Enter selection: 0 Attempting to fetch Terms of Service from 'https://acme-v02.api.letsencrypt.org/directory'.. Terms of Service: https://letsencrypt.org/documents/LE-SA-v1.2-November-15-2017.pdf Do you agree to the above terms? [y|N]: y Attempting to register account with 'https://acme-v02.api.letsencrypt.org/directory'.. Registering new ACME account.. Registration successful, account URL: 'https://acme-v02.api.letsencrypt.org/acme/acct/403308140' Task OK On the pmg web interface, in Certificates section, in ACME accounts / challenges I can see my account, http challenge plugin is already configured.","title":"Proxmox Mail Gateway install"},{"location":"reports/2022-02-proxmox-mail-gateway-install/#proxmox-mail-gateway-configuration","text":"In section Configuration, options tab, set Administrator Email to tech@openfoodfacts.org . Following 4.7.1 Relaying put default relay to smtp-relay.gmail.com . In our case, normally we should not relay (it's for incoming mails) Removed any relay domains Ports: I move public port to 24 and private port to 25, this is because we will only join PMV from trusted servers 4.7.8 Networks : - In networks tab I put the address of off1 and off2 without any mask. - Same for ovh1/2/3. - I also added usual private networks: (see https://en.wikipedia.org/wiki/Reserved_IP_addresses ): 10.0.0.0/8 , 172.16.0.0/12 , 192.168.0.0/16 4.7.9 TLS - In TLS tab: I enables TLS 4.7.10 DKIM - I added selector with pmg-openfoodfacts value, key size 2048 - Then I activated DKIM and tell to sign all outgoing mail - I clicked \"View DNS record\" - In ovh I then have to use the DKIM record type and use the form to create a similar looking entry: checked version / Algorithm checked hash 256 / checked key type / public key (I add to join the string) / service type none / checked key valid for subdomains I added domain openfoodfacts.org (still in DKIM tab) At this point I did a backup (Configuration : backup / restore) I tested it directly on the PMG machine: echo \"Subject: sendmail test\" | sudo sendmail -f alex@openfoodfacts.org -v alex@openfoodfacts.org","title":"Proxmox Mail Gateway configuration"},{"location":"reports/2022-02-proxmox-mail-gateway-install/#iptables-on-ovh1","text":"FIXME: I stopped postfix on ovh1 sudo systemctl stop postfix sudo iptables -t nat -L shows me that masquerading is already configured: Chain POSTROUTING (policy ACCEPT) target prot opt source destination MASQUERADE all -- 10.1.0.0/16 anywhere MASQUERADE all -- 10.1.0.0/16 anywhere MASQUERADE all -- 10.1.0.0/16 anywhere MASQUERADE all -- anywhere anywhere MASQUERADE all -- 10.1.0.0/16 anywhere I configure NAT to postfix ~~ sudo iptables -t nat -A PREROUTING -p tcp --dport 25 -j DNAT --to 10.1.0.102:25 ~~ Note that, because we use nating, this means that pmg sees all incoming mails as coming from 10.1.0.1 thus avoiding PMG to filter out addresses So finally we have to limit to certain machines directly in iptables: sudo iptables -t nat -A PREROUTING -s 10.0.0.0/8,172.16.0.0/12,192.168.0.0/16 -d pmg.openfoodfacts.org -p tcp --dport 25 -j DNAT --to 10.1.0.102:25 sudo iptables -t nat -A PREROUTING -s 213.36.253.206,213.36.253.208,146.59.148.140,51.210.154.203,1.210.32.79 -d pmg.openfoodfacts.org -p tcp --dport 25 -j DNAT --to 10.1.0.102:25 \u2757\ufe0fNote : important to set destination, otherwise pmg won't be able to reach external servers (because nat rule will apply to it's outgoing requests !) To save rules (ovh1 use netfilter-persistent): cd /etc git status iptables-save -f iptables/rules.v4 etckeeper commit \"iptables rules to redirect port 25 to PMG proxmox mail gateway\"","title":"iptables on ovh1"},{"location":"reports/2022-02-proxmox-mail-gateway-install/#configuring-servers","text":"","title":"configuring servers"},{"location":"reports/2022-02-proxmox-mail-gateway-install/#on-dockers-aka-preprod","text":"Configuration VM200 to send emails (this is a good candidates for tests): sudo apt intall postfix * configuration type : satellite system * mail name: openfoodfacts.org * relayhost: pmg.openfoodfacts.org * mail for root: tech@openfoodfacts.org * other dest: blank * sync: no * local network: leave default * use procmail: no * default for the rest test on preprod: echo \"Subject: sendmail test\" | sendmail -f alex@openfoodfacts.org -v alex@openfoodfacts.org","title":"on dockers (aka preprod)"},{"location":"reports/2022-02-proxmox-mail-gateway-install/#on-off1","text":"dpkg-reconfigure exim4-config * mail sent by smarthost; no local mail * mail name: openfoodfacts.org * listen: 127.0.0.1 * other dest: off1.free.org * visible domain name : openfoodfacts.org * IP address smarthost: pmg.openfoodfacts.org * keep DNS queries minimal (dial-up): no * local: maildir format in home * split config : no","title":"on off1"},{"location":"reports/2022-02-proxmox-mail-gateway-install/#on-off2","text":"Same config of exim as on off1. But Initially I cannot send ! It's an iptables rule directly on the machine that block OUTGOING smtp requests: REJECT tcp -- anywhere anywhere tcp dpt:smtp reject-with icmp-port-unreachable` I removed this rule and put instead a rule to limit destination: iptables -A OUTPUT -p tcp '!' -d '146.59.148.140' --dport smtp -j REJECT --reject-with icmp-port-unreachable I don't see any difference in iptables on ovh1. I've tried to log on ovh1: sudo iptables -I INPUT -s 213.36.253.208,213.36.253.206 -j LOG --log-prefix '** ALEX DEBUG **' sudo iptables -t nat -I PREROUTING -s 213.36.253.208,213.36.253.206 -j LOG --log-prefix '** ALEX DEBUG PRE **' but I don't see any incoming request ! Trying to get root mails: I look into masquerading and see Step 4 \u2014 Forwarding System Mail https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-postfix-as-a-send-only-smtp-server-on-debian-10 https://debian-facile.org/doc:reseau:exim4:redirection-mails-locaux but on debian /etc/mailname also does the job see https://wiki.debian.org/Postfix and https://wiki.debian.org/EtcMailName but I'm not able to have alias work with root: root@openfoodfacts.org","title":"on off2"},{"location":"reports/2022-02-proxmox-mail-gateway-install/#on-ovh3","text":"strange configuration in aliases, sending emails to admins@openfoodfacts.org ! When I unblocked the mails, I got an incredible of mails sent to root@openfoodfacts.org ! (because of a munin error, running in cron every 5 minute and sending a mail every time). I add to suppress whole queue in PMG admin\u2026 (because it does not allow to bulk delete in queue). I received emails for more than 2 hours (fortunately gmail did some throttling) Munin error was: There is nothing to do here, since there are no nodes with any plugins . I did a munin-node-configure --suggest --shell without success. Finally changing munin.conf to use 127.0.0.1 for ovh3 node (instead of 10.1.0.3) fixed it !","title":"on ovh3"},{"location":"reports/2022-02-proxmox-mail-gateway-install/#testing-mail-quality","text":"in thunderbird I setup a smtp using pmg.openfoodfacts.org I added an ip rule to forward if source is my ip address (from home) i sent a mail, result: https://www.mail-tester.com/test-kuklm574u then https://www.mail-tester.com/test-g6jk6pdto (after fixing name)","title":"Testing mail quality"},{"location":"reports/2022-02-remove-containers-ovh1/","text":"2022-02 Removing some containers on ovh1 # Backups # On datacenter we have a backup menu --> click edit you can see the policies. We only have one. We use snapshot mode as we are on ZFS, and we use ZSTD compression. There also we configure which mail will be notified on errors. Some VM maybe excluded because inactive Backups goes in the backups disk resource (NFS mount of ovh3 zpool, NFS is integrated in ZFS). If we click on it we can see the backups. Backup of a machine # 115 backup is a bit old. We do a manal backup. We go on 115, backup tab, type snapshot , compression ZSTD . Problem: It can't acquire global lock\u2026 We see that there are no backup since quite a long date of ovh1 machine ! We also see that backup task on ovh1 have been failing for a while ! We didn't have mail, but ovh1 mail was not sending reliabely and more over the address where to send alerts was wrong (it was that of the CA). It seems that there is an old blocked vzdump running since 2021 !. We had to kill a lot of processes to get all down, and then remove /var/run/vzdump.{pid,lock} We then had to remove a backup because we where above the limit of 4 backups We then backup, it was a bit long (25 minutes). Removal of a VM # Important : * Ensure you have backups for the machine before removal * stop the machine In options we edit protection to remove \"Yes\" if present. In upper bar on the machine : More -> destroy You then check \"remove from tasks\" We had a failure because it cun't umount rpool/subvol-115-disk-0. This maybe due to mounts inside it. cat mtab |grep subvol ... 10.0.0.3:/rpool/off/images-clone /rpool/subvol-115-disk-0/mnt/images nfs4 rw,relatime,vers=4.2,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.0.0.1,local_lock=none,addr=10.0.0.3 0 0 10.0.0.3:/rpool/off/products-clone /rpool/subvol-115-disk-0/mnt/products nfs4 rw,relatime,vers=4.2,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.0.0.1,local_lock=none,addr=10.0.0.3 0 0 ... umount myself: umount /rpool/subvol-115-disk-0/mnt/images umount /rpool/subvol-115-disk-0/mnt/products we check zfs / CT volumes, there is no more 115 volume, and we see the free space on the graph. We also removed 110 machine. It was hard because of a process in status D (uninteruptible sleep). We had to reboot OVH1. Side Note: we see that munin is down, it's because Christian monitor it from it's own munin. Important remember to report changes to https://docs.google.com/spreadsheets/d/19RePmE_-1V_He73fpMJYukEMjPWNmav1610yjpGKfD0/edit#gid=0 Creating a VM # Right click on ovh1 \"create VM) or use the button up right. We prefer not to reuse an ip that: * from 100 to 200 for lxc containers * from 200 and up fot QEMU Creating a QEMU VM # use the iso in backups","title":"2022-02 Removing some containers on ovh1"},{"location":"reports/2022-02-remove-containers-ovh1/#2022-02-removing-some-containers-on-ovh1","text":"","title":"2022-02 Removing some containers on ovh1"},{"location":"reports/2022-02-remove-containers-ovh1/#backups","text":"On datacenter we have a backup menu --> click edit you can see the policies. We only have one. We use snapshot mode as we are on ZFS, and we use ZSTD compression. There also we configure which mail will be notified on errors. Some VM maybe excluded because inactive Backups goes in the backups disk resource (NFS mount of ovh3 zpool, NFS is integrated in ZFS). If we click on it we can see the backups.","title":"Backups"},{"location":"reports/2022-02-remove-containers-ovh1/#backup-of-a-machine","text":"115 backup is a bit old. We do a manal backup. We go on 115, backup tab, type snapshot , compression ZSTD . Problem: It can't acquire global lock\u2026 We see that there are no backup since quite a long date of ovh1 machine ! We also see that backup task on ovh1 have been failing for a while ! We didn't have mail, but ovh1 mail was not sending reliabely and more over the address where to send alerts was wrong (it was that of the CA). It seems that there is an old blocked vzdump running since 2021 !. We had to kill a lot of processes to get all down, and then remove /var/run/vzdump.{pid,lock} We then had to remove a backup because we where above the limit of 4 backups We then backup, it was a bit long (25 minutes).","title":"Backup of a machine"},{"location":"reports/2022-02-remove-containers-ovh1/#removal-of-a-vm","text":"Important : * Ensure you have backups for the machine before removal * stop the machine In options we edit protection to remove \"Yes\" if present. In upper bar on the machine : More -> destroy You then check \"remove from tasks\" We had a failure because it cun't umount rpool/subvol-115-disk-0. This maybe due to mounts inside it. cat mtab |grep subvol ... 10.0.0.3:/rpool/off/images-clone /rpool/subvol-115-disk-0/mnt/images nfs4 rw,relatime,vers=4.2,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.0.0.1,local_lock=none,addr=10.0.0.3 0 0 10.0.0.3:/rpool/off/products-clone /rpool/subvol-115-disk-0/mnt/products nfs4 rw,relatime,vers=4.2,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.0.0.1,local_lock=none,addr=10.0.0.3 0 0 ... umount myself: umount /rpool/subvol-115-disk-0/mnt/images umount /rpool/subvol-115-disk-0/mnt/products we check zfs / CT volumes, there is no more 115 volume, and we see the free space on the graph. We also removed 110 machine. It was hard because of a process in status D (uninteruptible sleep). We had to reboot OVH1. Side Note: we see that munin is down, it's because Christian monitor it from it's own munin. Important remember to report changes to https://docs.google.com/spreadsheets/d/19RePmE_-1V_He73fpMJYukEMjPWNmav1610yjpGKfD0/edit#gid=0","title":"Removal of a VM"},{"location":"reports/2022-02-remove-containers-ovh1/#creating-a-vm","text":"Right click on ovh1 \"create VM) or use the button up right. We prefer not to reuse an ip that: * from 100 to 200 for lxc containers * from 200 and up fot QEMU","title":"Creating a VM"},{"location":"reports/2022-02-remove-containers-ovh1/#creating-a-qemu-vm","text":"use the iso in backups","title":"Creating a QEMU VM"},{"location":"reports/2022-03-03-zammad-elasticsearch-not-running/","text":"Elasticsearch not running on Zammad # Date: 2022-03-03 Problem # I wanted to Set up ES for Zammad After having access to the machine (which up to now is gracefully hosted by CQuest), I verify, as told by CQuest, that elasticsearch was installed. The problem was the elasticsearch service was not running, according to systemctl status elasticsearch . journalctl -r -u elasticsearch shows this error: Failed to start elasticsearch due to a fatal signal received by control process (code=killed, signal=9/KILL) Solution # After searching about in various directions, it reminds me of a OOMKill situation. Although we didn't have a OOMKill message in /var/log/syslog because this is an LXC container, so OOMKill happens in the guest. As I look into /etc/elasticsearch/jvm.options I've seen that by default it handles memory alone by looking at available memory. Being in an LXC container, it sees more memory than the container is allowed to take. Hence the kill situation. Thus I decided to fix the heap size manually. I created a file /etc/elasticsearch/jvm.options.d/memory.options 1 with: -Xms300m -Xmx300m After a systemctl start elasticsearch , elasticsearch was up. Resolution 2 - zammad messed up # I first had a hardtime figuring out how zammad was running. The real services in /etc/systemd/system/ are the one with a -1 in their name (others are just sleep commands, they act as placeholders to handle dependencies, I guess). But the executable was not there ! This was because I unadvertantly de-installed it, while trying to solve elasticsearch issue (where I did a uninstall / reinstall of elasticsearch). I did not see it was also de-installing zammad :-/ Hopefully I did not purge, and an apt install zammad solved the issue ! Finish ES integration # Following https://docs.zammad.org/en/latest/install/elasticsearch.html, I wanted to verify settings: $ zammad run rails r 'print Setting.get(\"es_index\") + \"\\n\"' zammad I reindex everything using zammad run rake searchindex:rebuild as indicated on zammad issue 1630 It failed with: rake aborted! Unable to send Ticket.find(33).search_index_update_backend backend: #<RuntimeError: Unable to process post request to elasticsearch URL 'http://localhost:9200/zammad_production_ticket/_doc/33?pipeline=zammad116611039594'. Elasticsearch is not reachable, probably because it's not running or even installed. Response: #<UserAgent::Result:0x0000563cf7686228 @success=false, @body=nil, @data=nil, @code=0, @content_type=nil, @error=\"#<Errno::ECONNREFUSED: Failed to open TCP connection to localhost:9200 (Connection refused - connect(2) for \\\"localhost\\\" port 9200)>\", @header=nil> Indeed systemctl status elasticsearch shows me it failed. Due to Terminating due to java.lang.OutOfMemoryError: Java heap space I edit /etc/elasticsearch/jvm.options.d/memory.options to give more memory: -Xms600m -Xmx600m Then relaunched: zammad run rake searchindex:rebuild Finally it works ! Lessons learned # This Elasticsearch memory problem is a recurring one ! In this case it was hard to spot at first sight. But this is always a lead to investigate on a hard kill of an Elasticsearch instance. Be prudent when doing apt remove to verify you aren't removing something else ! \ud83d\ude13 in fact I was induced in error because in jvm.options the mandatory .options extension was not cited, so I first created a file with no extension and it did not work (but no log could tell me that). Reading online doc I sort this out. The problem is already fixed in elasticsearch repos \u21a9","title":"Elasticsearch not running on Zammad"},{"location":"reports/2022-03-03-zammad-elasticsearch-not-running/#elasticsearch-not-running-on-zammad","text":"Date: 2022-03-03","title":"Elasticsearch not running on Zammad"},{"location":"reports/2022-03-03-zammad-elasticsearch-not-running/#problem","text":"I wanted to Set up ES for Zammad After having access to the machine (which up to now is gracefully hosted by CQuest), I verify, as told by CQuest, that elasticsearch was installed. The problem was the elasticsearch service was not running, according to systemctl status elasticsearch . journalctl -r -u elasticsearch shows this error: Failed to start elasticsearch due to a fatal signal received by control process (code=killed, signal=9/KILL)","title":"Problem"},{"location":"reports/2022-03-03-zammad-elasticsearch-not-running/#solution","text":"After searching about in various directions, it reminds me of a OOMKill situation. Although we didn't have a OOMKill message in /var/log/syslog because this is an LXC container, so OOMKill happens in the guest. As I look into /etc/elasticsearch/jvm.options I've seen that by default it handles memory alone by looking at available memory. Being in an LXC container, it sees more memory than the container is allowed to take. Hence the kill situation. Thus I decided to fix the heap size manually. I created a file /etc/elasticsearch/jvm.options.d/memory.options 1 with: -Xms300m -Xmx300m After a systemctl start elasticsearch , elasticsearch was up.","title":"Solution"},{"location":"reports/2022-03-03-zammad-elasticsearch-not-running/#resolution-2-zammad-messed-up","text":"I first had a hardtime figuring out how zammad was running. The real services in /etc/systemd/system/ are the one with a -1 in their name (others are just sleep commands, they act as placeholders to handle dependencies, I guess). But the executable was not there ! This was because I unadvertantly de-installed it, while trying to solve elasticsearch issue (where I did a uninstall / reinstall of elasticsearch). I did not see it was also de-installing zammad :-/ Hopefully I did not purge, and an apt install zammad solved the issue !","title":"Resolution 2 - zammad messed up"},{"location":"reports/2022-03-03-zammad-elasticsearch-not-running/#finish-es-integration","text":"Following https://docs.zammad.org/en/latest/install/elasticsearch.html, I wanted to verify settings: $ zammad run rails r 'print Setting.get(\"es_index\") + \"\\n\"' zammad I reindex everything using zammad run rake searchindex:rebuild as indicated on zammad issue 1630 It failed with: rake aborted! Unable to send Ticket.find(33).search_index_update_backend backend: #<RuntimeError: Unable to process post request to elasticsearch URL 'http://localhost:9200/zammad_production_ticket/_doc/33?pipeline=zammad116611039594'. Elasticsearch is not reachable, probably because it's not running or even installed. Response: #<UserAgent::Result:0x0000563cf7686228 @success=false, @body=nil, @data=nil, @code=0, @content_type=nil, @error=\"#<Errno::ECONNREFUSED: Failed to open TCP connection to localhost:9200 (Connection refused - connect(2) for \\\"localhost\\\" port 9200)>\", @header=nil> Indeed systemctl status elasticsearch shows me it failed. Due to Terminating due to java.lang.OutOfMemoryError: Java heap space I edit /etc/elasticsearch/jvm.options.d/memory.options to give more memory: -Xms600m -Xmx600m Then relaunched: zammad run rake searchindex:rebuild Finally it works !","title":"Finish ES integration"},{"location":"reports/2022-03-03-zammad-elasticsearch-not-running/#lessons-learned","text":"This Elasticsearch memory problem is a recurring one ! In this case it was hard to spot at first sight. But this is always a lead to investigate on a hard kill of an Elasticsearch instance. Be prudent when doing apt remove to verify you aren't removing something else ! \ud83d\ude13 in fact I was induced in error because in jvm.options the mandatory .options extension was not cited, so I first created a file with no extension and it did not work (but no log could tell me that). Reading online doc I sort this out. The problem is already fixed in elasticsearch repos \u21a9","title":"Lessons learned"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/","text":"Deploying openfoodfacts-search to staging # I'm taking note to further upgrade documentation or simply remember the steps through an example. Docker preparation # I added a prod.yml to make volume external (it's important to avoid the pitfall of having a docker down -v remove all data !, it also gives better control). I made it a bit different from other project where name are absolute, because it is also dangerous, if at some point we have two different deployments on same machine\u2026 volume would be shared. I prefer to avoid a nightmare. Avoiding root # We want to avoid dockers running root in production. I checked other container, one is elasticvue is in fact an nginx, elasticsearch change user after launch, redis is ok. Modifying the Dockerfile to create a user and pass user uid as parameter. Also modifying the makefile to add uid Securing Elasticvue access # Elasticvue gives full access to the ES instance, but we want to access it in prod because it is handy to have a quick access to ES. We need to secure it. As it is a vue app served by a nginx service, the best way is to have basic auth inside it. I first tried to use the configuration by template option provided by nginx official docker image, creating a elasticvue.conf.template and elasticvue_htppasswd.template. but finally elasticvue image use an old nginx docker version, which do not have this template mechanism. As it does not even have a specific entrypoint I redefined the entrypoint and use a script that creates the htpasswd file and use sed to edit the config, this is even more flexible. Image creation # I first added the image creation github action. Did copy from off-server (but we got template also in openfoodfacts) and adapting it. I had to take care that image name should be the same as the one in docker-compose. I also had to add the TAG variable in docker-compose to set image version. I did it on deploy-init-stagging branch (starting with deploy-* to trigger action). Deployment # I first added the image deployment github action. Did copy from off-server (but we got template also in openfoodfacts ) and adapting it. I commented prod deployment in the triggers (v.*) because I wanted to be sure stagging would work before. The environment name is important because it is the folder where the project will live. Better have it ending with -net or -org to mark the type of deployment (help not messing up in servers). If it's unique among servers it's better so that we can mix deployment on same machine if we want (eg. in an emergency scenario). Did wrote the create_external_volumes in Makefile. I tested locally by tweaking the env variable, and removing volumes afterwards declare -x DOCKER_LOCAL_DATA = $( pwd ) /tmp/ declare -x COMPOSE_PROJECT_NAME = po_search_prod declare -x COMPOSE_PATH_SEPARATOR = \";\" declare -x COMPOSE_FILE = \"docker-compose.yml;docker/prod.yml\" make create_external_volumes # test it works just starting setup docker-compose up setup sudo ls tmp/* # clean docker-compose rm -sf setup docker volume rm po_search_prod_ { certs,esdata01,esdata02,rediscache } unset DOCKER_LOCAL_DATA COMPOSE_PROJECT_NAME COMPOSE_PATH_SEPARATOR COMPOSE_FILE Secrets # There are a lot secrets to set on the github repo, I had to look at all used variables. I edited branch protection rule, because workflow are sensible: - restrict only admins and maintainers to push to deploy-* branches - same for main branch, with of course pull request review etc. To create a GRAPHANA token I had to go to graphana configuration -> API keys - made an editor key and put it at repository level First run # I did had a very hard way making connection to server successful: Lessons learned: - you have to connect through ovh1 as proxy, not ovh2 - secret key is to be as in the id_rsa key, that is with the \"BEGIN PRIVATE\u2026\" and \"END \u2026\" lines I had problem with the volume creation because /srv/off/docker_data was owned by root. sudo chown off:off /srv/off/docker_data Reverse proxy setup # On OVH1, attaching to lxc 101. Added config (copying from robotoff, removing all certbot specific stuff) Then test and reload: nginx -t systemctl reload nginx Also I generated the certificates: certbot -d search.openfoodfacts.net Enabling live update (2022-11-10) # see https://github.com/openfoodfacts/openfoodfacts-search/issues/28 Fix common net name # First I tried to see if I could ping redis from backend in off-net deployment. sudo -u off bash $ cd /home/off $ docker-compose exec -u root backend bash $ apt update && apt install iputils-ping $ ping searchredis PING searchredis.openfoodfacts.org ( 213 .36.253.206 ) 56 ( 84 ) bytes of data. 64 bytes from off1.free.org ( 213 .36.253.206 ) : icmp_seq = 1 ttl = 49 time = 6 .72 ms This is not working. After a small investigation, I found that the problem is the \"webnet\" name, which is po_webnet in off-net and webnet in off-search-net. in openfoodfacts-search github repo, I changed te deploy workflow so that COMMON_NET_NAME is po_webnet I git pushed as a \"deploy-\" branch and created a PR . then I was able to really ping searchredis container from backend ping searchredis PING searchredis ( 172 .30.0.12 ) 56 ( 84 ) bytes of data. 64 bytes from po_search_searchredis_1.po_webnet ( 172 .30.0.12 ) : icmp_seq = 1 ttl = 64 time = 0 .407 ms see https://github.com/openfoodfacts/openfoodfacts-search/issues/27 Setting REDIS_URL in off-net # simply set it in deploy (although I first forgot the port, which is part of the URL) see: https://github.com/openfoodfacts/openfoodfacts-server/pull/7682 Setting OPENFOODFACTS_API_URL # The search update was working but I did get errors telling some product did not exist, and having my products updates not taken into account. I finally realized, updates where fetch from openfoodfacts.org because we forgot to set OPENFOODFACTS_API_URL in deployment. I then had errors because I miss the basic auth that is necessary to reach openfoodfacts.net service. I added it in url and it worked. see https://github.com/openfoodfacts/openfoodfacts-search/pull/29","title":"Deploying openfoodfacts-search to staging"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#deploying-openfoodfacts-search-to-staging","text":"I'm taking note to further upgrade documentation or simply remember the steps through an example.","title":"Deploying openfoodfacts-search to staging"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#docker-preparation","text":"I added a prod.yml to make volume external (it's important to avoid the pitfall of having a docker down -v remove all data !, it also gives better control). I made it a bit different from other project where name are absolute, because it is also dangerous, if at some point we have two different deployments on same machine\u2026 volume would be shared. I prefer to avoid a nightmare.","title":"Docker preparation"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#avoiding-root","text":"We want to avoid dockers running root in production. I checked other container, one is elasticvue is in fact an nginx, elasticsearch change user after launch, redis is ok. Modifying the Dockerfile to create a user and pass user uid as parameter. Also modifying the makefile to add uid","title":"Avoiding root"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#securing-elasticvue-access","text":"Elasticvue gives full access to the ES instance, but we want to access it in prod because it is handy to have a quick access to ES. We need to secure it. As it is a vue app served by a nginx service, the best way is to have basic auth inside it. I first tried to use the configuration by template option provided by nginx official docker image, creating a elasticvue.conf.template and elasticvue_htppasswd.template. but finally elasticvue image use an old nginx docker version, which do not have this template mechanism. As it does not even have a specific entrypoint I redefined the entrypoint and use a script that creates the htpasswd file and use sed to edit the config, this is even more flexible.","title":"Securing Elasticvue access"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#image-creation","text":"I first added the image creation github action. Did copy from off-server (but we got template also in openfoodfacts) and adapting it. I had to take care that image name should be the same as the one in docker-compose. I also had to add the TAG variable in docker-compose to set image version. I did it on deploy-init-stagging branch (starting with deploy-* to trigger action).","title":"Image creation"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#deployment","text":"I first added the image deployment github action. Did copy from off-server (but we got template also in openfoodfacts ) and adapting it. I commented prod deployment in the triggers (v.*) because I wanted to be sure stagging would work before. The environment name is important because it is the folder where the project will live. Better have it ending with -net or -org to mark the type of deployment (help not messing up in servers). If it's unique among servers it's better so that we can mix deployment on same machine if we want (eg. in an emergency scenario). Did wrote the create_external_volumes in Makefile. I tested locally by tweaking the env variable, and removing volumes afterwards declare -x DOCKER_LOCAL_DATA = $( pwd ) /tmp/ declare -x COMPOSE_PROJECT_NAME = po_search_prod declare -x COMPOSE_PATH_SEPARATOR = \";\" declare -x COMPOSE_FILE = \"docker-compose.yml;docker/prod.yml\" make create_external_volumes # test it works just starting setup docker-compose up setup sudo ls tmp/* # clean docker-compose rm -sf setup docker volume rm po_search_prod_ { certs,esdata01,esdata02,rediscache } unset DOCKER_LOCAL_DATA COMPOSE_PROJECT_NAME COMPOSE_PATH_SEPARATOR COMPOSE_FILE","title":"Deployment"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#secrets","text":"There are a lot secrets to set on the github repo, I had to look at all used variables. I edited branch protection rule, because workflow are sensible: - restrict only admins and maintainers to push to deploy-* branches - same for main branch, with of course pull request review etc. To create a GRAPHANA token I had to go to graphana configuration -> API keys - made an editor key and put it at repository level","title":"Secrets"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#first-run","text":"I did had a very hard way making connection to server successful: Lessons learned: - you have to connect through ovh1 as proxy, not ovh2 - secret key is to be as in the id_rsa key, that is with the \"BEGIN PRIVATE\u2026\" and \"END \u2026\" lines I had problem with the volume creation because /srv/off/docker_data was owned by root. sudo chown off:off /srv/off/docker_data","title":"First run"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#reverse-proxy-setup","text":"On OVH1, attaching to lxc 101. Added config (copying from robotoff, removing all certbot specific stuff) Then test and reload: nginx -t systemctl reload nginx Also I generated the certificates: certbot -d search.openfoodfacts.net","title":"Reverse proxy setup"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#enabling-live-update-2022-11-10","text":"see https://github.com/openfoodfacts/openfoodfacts-search/issues/28","title":"Enabling live update (2022-11-10)"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#fix-common-net-name","text":"First I tried to see if I could ping redis from backend in off-net deployment. sudo -u off bash $ cd /home/off $ docker-compose exec -u root backend bash $ apt update && apt install iputils-ping $ ping searchredis PING searchredis.openfoodfacts.org ( 213 .36.253.206 ) 56 ( 84 ) bytes of data. 64 bytes from off1.free.org ( 213 .36.253.206 ) : icmp_seq = 1 ttl = 49 time = 6 .72 ms This is not working. After a small investigation, I found that the problem is the \"webnet\" name, which is po_webnet in off-net and webnet in off-search-net. in openfoodfacts-search github repo, I changed te deploy workflow so that COMMON_NET_NAME is po_webnet I git pushed as a \"deploy-\" branch and created a PR . then I was able to really ping searchredis container from backend ping searchredis PING searchredis ( 172 .30.0.12 ) 56 ( 84 ) bytes of data. 64 bytes from po_search_searchredis_1.po_webnet ( 172 .30.0.12 ) : icmp_seq = 1 ttl = 64 time = 0 .407 ms see https://github.com/openfoodfacts/openfoodfacts-search/issues/27","title":"Fix common net name"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#setting-redis_url-in-off-net","text":"simply set it in deploy (although I first forgot the port, which is part of the URL) see: https://github.com/openfoodfacts/openfoodfacts-server/pull/7682","title":"Setting REDIS_URL in off-net"},{"location":"reports/2022-07-08-journey-to-deploy-off-search/#setting-openfoodfacts_api_url","text":"The search update was working but I did get errors telling some product did not exist, and having my products updates not taken into account. I finally realized, updates where fetch from openfoodfacts.org because we forgot to set OPENFOODFACTS_API_URL in deployment. I then had errors because I miss the basic auth that is necessary to reach openfoodfacts.net service. I added it in url and it worked. see https://github.com/openfoodfacts/openfoodfacts-search/pull/29","title":"Setting OPENFOODFACTS_API_URL"},{"location":"reports/2022-07-11-infra-workshop/","text":"Infrastructure future - workshop on 11th July 2022 # Participants: Alex, Charles, Christian, Stephane, Pierre Goals # what goals do we want to achieve? Be able to scale horizontally disk space / scalability is one the main issue network management also Improve both the software and hardware side of the infrastructure Take ecology (carbon impact etc.) into account Human redundancy reduce stress Monitor thresholds, trends and critical alerts Redundancy, geographical PCA/PRA delegate, scale the potential for HR Reduce stress level Decrease the barrier to contribute for people who want to train their models 32 GB RAM, GPU for training Solutions # Priorities # Forecast threshold and critical alerts and trends Identify and document in one place thresholds and critical things to monitor => Infra repo on github; rationale: external tool decentralized tool: .md files than can be cloned by everyone Alex Identify and document mega-trends (eg. images weight on disk) Infra repo on github Which trends; what to look for to identify the trends What\u2019s critical and what\u2019s not Who? Software evolutions of the monitoring infrastructure which tools Eg. Elastic search Human process Documented in infrastructure repo. Who for what kind of task? Pipeline to acquire, fidelize and level up competent people/contributors Public infra meetings (40 minutes each month) Prioritization of issues Nearly achieved threshold \u2026 Alex, St\u00e9phane, Christian; community: Olivier? Hangy? Syl20? Alligator? SRE\u2019s Meetup in Paris? Admin sys without borders? => Github issues => Automatic issues? => Sentry? Human redundancy everywhere # Documentation: Document global infrastructure => Github openfoodfacts-infrastructure repo => .md files List all systems/areas and who masters what Identify gaps where we have only 1 or 2 persons e.g. zfs Identify owners: technical services, product owner Infrastructure spreadsheet Distinguish OFF\u2019s core infrastructure and peripheral services 3 levels: critical for us, critical for others, peripheral Technologies upskilling Fill gaps Skills spreadsheet: Munin, network, MongoDB, Docker, Proxmox, Nginx At least 2 people skilled of each technology Skills table: CT and VM list of OFF infrastructure TODO: fill the spreadsheet (All) Colleague's courses: ZFS, Docker, Proxmox, \u2026 Commercial courses: security (YesWeHack ?), diminish complexity where possible some complexity: due to hosting on OVH Horizontal scaling # 1rst step: OFF1-OFF2 redundancy. MongoDB on OFF1 OFF2 hardware + OS upgrade Proxmox on OFF1/OFF2? Ask free to add a 3rd machine to make the migration. This machine could be the future test server for IA. Email to Jean-Claude (Christian). Christian & Stephane are listing hardware needs. Allow to test the horizontal scaling. What main issues do we want to solve? Adding hardware easily. Disk space and availability: a. Scaling with S3. b. Software: distinguish cold and hot pictures: - some pictures are never asked c. Deduplicate. - first mesure impact - excellent topic for a contributor d. When croped images are not the latest one, only keep the crop. e. Convert JPEG to webp (-50%). f. Decrease resolution for 48Mpixel images (=> 12 Mpixel) Priority for disk space: f, e b for the long term CPU. What is scaling horizontally? MongoDB: OK. Apache: OK. Images? Use software routing to manage data with or without images or STO. STO? Short term # Disk space: 6 month to solve this issue. Very short term: x3 is possible but doesn\u2019t solve the issue in the long term. Against rsync! => ZFS synchronization OFF1 & OFF2 disks to ZFS buy 2x2 equivalent disks (+1 in case of emergency). Ask Syl20 for MongoDB optimisation. Focus on IO: on off1 rsync are impacting performance on off2 at night it gets slow Product use cases: scan speed facet speed advanced search speed nightly data exports of concurrent scan requests # Increase modularity: being able to upgrade with confidence being able to deploy regularly, and with confidence what kind of evolutions to achieve these goals? what kind of hardware evolutions to follow: 1. products' growth, 2. resource consumption growth (API, web, etc.) and 3. software evolution. what short-term evolutions to manage products' growth (6-18 months). Decrease the barrier to contribute for people who want to train their models # 64 GB RAM, GPU for training, Tool server with OSM? Potential resources # https://sre.google/books/ Small tasks # Document Munin in openfoodfacts-infrastructure Document what to look for in Munin Document monitoring usage Add API documentation for the reuse of Robotoff Plan de reprise de Robotoff","title":"Infrastructure future - workshop on 11th July 2022"},{"location":"reports/2022-07-11-infra-workshop/#infrastructure-future-workshop-on-11th-july-2022","text":"Participants: Alex, Charles, Christian, Stephane, Pierre","title":"Infrastructure future - workshop on 11th July 2022"},{"location":"reports/2022-07-11-infra-workshop/#goals","text":"what goals do we want to achieve? Be able to scale horizontally disk space / scalability is one the main issue network management also Improve both the software and hardware side of the infrastructure Take ecology (carbon impact etc.) into account Human redundancy reduce stress Monitor thresholds, trends and critical alerts Redundancy, geographical PCA/PRA delegate, scale the potential for HR Reduce stress level Decrease the barrier to contribute for people who want to train their models 32 GB RAM, GPU for training","title":"Goals"},{"location":"reports/2022-07-11-infra-workshop/#solutions","text":"","title":"Solutions"},{"location":"reports/2022-07-11-infra-workshop/#priorities","text":"Forecast threshold and critical alerts and trends Identify and document in one place thresholds and critical things to monitor => Infra repo on github; rationale: external tool decentralized tool: .md files than can be cloned by everyone Alex Identify and document mega-trends (eg. images weight on disk) Infra repo on github Which trends; what to look for to identify the trends What\u2019s critical and what\u2019s not Who? Software evolutions of the monitoring infrastructure which tools Eg. Elastic search Human process Documented in infrastructure repo. Who for what kind of task? Pipeline to acquire, fidelize and level up competent people/contributors Public infra meetings (40 minutes each month) Prioritization of issues Nearly achieved threshold \u2026 Alex, St\u00e9phane, Christian; community: Olivier? Hangy? Syl20? Alligator? SRE\u2019s Meetup in Paris? Admin sys without borders? => Github issues => Automatic issues? => Sentry?","title":"Priorities"},{"location":"reports/2022-07-11-infra-workshop/#human-redundancy-everywhere","text":"Documentation: Document global infrastructure => Github openfoodfacts-infrastructure repo => .md files List all systems/areas and who masters what Identify gaps where we have only 1 or 2 persons e.g. zfs Identify owners: technical services, product owner Infrastructure spreadsheet Distinguish OFF\u2019s core infrastructure and peripheral services 3 levels: critical for us, critical for others, peripheral Technologies upskilling Fill gaps Skills spreadsheet: Munin, network, MongoDB, Docker, Proxmox, Nginx At least 2 people skilled of each technology Skills table: CT and VM list of OFF infrastructure TODO: fill the spreadsheet (All) Colleague's courses: ZFS, Docker, Proxmox, \u2026 Commercial courses: security (YesWeHack ?), diminish complexity where possible some complexity: due to hosting on OVH","title":"Human redundancy everywhere"},{"location":"reports/2022-07-11-infra-workshop/#horizontal-scaling","text":"1rst step: OFF1-OFF2 redundancy. MongoDB on OFF1 OFF2 hardware + OS upgrade Proxmox on OFF1/OFF2? Ask free to add a 3rd machine to make the migration. This machine could be the future test server for IA. Email to Jean-Claude (Christian). Christian & Stephane are listing hardware needs. Allow to test the horizontal scaling. What main issues do we want to solve? Adding hardware easily. Disk space and availability: a. Scaling with S3. b. Software: distinguish cold and hot pictures: - some pictures are never asked c. Deduplicate. - first mesure impact - excellent topic for a contributor d. When croped images are not the latest one, only keep the crop. e. Convert JPEG to webp (-50%). f. Decrease resolution for 48Mpixel images (=> 12 Mpixel) Priority for disk space: f, e b for the long term CPU. What is scaling horizontally? MongoDB: OK. Apache: OK. Images? Use software routing to manage data with or without images or STO. STO?","title":"Horizontal scaling"},{"location":"reports/2022-07-11-infra-workshop/#short-term","text":"Disk space: 6 month to solve this issue. Very short term: x3 is possible but doesn\u2019t solve the issue in the long term. Against rsync! => ZFS synchronization OFF1 & OFF2 disks to ZFS buy 2x2 equivalent disks (+1 in case of emergency). Ask Syl20 for MongoDB optimisation. Focus on IO: on off1 rsync are impacting performance on off2 at night it gets slow Product use cases: scan speed facet speed advanced search speed nightly data exports","title":"Short term"},{"location":"reports/2022-07-11-infra-workshop/#of-concurrent-scan-requests","text":"Increase modularity: being able to upgrade with confidence being able to deploy regularly, and with confidence what kind of evolutions to achieve these goals? what kind of hardware evolutions to follow: 1. products' growth, 2. resource consumption growth (API, web, etc.) and 3. software evolution. what short-term evolutions to manage products' growth (6-18 months).","title":"of concurrent scan requests"},{"location":"reports/2022-07-11-infra-workshop/#decrease-the-barrier-to-contribute-for-people-who-want-to-train-their-models","text":"64 GB RAM, GPU for training, Tool server with OSM?","title":"Decrease the barrier to contribute for people who want to train their models"},{"location":"reports/2022-07-11-infra-workshop/#potential-resources","text":"https://sre.google/books/","title":"Potential resources"},{"location":"reports/2022-07-11-infra-workshop/#small-tasks","text":"Document Munin in openfoodfacts-infrastructure Document what to look for in Munin Document monitoring usage Add API documentation for the reuse of Robotoff Plan de reprise de Robotoff","title":"Small tasks"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/","text":"Kibana down because of Elasticsearch circuit_breaking_exception # Symptoms # We got a lot of alerts in infrastructure-alerts slack channel for: Service probe on URL 'https://kibana.openfoodfacts.org/status' failed for more than 5 minutes. Going to the status url we got a \"server error\". Trying to diagnose and remedy # Base problem # On the machine, looking at kibana logs, while doing a request docker-compose logs --tail=0 -f kibana we see kiba na _ 1 | { \"type\" : \"log\" , \"@timestamp\" : \"2022-07-27T12:36:07+00:00\" , \"tags\" :[ \"error\" , \"plugins\" , \"security\" , \"authentication\" ], \"pid\" : 1216 , \"message\" : \"License is not available, authentication is not possible.\" } kiba na _ 1 | { \"type\" : \"log\" , \"@timestamp\" : \"2022-07-27T12:36:07+00:00\" , \"tags\" :[ \"warning\" , \"plugins\" , \"licensing\" ], \"pid\" : 1216 , \"message\" : \"License information could not be obtained from Elasticsearch due to {\\\"error\\\":{\\\"root_cause\\\":[{\\\"type\\\":\\\"circuit_breaking_exception\\\",\\\"reason\\\":\\\"[parent] Data too large, data for [<http_request>] would be [1028220976/980.5mb], which is larger than the limit of [1020054732/972.7mb], real usage: [1028220976/980.5mb], new bytes reserved: [0/0b], usages [request=0/0b, fielddata=0/0b, in_flight_requests=0/0b, model_inference=0/0b, eql_sequence=0/0b, accounting=76757928/73.2mb]\\\",\\\"bytes_wanted\\\":1028220976,\\\"bytes_limit\\\":1020054732,\\\"durability\\\":\\\"PERMANENT\\\"}],\\\"type\\\":\\\"circuit_breaking_exception\\\",\\\"reason\\\":\\\"[parent] Data too large, data for [<http_request>] would be [1028220976/980.5mb], which is larger than the limit of [1020054732/972.7mb], real usage: [1028220976/980.5mb], new bytes reserved: [0/0b], usages [request=0/0b, fielddata=0/0b, in_flight_requests=0/0b, model_inference=0/0b, eql_sequence=0/0b, accounting=76757928/73.2mb]\\\",\\\"bytes_wanted\\\":1028220976,\\\"bytes_limit\\\":1020054732,\\\"durability\\\":\\\"PERMANENT\\\"},\\\"status\\\":429} error\" } More memory # The day before, I gave more memory to ES to see if it was the problem, but it's not (see openfoodfacts-monitoring:#54 ). Continue diagnosis # If we go on kibana container we can reproduce error: docker-compose exec kibana bash ES is there: curl -XGET http://elasticsearch:9200 but we get the circuit_breaking_exception while querying curl - XGET h tt p : //elasticsearch:9200/_license?pretty=true { \"error\" : { \"root_cause\" : [ { \"type\" : \"circuit_breaking_exception\" , \"reason\" : \"[parent] Data too large, data for [<http_request>] would be [1027528328/979.9mb], which is larger than the limit of [1020054732/972.7mb], real usage: [1027528328/979.9mb], new bytes reserved: [0/0b], usages [request=0/0b, fielddata=0/0b, in_flight_requests=0/0b, model_inference=0/0b, eql_sequence=0/0b, accounting=76607288/73mb]\" , \"bytes_wanted\" : 1027528328 , \"bytes_limit\" : 1020054732 , \"durability\" : \"PERMANENT\" } ], \"type\" : \"circuit_breaking_exception\" , \"reason\" : \"[parent] Data too large, data for [<http_request>] would be [1027528328/979.9mb], which is larger than the limit of [1020054732/972.7mb], real usage: [1027528328/979.9mb], new bytes reserved: [0/0b], usages [request=0/0b, fielddata=0/0b, in_flight_requests=0/0b, model_inference=0/0b, eql_sequence=0/0b, accounting=76607288/73mb]\" , \"bytes_wanted\" : 1027528328 , \"bytes_limit\" : 1020054732 , \"durability\" : \"PERMANENT\" }, \"status\" : 429 } trying to get stats for breaker: curl -XGET elasticsearch:9200/_nodes/stats/breaker?pretty=true we see: \"parent\" : { \"limit_size_in_bytes\" : 1020054732 , \"limit_size\" : \"972.7mb\" , \"estimated_size_in_bytes\" : 1030370648 , \"estimated_size\" : \"982.6mb\" , \"overhead\" : 1.0 , \"tripped\" : 39108 } Parent is the responsible for all memory according to amazon help center : The parent circuit breaker (a circuit breaker type) is responsible for the overall memory usage of your cluster. I tried to clear out fielddata cache: curl -XPOST http://elasticsearch:9200/*/_cache/clear?fielddata=true but it does not solve the problem. Tunning parent circuit breaker # Comparing to reference documentation , our settings seems a bit low compared to defaults. I will try to augment the parent size for now, and set it to real memory tracking. Edited the docker-compose to add configuration (through environment variables): elasticsearch : ... environment : ... - \"indices.breaker.total.use_real_memory=true\" - \"indices.breaker.request.limit=95%\" and restarted the container docker-compose restart elasticsearch It's not enough. The issue might be that we have too much data (17Gb indexes) Garbage collection ? # Is that maybe related to Garbage collection ? Listing /usr/share/elasticsearch/config/jvm.options in the container shows that G1GC is in use: ## GC configuration 8 -13:-XX:+UseConcMarkSweepGC 8 -13:-XX:CMSInitiatingOccupancyFraction = 75 8 -13:-XX:+UseCMSInitiatingOccupancyOnly ## G1GC Configuration # NOTE: G1 GC is only supported on JDK version 10 or later # to use G1GC, uncomment the next two lines and update the version on the # following three lines to your version of the JDK # 10-13:-XX:-UseConcMarkSweepGC # 10-13:-XX:-UseCMSInitiatingOccupancyOnly 14 -:-XX:+UseG1GC line 14-: applies for we are on version 16 of the jdk: # jdk/bin/java --version openjdk 16.0.2 2021-07-20 Data size # In elasticsearch container: curl localhost:9200/_cluster/stats?pretty=true We got ... \"segments\" : { \"count\" : 2390 , \"memory_in_bytes\" : 76423432 , \"terms_memory_in_bytes\" : 65353984 , \"stored_fields_memory_in_bytes\" : 1225392 , \"term_vectors_memory_in_bytes\" : 0 , \"norms_memory_in_bytes\" : 9117696 , \"points_memory_in_bytes\" : 0 , \"doc_values_memory_in_bytes\" : 726360 , \"index_writer_memory_in_bytes\" : 35328144 , \"version_map_memory_in_bytes\" : 0 , \"fixed_bit_set_memory_in_bytes\" : 960 , \"max_unsafe_auto_id_timestamp\" : 1658927252002 , \"file_sizes\" : { } }, ... } other memory values (for querycache or fielddata ) ar 0 or near 0 . This is 188,175,008 bytes in total, which is 180m. If we look at nodes using cat nodes API : curl -XGET \"localhost:9200/_cat/nodes?v=true&h=name,node*,heap*\" name id node.role heap.current heap.percent heap.max 0665175f0369 3vOq cdfhilmrstw 978.5mb 95 1gb we see two potential problems: * heap size is only 1gb (it should be 2 according to our heap settings) * our a node use 95% memory Too many shards ? # following Es blog on memory troubleshooting Over sharding is a usual suspect, let see: # curl - XGET \"localhost:9200/_cluster/health?filter_path=status,*_shards&pretty=true\" { \"status\" : \"yellow\" , \"active_primary_shards\" : 317 , \"active_shards\" : 317 , \"relocating_shards\" : 0 , \"initializing_shards\" : 0 , \"unassigned_shards\" : 302 , \"delayed_unassigned_shards\" : 0 } while recommanded for our setting (2Gb) is 40 shards ! Also having unassigned_shards seems a bad news ! The problems comes from the fact that we have many indices. The ILM (Index Life Cycle Management) should prevent that, but it does not. Repairing (backup then remove old indices) # More memory # First I need to snapshot to eventually clear some indices. But with the exception I can't do it, I can't even check a snapshot repository already exists: $ curl -XGET 127.0.0.1:9200/_snapshot {\"error\":{\"root_cause\":[{\"type\":\"circuit_breaking_exception\",\"reason\":\"[parent] Data too large, data for [<http_request>] would be [1026619992/979mb], which is larger than the limit of [1020054732/972.7mb], real usage: [1026619992/979mb], new bytes reserved: [0/0b], usages [request=0/0b, fielddata=815/815b, in_flight_request... No choice then, we have to give ES enough memory to be able to handle current indices\u2026 we go for a hype of 8G. I change the docker-compose.yml : elasticsearch : ... environment : ... - \"ES_JAVA_OPTS=-Xms8048m -Xmx8048m\" ... mem_limit : 9g But then again after a while we are blocked by the exception again :-( Even stranger, we have a larger than the limit of [1020054732/972.7mb] circuit breaker limit was for request not total ! changed indices.breaker.request.limit=95% to indices.breaker.total.limit=95% . ... same result ! Snapshoting (backup) # Trying to backup: modified docker-compose.yml to add elasticsearch ... environment : ... - \"path.repo=/opt/elasticsearch/backups\" ... volumes : - elasticsearch-backup:/opt/elasticsearch volumes : ... elasticsearch-backup : - Create directories docker-compose run --rm elasticsearch bash cd /opt/elasticsearch mkdir backups chown elasticsearch /opt/elasticsearch/ -R Recreated container docker-compose rm -sf elasticsearch docker-compose up -d elasticsearch Created the snapshot repository curl -X PUT \"http://localhost:9200/_snapshot/backups?pretty\" -H 'Content-Type: application/json' -d ' { \"type\": \"fs\", \"settings\": { \"location\": \"/opt/elasticsearch/backups\" } } ' Note: (we had to retry several times because of circuit breaking exceptions) get all indices names curl -XGET \"http://localhost:9200/*?pretty=true\" | grep '^ \"logs' make a global snapshot, in a screen !!! screen ... curl -X PUT \"localhost:9200/_snapshot/backups/2022-07-31?wait_for_completion=true&pretty\" -H 'Content-Type: application/json' -d ' { \"indices\": [], \"ignore_unavailable\": true, \"include_global_state\": false, \"metadata\": { \"taken_by\": \"alex\", \"taken_because\": \"backup before removing some indices\" } } ' And it's a success ! { \"snapshot\" : { \"snapshot\" : \"2022-07-31\" , \"uuid\" : \"kHQXBTGhQb-4-jH0m5mMfg\" , \"repository\" : \"backups\" , \"version_id\" : 7140199 , ... \"state\" : \"SUCCESS\" , \"start_time\" : \"2022-07-31T18:06:51.220Z\" , \"start_time_in_millis\" : 1659290811220 , \"end_time\" : \"2022-07-31T18:15:08.301Z\" , \"end_time_in_millis\" : 1659291308301 , \"duration_in_millis\" : 497081 , \"failures\" : [ ], \"shards\" : { \"total\" : 322 , \"failed\" : 0 , \"successful\" : 322 }, \"feature_states\" : [ ] } } So we start removing indices ! Removing indices # Let's remove 2021 logs curl -X DELETE \"localhost:9200/logs-2021.*?pretty=true\" Is hard to pass, so we restart elasticsearch and try again\u2026 success. Lets remove 2022.01\u2026 until 04 Finally it works ! Fixing Snapshot policy and ILM policy # It is a temporary fix\u2026 now we have to use kibana to setup a better policy (and maybe take ES memory down a bit again ?) Create snapshot policy # In Kibana, go to Management -> Stack Management -> Snapshot and restore In \"Repositories\" we see the backups repository that was created before, pointing to /opt/elasticsearch/backups Remember that we had to edit the docker-compose.yml file to add the path.repo environment variable and the elasticsearch-backup volume. In \"Policies\", we create a policy named weekly-snapshots as follow: - Snapshot name: <weekly-snap-{now/d}> - Repository: backups - Schedule : weekly - Data streams and indices: All indices - Ignore unavailable indices: No - Allow partial shards: No - Include global state: Yes - Retention: delete after 360d - Min count: 20 Create Index Lifecycle Management policy # This part is very important. Logs are creating a lot of indices (as logrotate would) and so we have to automatically manage what happens with old indices so that we do not go beyond hardware capabilities (too much indices takes too much memory). ES has a mechanism for that called Index Lifecycle Management (ILM). In Kibana, go to Management -> Stack Management -> Index Lifecycle Policy There is already a log policy. Edit it to have this: Hot phase (the phase where the index is the active index for logs) Roll over : use recommended defaults set index priority to 100 Warm phase (the phase where the index is history of logs, you want to search) Move data into phase when 58 days old replicas: 0 Shrink: to 1 Force merge to 1 + compress mark read only index priority: 50 Cold phase (historical data, last phase before removal) Move data into phase when: 119 days old Do not make them searchable Delete phase: move data intor phase when 240 days old Wait for snapshot policy: weekly-snapshots Attach policy to index template # Attach the logs policy you edited to the logs index template. In Management -> Stack Management -> Index Lifecycle Policy, On the line corresponding to logs policy, use Action button + Add policy to index template Set template to logs Set alias to logs-current We notice that logs template pattern was not coherent, as it was logs-*-* , so we also add logs-*.*.* that match our indexes names. To do this goes to Management -> Stack Management -> Index Management -> Index Templates, and edit the logs template. Now we have to create the active alias. It should correspond to our last index. This was logs-2022.08.18 at time of writing. In the Dev Tool -> Console, we run: POST _aliases { \"actions\": [ { \"add\": { \"index\": \"logs-2022.08.18\", \"alias\": \"logs-current\", \"is_write_index\": true } } ] } We then go to Management -> Stack Management -> Index Management Select logs-2022.08.18 and use \"Manage\" apply log policy, with: * policy: logs * alias: logs-current Attach old index to a policy # This will add it for future indices, but we want to add it to existing indices (we follow ES doc, section Manage existing indices ). going to Management -> Stack Management -> Index Lifecycle Policy we create a policy that is exactly the same as logs policy but: name is logs-old rollover is disabled we apply it to 05 logs, in Dev Tool -> Console, by running: PUT logs-2022.05.*/_settings { \"index\": { \"lifecycle\": { \"name\": \"logs-old\" } } } we do the same for every monthes until 08, but beware not to apply it to 2022.08.18 ! Useful resources # About circuit breaker exception: - https://aws.amazon.com/premiumsupport/knowledge-center/opensearch-circuit-breaker-exception/ - https://www.elastic.co/blog/improving-node-resiliency-with-the-real-memory-circuit-breaker - https://www.elastic.co/guide/en/elasticsearch/reference/7.0/circuit-breaker.html#parent-circuit-breaker About index lifecycle management (ILM): * https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started-index-lifecycle-management.html * https://www.elastic.co/guide/en/elasticsearch/reference/current/index-lifecycle-management.html","title":"Kibana down because of Elasticsearch circuit_breaking_exception"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#kibana-down-because-of-elasticsearch-circuit_breaking_exception","text":"","title":"Kibana down because of Elasticsearch circuit_breaking_exception"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#symptoms","text":"We got a lot of alerts in infrastructure-alerts slack channel for: Service probe on URL 'https://kibana.openfoodfacts.org/status' failed for more than 5 minutes. Going to the status url we got a \"server error\".","title":"Symptoms"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#trying-to-diagnose-and-remedy","text":"","title":"Trying to diagnose and remedy"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#base-problem","text":"On the machine, looking at kibana logs, while doing a request docker-compose logs --tail=0 -f kibana we see kiba na _ 1 | { \"type\" : \"log\" , \"@timestamp\" : \"2022-07-27T12:36:07+00:00\" , \"tags\" :[ \"error\" , \"plugins\" , \"security\" , \"authentication\" ], \"pid\" : 1216 , \"message\" : \"License is not available, authentication is not possible.\" } kiba na _ 1 | { \"type\" : \"log\" , \"@timestamp\" : \"2022-07-27T12:36:07+00:00\" , \"tags\" :[ \"warning\" , \"plugins\" , \"licensing\" ], \"pid\" : 1216 , \"message\" : \"License information could not be obtained from Elasticsearch due to {\\\"error\\\":{\\\"root_cause\\\":[{\\\"type\\\":\\\"circuit_breaking_exception\\\",\\\"reason\\\":\\\"[parent] Data too large, data for [<http_request>] would be [1028220976/980.5mb], which is larger than the limit of [1020054732/972.7mb], real usage: [1028220976/980.5mb], new bytes reserved: [0/0b], usages [request=0/0b, fielddata=0/0b, in_flight_requests=0/0b, model_inference=0/0b, eql_sequence=0/0b, accounting=76757928/73.2mb]\\\",\\\"bytes_wanted\\\":1028220976,\\\"bytes_limit\\\":1020054732,\\\"durability\\\":\\\"PERMANENT\\\"}],\\\"type\\\":\\\"circuit_breaking_exception\\\",\\\"reason\\\":\\\"[parent] Data too large, data for [<http_request>] would be [1028220976/980.5mb], which is larger than the limit of [1020054732/972.7mb], real usage: [1028220976/980.5mb], new bytes reserved: [0/0b], usages [request=0/0b, fielddata=0/0b, in_flight_requests=0/0b, model_inference=0/0b, eql_sequence=0/0b, accounting=76757928/73.2mb]\\\",\\\"bytes_wanted\\\":1028220976,\\\"bytes_limit\\\":1020054732,\\\"durability\\\":\\\"PERMANENT\\\"},\\\"status\\\":429} error\" }","title":"Base problem"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#more-memory","text":"The day before, I gave more memory to ES to see if it was the problem, but it's not (see openfoodfacts-monitoring:#54 ).","title":"More memory"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#continue-diagnosis","text":"If we go on kibana container we can reproduce error: docker-compose exec kibana bash ES is there: curl -XGET http://elasticsearch:9200 but we get the circuit_breaking_exception while querying curl - XGET h tt p : //elasticsearch:9200/_license?pretty=true { \"error\" : { \"root_cause\" : [ { \"type\" : \"circuit_breaking_exception\" , \"reason\" : \"[parent] Data too large, data for [<http_request>] would be [1027528328/979.9mb], which is larger than the limit of [1020054732/972.7mb], real usage: [1027528328/979.9mb], new bytes reserved: [0/0b], usages [request=0/0b, fielddata=0/0b, in_flight_requests=0/0b, model_inference=0/0b, eql_sequence=0/0b, accounting=76607288/73mb]\" , \"bytes_wanted\" : 1027528328 , \"bytes_limit\" : 1020054732 , \"durability\" : \"PERMANENT\" } ], \"type\" : \"circuit_breaking_exception\" , \"reason\" : \"[parent] Data too large, data for [<http_request>] would be [1027528328/979.9mb], which is larger than the limit of [1020054732/972.7mb], real usage: [1027528328/979.9mb], new bytes reserved: [0/0b], usages [request=0/0b, fielddata=0/0b, in_flight_requests=0/0b, model_inference=0/0b, eql_sequence=0/0b, accounting=76607288/73mb]\" , \"bytes_wanted\" : 1027528328 , \"bytes_limit\" : 1020054732 , \"durability\" : \"PERMANENT\" }, \"status\" : 429 } trying to get stats for breaker: curl -XGET elasticsearch:9200/_nodes/stats/breaker?pretty=true we see: \"parent\" : { \"limit_size_in_bytes\" : 1020054732 , \"limit_size\" : \"972.7mb\" , \"estimated_size_in_bytes\" : 1030370648 , \"estimated_size\" : \"982.6mb\" , \"overhead\" : 1.0 , \"tripped\" : 39108 } Parent is the responsible for all memory according to amazon help center : The parent circuit breaker (a circuit breaker type) is responsible for the overall memory usage of your cluster. I tried to clear out fielddata cache: curl -XPOST http://elasticsearch:9200/*/_cache/clear?fielddata=true but it does not solve the problem.","title":"Continue diagnosis"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#tunning-parent-circuit-breaker","text":"Comparing to reference documentation , our settings seems a bit low compared to defaults. I will try to augment the parent size for now, and set it to real memory tracking. Edited the docker-compose to add configuration (through environment variables): elasticsearch : ... environment : ... - \"indices.breaker.total.use_real_memory=true\" - \"indices.breaker.request.limit=95%\" and restarted the container docker-compose restart elasticsearch It's not enough. The issue might be that we have too much data (17Gb indexes)","title":"Tunning parent circuit breaker"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#garbage-collection","text":"Is that maybe related to Garbage collection ? Listing /usr/share/elasticsearch/config/jvm.options in the container shows that G1GC is in use: ## GC configuration 8 -13:-XX:+UseConcMarkSweepGC 8 -13:-XX:CMSInitiatingOccupancyFraction = 75 8 -13:-XX:+UseCMSInitiatingOccupancyOnly ## G1GC Configuration # NOTE: G1 GC is only supported on JDK version 10 or later # to use G1GC, uncomment the next two lines and update the version on the # following three lines to your version of the JDK # 10-13:-XX:-UseConcMarkSweepGC # 10-13:-XX:-UseCMSInitiatingOccupancyOnly 14 -:-XX:+UseG1GC line 14-: applies for we are on version 16 of the jdk: # jdk/bin/java --version openjdk 16.0.2 2021-07-20","title":"Garbage collection ?"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#data-size","text":"In elasticsearch container: curl localhost:9200/_cluster/stats?pretty=true We got ... \"segments\" : { \"count\" : 2390 , \"memory_in_bytes\" : 76423432 , \"terms_memory_in_bytes\" : 65353984 , \"stored_fields_memory_in_bytes\" : 1225392 , \"term_vectors_memory_in_bytes\" : 0 , \"norms_memory_in_bytes\" : 9117696 , \"points_memory_in_bytes\" : 0 , \"doc_values_memory_in_bytes\" : 726360 , \"index_writer_memory_in_bytes\" : 35328144 , \"version_map_memory_in_bytes\" : 0 , \"fixed_bit_set_memory_in_bytes\" : 960 , \"max_unsafe_auto_id_timestamp\" : 1658927252002 , \"file_sizes\" : { } }, ... } other memory values (for querycache or fielddata ) ar 0 or near 0 . This is 188,175,008 bytes in total, which is 180m. If we look at nodes using cat nodes API : curl -XGET \"localhost:9200/_cat/nodes?v=true&h=name,node*,heap*\" name id node.role heap.current heap.percent heap.max 0665175f0369 3vOq cdfhilmrstw 978.5mb 95 1gb we see two potential problems: * heap size is only 1gb (it should be 2 according to our heap settings) * our a node use 95% memory","title":"Data size"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#too-many-shards","text":"following Es blog on memory troubleshooting Over sharding is a usual suspect, let see: # curl - XGET \"localhost:9200/_cluster/health?filter_path=status,*_shards&pretty=true\" { \"status\" : \"yellow\" , \"active_primary_shards\" : 317 , \"active_shards\" : 317 , \"relocating_shards\" : 0 , \"initializing_shards\" : 0 , \"unassigned_shards\" : 302 , \"delayed_unassigned_shards\" : 0 } while recommanded for our setting (2Gb) is 40 shards ! Also having unassigned_shards seems a bad news ! The problems comes from the fact that we have many indices. The ILM (Index Life Cycle Management) should prevent that, but it does not.","title":"Too many shards ?"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#repairing-backup-then-remove-old-indices","text":"","title":"Repairing (backup then remove old indices)"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#more-memory_1","text":"First I need to snapshot to eventually clear some indices. But with the exception I can't do it, I can't even check a snapshot repository already exists: $ curl -XGET 127.0.0.1:9200/_snapshot {\"error\":{\"root_cause\":[{\"type\":\"circuit_breaking_exception\",\"reason\":\"[parent] Data too large, data for [<http_request>] would be [1026619992/979mb], which is larger than the limit of [1020054732/972.7mb], real usage: [1026619992/979mb], new bytes reserved: [0/0b], usages [request=0/0b, fielddata=815/815b, in_flight_request... No choice then, we have to give ES enough memory to be able to handle current indices\u2026 we go for a hype of 8G. I change the docker-compose.yml : elasticsearch : ... environment : ... - \"ES_JAVA_OPTS=-Xms8048m -Xmx8048m\" ... mem_limit : 9g But then again after a while we are blocked by the exception again :-( Even stranger, we have a larger than the limit of [1020054732/972.7mb] circuit breaker limit was for request not total ! changed indices.breaker.request.limit=95% to indices.breaker.total.limit=95% . ... same result !","title":"More memory"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#snapshoting-backup","text":"Trying to backup: modified docker-compose.yml to add elasticsearch ... environment : ... - \"path.repo=/opt/elasticsearch/backups\" ... volumes : - elasticsearch-backup:/opt/elasticsearch volumes : ... elasticsearch-backup : - Create directories docker-compose run --rm elasticsearch bash cd /opt/elasticsearch mkdir backups chown elasticsearch /opt/elasticsearch/ -R Recreated container docker-compose rm -sf elasticsearch docker-compose up -d elasticsearch Created the snapshot repository curl -X PUT \"http://localhost:9200/_snapshot/backups?pretty\" -H 'Content-Type: application/json' -d ' { \"type\": \"fs\", \"settings\": { \"location\": \"/opt/elasticsearch/backups\" } } ' Note: (we had to retry several times because of circuit breaking exceptions) get all indices names curl -XGET \"http://localhost:9200/*?pretty=true\" | grep '^ \"logs' make a global snapshot, in a screen !!! screen ... curl -X PUT \"localhost:9200/_snapshot/backups/2022-07-31?wait_for_completion=true&pretty\" -H 'Content-Type: application/json' -d ' { \"indices\": [], \"ignore_unavailable\": true, \"include_global_state\": false, \"metadata\": { \"taken_by\": \"alex\", \"taken_because\": \"backup before removing some indices\" } } ' And it's a success ! { \"snapshot\" : { \"snapshot\" : \"2022-07-31\" , \"uuid\" : \"kHQXBTGhQb-4-jH0m5mMfg\" , \"repository\" : \"backups\" , \"version_id\" : 7140199 , ... \"state\" : \"SUCCESS\" , \"start_time\" : \"2022-07-31T18:06:51.220Z\" , \"start_time_in_millis\" : 1659290811220 , \"end_time\" : \"2022-07-31T18:15:08.301Z\" , \"end_time_in_millis\" : 1659291308301 , \"duration_in_millis\" : 497081 , \"failures\" : [ ], \"shards\" : { \"total\" : 322 , \"failed\" : 0 , \"successful\" : 322 }, \"feature_states\" : [ ] } } So we start removing indices !","title":"Snapshoting (backup)"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#removing-indices","text":"Let's remove 2021 logs curl -X DELETE \"localhost:9200/logs-2021.*?pretty=true\" Is hard to pass, so we restart elasticsearch and try again\u2026 success. Lets remove 2022.01\u2026 until 04 Finally it works !","title":"Removing indices"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#fixing-snapshot-policy-and-ilm-policy","text":"It is a temporary fix\u2026 now we have to use kibana to setup a better policy (and maybe take ES memory down a bit again ?)","title":"Fixing Snapshot policy and ILM policy"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#create-snapshot-policy","text":"In Kibana, go to Management -> Stack Management -> Snapshot and restore In \"Repositories\" we see the backups repository that was created before, pointing to /opt/elasticsearch/backups Remember that we had to edit the docker-compose.yml file to add the path.repo environment variable and the elasticsearch-backup volume. In \"Policies\", we create a policy named weekly-snapshots as follow: - Snapshot name: <weekly-snap-{now/d}> - Repository: backups - Schedule : weekly - Data streams and indices: All indices - Ignore unavailable indices: No - Allow partial shards: No - Include global state: Yes - Retention: delete after 360d - Min count: 20","title":"Create snapshot policy"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#create-index-lifecycle-management-policy","text":"This part is very important. Logs are creating a lot of indices (as logrotate would) and so we have to automatically manage what happens with old indices so that we do not go beyond hardware capabilities (too much indices takes too much memory). ES has a mechanism for that called Index Lifecycle Management (ILM). In Kibana, go to Management -> Stack Management -> Index Lifecycle Policy There is already a log policy. Edit it to have this: Hot phase (the phase where the index is the active index for logs) Roll over : use recommended defaults set index priority to 100 Warm phase (the phase where the index is history of logs, you want to search) Move data into phase when 58 days old replicas: 0 Shrink: to 1 Force merge to 1 + compress mark read only index priority: 50 Cold phase (historical data, last phase before removal) Move data into phase when: 119 days old Do not make them searchable Delete phase: move data intor phase when 240 days old Wait for snapshot policy: weekly-snapshots","title":"Create Index Lifecycle Management policy"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#attach-policy-to-index-template","text":"Attach the logs policy you edited to the logs index template. In Management -> Stack Management -> Index Lifecycle Policy, On the line corresponding to logs policy, use Action button + Add policy to index template Set template to logs Set alias to logs-current We notice that logs template pattern was not coherent, as it was logs-*-* , so we also add logs-*.*.* that match our indexes names. To do this goes to Management -> Stack Management -> Index Management -> Index Templates, and edit the logs template. Now we have to create the active alias. It should correspond to our last index. This was logs-2022.08.18 at time of writing. In the Dev Tool -> Console, we run: POST _aliases { \"actions\": [ { \"add\": { \"index\": \"logs-2022.08.18\", \"alias\": \"logs-current\", \"is_write_index\": true } } ] } We then go to Management -> Stack Management -> Index Management Select logs-2022.08.18 and use \"Manage\" apply log policy, with: * policy: logs * alias: logs-current","title":"Attach policy to index template"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#attach-old-index-to-a-policy","text":"This will add it for future indices, but we want to add it to existing indices (we follow ES doc, section Manage existing indices ). going to Management -> Stack Management -> Index Lifecycle Policy we create a policy that is exactly the same as logs policy but: name is logs-old rollover is disabled we apply it to 05 logs, in Dev Tool -> Console, by running: PUT logs-2022.05.*/_settings { \"index\": { \"lifecycle\": { \"name\": \"logs-old\" } } } we do the same for every monthes until 08, but beware not to apply it to 2022.08.18 !","title":"Attach old index to a policy"},{"location":"reports/2022-07-kibana-down-es-circuit-breaking-exception/#useful-resources","text":"About circuit breaker exception: - https://aws.amazon.com/premiumsupport/knowledge-center/opensearch-circuit-breaker-exception/ - https://www.elastic.co/blog/improving-node-resiliency-with-the-real-memory-circuit-breaker - https://www.elastic.co/guide/en/elasticsearch/reference/7.0/circuit-breaker.html#parent-circuit-breaker About index lifecycle management (ILM): * https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started-index-lifecycle-management.html * https://www.elastic.co/guide/en/elasticsearch/reference/current/index-lifecycle-management.html","title":"Useful resources"},{"location":"reports/2022-08-17-openfoodfacts-net-unreachable/","text":"2022-08-17 openfoodfacts.net unreachable # What happens # day before stagging (preprod) environment was down due to a bug introduced in the code A PR #openfoodfacts-server:7214 was submitted and merged to fix it github action went bogus, but was reruned later and passed though openfoodfacts.net was unreachable Why this happens # the day before, Charles saw that wiki and some other services were down and had to reboot proxy1 openfoodfacts.net was still using the old way to redirect: redirect to ovh1.openfoodfacts ip filters would then forward to internal ip 101 (aka proxy VM) this kind of forwarding did not work any more but I don't know why Also this way of doing as the drawback that nginx can't see the correct incoming address, but see instead 10.0.0.1 What was done # Resolving by pointing to proxy1 instead of ovh1 # I tested that using proxy1 address would work, using: curl --connect-to world.openfoodfacts.net:443:193.70.55.124:443 https://off:***@world.openfoodfacts.net Go on OVH interface, web cloud, domain names, openfoodfacts.net, dns zones Change CNAME and A previously pointing to ovh1 to point to proxy1 A rule for: openfoodfacts.net and *.openfoodfacts.net CNAME rule for: events.openfoodfacts.net , productopener.openfoodfacts.net , robotoff.openfoodfacts.net We had to wait for propagation... On the A rule I changed DNS TTL to 360s as it is a test environment... (suggested by Benoit382 on slack ) Trying to understand why ovh1 forwarding is down # Forwarding rule and Masquerading rule seems there: sudo iptables -t nat -L --verbose --line-numbers Chain PREROUTING ( policy ACCEPT 3135 packets, 201K bytes ) num pkts bytes target prot opt in out source destination 1 3016K 180M DNAT tcp -- any any anywhere ovh1.openfoodfacts.org tcp dpt:https to:10.1.0.101:443 2 318K 18M DNAT tcp -- any any anywhere ovh1.openfoodfacts.org tcp dpt:http to:10.1.0.101:80 ( ... ) Chain POSTROUTING ( policy ACCEPT 0 packets, 0 bytes ) num pkts bytes target prot opt in out source destination ( ... ) 3 2203K 150M MASQUERADE all -- any vmbr1 10 .1.0.0/16 anywhere 4 10M 697M MASQUERADE all -- any any anywhere anywhere 5 0 0 MASQUERADE all -- any vmbr1 10 .1.0.0/16 anywhere 6 0 0 MASQUERADE all -- any vmbr1 10 .1.0.0/16 anywhere But the proxy nginx does not seems accessible to ovh1: nc -vz 10.1.0.101 443 and nc -vz 10.1.0.101 80 are not responding While on the proxy itself, it is: # nc -vz 10.1.0.101 443 proxy [10.1.0.101] 443 (https) open This is linked to previous problem resolution, see Reverse proxy down on 18 f\u00e9v 2022, Final resolution . Solution: on proxy add a rule for routing 10.1.0.xxx packets: sudo ip route add 10.0.0.0/8 dev eth0 Now nc -vz 10.1.0.101 443 works from ovh1 and service is up again.","title":"2022-08-17 openfoodfacts.net unreachable"},{"location":"reports/2022-08-17-openfoodfacts-net-unreachable/#2022-08-17-openfoodfactsnet-unreachable","text":"","title":"2022-08-17 openfoodfacts.net unreachable"},{"location":"reports/2022-08-17-openfoodfacts-net-unreachable/#what-happens","text":"day before stagging (preprod) environment was down due to a bug introduced in the code A PR #openfoodfacts-server:7214 was submitted and merged to fix it github action went bogus, but was reruned later and passed though openfoodfacts.net was unreachable","title":"What happens"},{"location":"reports/2022-08-17-openfoodfacts-net-unreachable/#why-this-happens","text":"the day before, Charles saw that wiki and some other services were down and had to reboot proxy1 openfoodfacts.net was still using the old way to redirect: redirect to ovh1.openfoodfacts ip filters would then forward to internal ip 101 (aka proxy VM) this kind of forwarding did not work any more but I don't know why Also this way of doing as the drawback that nginx can't see the correct incoming address, but see instead 10.0.0.1","title":"Why this happens"},{"location":"reports/2022-08-17-openfoodfacts-net-unreachable/#what-was-done","text":"","title":"What was done"},{"location":"reports/2022-08-17-openfoodfacts-net-unreachable/#resolving-by-pointing-to-proxy1-instead-of-ovh1","text":"I tested that using proxy1 address would work, using: curl --connect-to world.openfoodfacts.net:443:193.70.55.124:443 https://off:***@world.openfoodfacts.net Go on OVH interface, web cloud, domain names, openfoodfacts.net, dns zones Change CNAME and A previously pointing to ovh1 to point to proxy1 A rule for: openfoodfacts.net and *.openfoodfacts.net CNAME rule for: events.openfoodfacts.net , productopener.openfoodfacts.net , robotoff.openfoodfacts.net We had to wait for propagation... On the A rule I changed DNS TTL to 360s as it is a test environment... (suggested by Benoit382 on slack )","title":"Resolving by pointing to proxy1 instead of ovh1"},{"location":"reports/2022-08-17-openfoodfacts-net-unreachable/#trying-to-understand-why-ovh1-forwarding-is-down","text":"Forwarding rule and Masquerading rule seems there: sudo iptables -t nat -L --verbose --line-numbers Chain PREROUTING ( policy ACCEPT 3135 packets, 201K bytes ) num pkts bytes target prot opt in out source destination 1 3016K 180M DNAT tcp -- any any anywhere ovh1.openfoodfacts.org tcp dpt:https to:10.1.0.101:443 2 318K 18M DNAT tcp -- any any anywhere ovh1.openfoodfacts.org tcp dpt:http to:10.1.0.101:80 ( ... ) Chain POSTROUTING ( policy ACCEPT 0 packets, 0 bytes ) num pkts bytes target prot opt in out source destination ( ... ) 3 2203K 150M MASQUERADE all -- any vmbr1 10 .1.0.0/16 anywhere 4 10M 697M MASQUERADE all -- any any anywhere anywhere 5 0 0 MASQUERADE all -- any vmbr1 10 .1.0.0/16 anywhere 6 0 0 MASQUERADE all -- any vmbr1 10 .1.0.0/16 anywhere But the proxy nginx does not seems accessible to ovh1: nc -vz 10.1.0.101 443 and nc -vz 10.1.0.101 80 are not responding While on the proxy itself, it is: # nc -vz 10.1.0.101 443 proxy [10.1.0.101] 443 (https) open This is linked to previous problem resolution, see Reverse proxy down on 18 f\u00e9v 2022, Final resolution . Solution: on proxy add a rule for routing 10.1.0.xxx packets: sudo ip route add 10.0.0.0/8 dev eth0 Now nc -vz 10.1.0.101 443 works from ovh1 and service is up again.","title":"Trying to understand why ovh1 forwarding is down"}]}